{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_YOLOv5_chizai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetYOLOv5_for_chizai**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "30e4512a-0e92-443f-c4cb-cb48bee423bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #サポートパッチのインポート\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545e1db7-8ed9-4de9-f266-2c5abddbceb5"
      },
      "source": [
        "'''\n",
        "・YOLOv5のモデルを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Inference of sample images**\n",
        "\n",
        "yolov5n_130epch.pt (developed using Osaka Univ dataset)"
      ],
      "metadata": {
        "id": "W8Kk6suE7POP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5n_130epch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_olympia_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "dataset_olympia_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "dataset_handai_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav\"\n",
        "dataset_handai_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/cont\""
      ],
      "metadata": {
        "id": "LY1JefL167J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()\n"
      ],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "5FWS5BJJ67MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{dataset_olympia_cont}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # 横幅を640pxにリサイズ\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "hzTyoHPw_U-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-GradCAM (新バージョン)**"
      ],
      "metadata": {
        "id": "xtC0Ozyf_W7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-GradCAM (2年前以前のバージョン専用)**\n",
        "https://colab.research.google.com/github/pooya-mohammadi/yolov5-gradcam/blob/master/main.ipynb#scrollTo=2z2oxEMRCBOk\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n"
      ],
      "metadata": {
        "id": "nshZUVxm_CrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###モデルの読み込み\n",
        "import torch\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
        "hubconf_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\"\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "model = torch.hub.load(hubconf_path, 'custom', model_path, source='local')\n",
        "print(model(torch.randn(1,3,640,640)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkMr1F8b26Kj",
        "outputId": "e66d68cd-dd36-4884-d3be-f70fbc70553d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 4.6 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 24.8 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 6.2s, installed 1 package: ['gitpython>=3.1.30']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 2269e2e Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4.88041e+00, 1.35679e-01, 3.84668e+01,  ..., 5.65154e-08, 3.26122e-01, 6.49781e-01],\n",
            "         [1.33316e+01, 5.77327e-01, 3.85642e+01,  ..., 4.33717e-08, 3.43050e-01, 6.59148e-01],\n",
            "         [2.12945e+01, 8.89058e-01, 3.81220e+01,  ..., 1.40185e-07, 3.51219e-01, 6.28838e-01],\n",
            "         ...,\n",
            "         [5.55764e+02, 6.07017e+02, 2.99500e+02,  ..., 1.45832e-06, 6.80712e-01, 2.18833e-01],\n",
            "         [5.82885e+02, 6.07734e+02, 2.86606e+02,  ..., 1.20806e-07, 6.77674e-01, 1.65309e-01],\n",
            "         [6.18567e+02, 6.11243e+02, 2.79418e+02,  ..., 3.64277e-08, 6.29886e-01, 1.68246e-01]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "#!pip install -U opencv-python --q\n",
        "# restart runtime required??"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwfgTUYT67Sk",
        "outputId": "47369edd-906a-40b0-dcd1-b8486321cc0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pooya-mohammadi/deep_utils.git\n",
            "  Cloning https://github.com/pooya-mohammadi/deep_utils.git to /tmp/pip-req-build-9bx19a2u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pooya-mohammadi/deep_utils.git /tmp/pip-req-build-9bx19a2u\n",
            "  Resolved https://github.com/pooya-mohammadi/deep_utils.git to commit 562e6af2dd13b34cfad3585838845dfd7fd72513\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (2023.11.17)\n",
            "Building wheels for collected packages: deep-utils\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deep-utils: filename=deep_utils-1.3.10-py3-none-any.whl size=535938 sha256=d62e69bbeb4c6ed14d967c4e89837b963b9b831d11535f3ab37d1561cd0d736b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fx512y9m/wheels/8f/0a/f4/5e2b92d9573699e3e30ce319a4b06218eb281695935d0b8b54\n",
            "Successfully built deep-utils\n",
            "Installing collected packages: deep-utils\n",
            "Successfully installed deep-utils-1.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWfeBZbD67Uz",
        "outputId": "b472dc32-c721-4f30-84f8-1f66aa332a20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 28.45 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%cd /content/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE5fpiWP67W-",
        "outputId": "8ed9b3c1-49ef-4019-ea8d-83030d03ec91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5-gradcam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "%matplotlib inline\n",
        "\n",
        "sys.path.append('/content/yolov5-gradcam')  # yolov5-gradcamディレクトリをパスに追加\n",
        "\n",
        "from models.gradcam import YOLOV5GradCAM\n",
        "from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "\n",
        "import cv2\n",
        "from deep_utils import Box, split_extension\n",
        "\n",
        "# Set parameters directly\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "#model_path = \"yolov5n.pt\"\n",
        "#img_path = \"/content/drive/MyDrive/AI_laboratory_course/goldenretriever-3724972_640.jpg\"\n",
        "img_path = \"/content/drive/MyDrive/AI_laboratory_course/puppies.jpg\"\n",
        "output_dir = \"out\"\n",
        "img_size = 640\n",
        "target_layer = \"model_23_cv3_act\" #デフォルトはYOLOv5sだがどのモデルでも同じっぽい\n",
        "method = \"gradcam\"\n",
        "device = \"cpu\"\n",
        "names = [\"cont\", \"grav\"]\n",
        "#names = None\n",
        "\n",
        "def get_res_img(bbox, mask, res_img):\n",
        "    mask = mask.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "    res_img = res_img / 255\n",
        "    res_img = cv2.add(res_img, n_heatmat)\n",
        "    res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.imshow\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "    res_img = Box.put_text(res_img, cls_name, (x1, y1))\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def main(img_path):\n",
        "    print(names)\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "    # for i, mask in enumerate(masks):\n",
        "    #     res_img = result.copy()\n",
        "    #     bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "    #     res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "    #     res_img = put_text_box(bbox, cls_name, res_img)\n",
        "    #     images.append(res_img)\n",
        "\n",
        "\n",
        "    for i, mask in enumerate(masks):\n",
        "        res_img = result.copy()\n",
        "        bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "        res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        show_image(res_img)  # Display each image individually\n",
        "    final_image = concat_images(images)\n",
        "    show_image(final_image)  # Display the image instead of saving it\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    for item in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img = cv2.imread(img_path)\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "        print(\"total time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "        for i, mask in enumerate(masks):\n",
        "            res_img = result.copy()\n",
        "            bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "            res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "        final_image = concat_images(images)\n",
        "        show_image(final_image)  # Display the image instead of saving it\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXPh1IB4VqcM",
        "outputId": "d5fe233d-e65f-4cf7-e07c-6b3552fff1ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cont', 'grav']\n",
            "[INFO] Loading the model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3e2019f0cd56>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mfolder_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-3e2019f0cd56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Loading the model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOV5TorchObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gradcam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5-gradcam/models/yolo_v5_object_detector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_weight, device, img_size, names, mode, confidence, iou_thresh, agnostic_nms)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miou_thresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magnostic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magnostic_nms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Model is loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5-gradcam/models/experimental.py\u001b[0m in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfuse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fused or un-fused model in eval mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'DetectionModel' on <module 'models.yolo' from '/content/yolov5-gradcam/models/yolo.py'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSkqOCk1IZ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbaoVy-bIZ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76nkcUUIzUey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AoBfT0SzUgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwDnNonIzUi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNTnFR0MzUkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UvEHekMzUnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lr2FoWajIZ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofa0v0XSIaBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPP8B8rbIaDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "\n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# ディレクトリのパス\n",
        "directory_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont/images_cropped\"\n",
        "\n",
        "# ディレクトリ内のjpgファイルをリストアップ\n",
        "jpg_files = [f for f in os.listdir(directory_path) if f.endswith('.JPG')]\n",
        "\n",
        "# 1つのjpgファイルを表示\n",
        "if len(jpg_files) > 0:\n",
        "    file_to_display = jpg_files[0]  # 1つ目のファイルを表示\n",
        "    file_path = os.path.join(directory_path, file_to_display)\n",
        "    display(Image(filename=file_path))\n",
        "else:\n",
        "    print(\"指定されたディレクトリ内にjpgファイルが見つかりませんでした。\")\n"
      ],
      "metadata": {
        "id": "OiVhW7PAZhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**データセットのpreprocess**"
      ],
      "metadata": {
        "id": "RPDTAk_iNfY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels\n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### フォルダのリセット #######\n",
        "\n",
        "# # MobileNet用に224px四方に成形しておく\n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "# if os.path.exists(dst_folder):\n",
        "#     shutil.rmtree(dst_folder)\n",
        "# os.makedirs(f\"{dst_folder}/train\")\n",
        "# os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:\n",
        "          print(i)\n",
        "          print(in_path + '/' + i)\n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "\n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()\n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "\n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "\n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "\n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min')\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "eI2_SlJUcqDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3**"
      ],
      "metadata": {
        "id": "BU8Bw69eN4gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "91wReBK3OCjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "##################\n",
        "## Define GradCAM ##\n",
        "##################\n",
        "\n",
        "def compute_gradcam(model, image_tensor, target_class=None):\n",
        "    model.eval()\n",
        "\n",
        "    features = None\n",
        "    grads = None\n",
        "\n",
        "    def hook_feature(module, input, output):\n",
        "        nonlocal features\n",
        "        features = output\n",
        "\n",
        "    def hook_grad(module, grad_in, grad_out):\n",
        "        nonlocal grads\n",
        "        grads = grad_out[0]\n",
        "\n",
        "    final_conv = model.blocks[-1]  # この部分はモデルの構造により調整が必要\n",
        "    final_conv.register_forward_hook(hook_feature)\n",
        "    final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax().item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        activations = features.cpu().numpy()\n",
        "        grad_values = grads.cpu().numpy()\n",
        "        weights = np.mean(grad_values, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[0, i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (image_tensor.shape[2], image_tensor.shape[3]))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "\n",
        "    # Memory clean-up\n",
        "    del features, grads, activations, grad_values, weights\n",
        "    torch.cuda.empty_cache()  # If you're using CUDA\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "hzvNpNeravOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dlibを用いた眼周囲抜き出し\n",
        "\n",
        "こちらの方がmediapipeより検出率、精度が高い"
      ],
      "metadata": {
        "id": "mqchGqI3SZT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib --q\n",
        "!pip install opencv-python --q\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n"
      ],
      "metadata": {
        "id": "zq_FRvcq0Ilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "# mismatched = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = mismatched\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 30))\n",
        "\n",
        "\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "counter_0_overall_correct = 0\n",
        "counter_1 = 0  # img_label == pred.item()\n",
        "counter_2 = 0  # img_label != pred_item\n",
        "counter_3_same_pred_and_mask_correct = 0  # pred.item() == mask_diagnosis and img_label == pred.item()\n",
        "counter_3_same_pred_and_mask_incorrect = 0  # pred.item() == mask_diagnosis and img_lanbel != pred.label\n",
        "counter_4_diff_pred_and_mask_model_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_4_diff_pred_and_mask_mask_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_5_matched_pred_1_correct = 0\n",
        "counter_5_matched_pred_0_correct = 0\n",
        "counter_5_matched_pred_1_incorrect = 0\n",
        "counter_5_matched_pred_0_incorrect = 0\n",
        "misclassified_indices, mismatched_indices = [], []\n",
        "mismatched_indices_pred_1, mismatched_indices_pred_0 = [], []\n",
        "mismatched_indices_pred_1_model_correct, mismatched_indices_pred_0_model_correct = [], []\n",
        "mismatched_indices_pred_1_mask_correct, mismatched_indices_pred_0_mask_correct = [], []\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "\n",
        "def crop_bilateral(in_path):\n",
        "    img = cv2.imread(in_path)\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    mask = np.zeros_like(grayscale_img)\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "    for (x, y, w, h) in eye_list:\n",
        "        mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "    return mask\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        # if img_label == 1:  #sort by label\n",
        "        # #if img_label == 0:\n",
        "        # #if img_label == 0 or img_label == 1:\n",
        "            #GradCAM\n",
        "            pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "            numpy_image = np.array(pilr_image)\n",
        "            cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "            image_tensor = test_data_transforms(pilr_image)\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "            output = model_ft(image_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = nn.Softmax(dim=1)(output)[0][1].cpu().detach().item()\n",
        "\n",
        "            cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "            cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "\n",
        "            #注目部位を２値化して表示\n",
        "            mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "            mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "            mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            #HaarCascadeによる眼周囲抜き出し\n",
        "            periocular_mask = crop_bilateral(img_path)\n",
        "\n",
        "            #GradCAM、眼周囲領域の共通点をマスク化する\n",
        "            common_mask_0 = cv2.bitwise_and(mask_0_resized, periocular_mask)\n",
        "            common_mask_1 = cv2.bitwise_and(mask_1_resized, periocular_mask)\n",
        "\n",
        "            area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "            area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "            mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "\n",
        "\n",
        "            # Convert the common masks to 3-channel images for visualization\n",
        "            common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "            common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "            heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "            heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "\n",
        "            heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "            overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "            # 1. 3チャンネルの画像に変換\n",
        "            periocular_colored_mask = cv2.cvtColor(periocular_mask, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            # 2. overlayed_image_1にperiocular_maskをオーバーレイ\n",
        "            overlayed_image_1_with_mask = cv2.addWeighted(overlayed_image_1, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # 3. overlayed_image_0にperiocular_maskをオーバーレイ\n",
        "            overlayed_image_0_with_mask = cv2.addWeighted(overlayed_image_0, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # 結果の表示\n",
        "            result_with_mask = np.hstack([cv2_image, overlayed_image_1_with_mask, overlayed_image_0_with_mask, common_mask_1, common_mask_0])\n",
        "\n",
        "            # Inside your loop\n",
        "            if img_label == pred.item():\n",
        "                counter_0_overall_correct += 1\n",
        "\n",
        "            if pred.item() == mask_diagnosis:\n",
        "                counter_1 += 1\n",
        "                if img_label == pred.item():\n",
        "                    counter_3_same_pred_and_mask_correct += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_correct += 1\n",
        "                        print(\"match_pred_1_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_correct\")\n",
        "                elif img_label != pred.item():\n",
        "                    counter_3_same_pred_and_mask_incorrect += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_incorrect += 1\n",
        "                        print(\"match_pred_1_incorrect\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_incorrect\")\n",
        "\n",
        "            if pred.item() != mask_diagnosis:\n",
        "                counter_2 += 1\n",
        "                mismatched_indices.append(i)\n",
        "                #print(f\"pred: {pred.item()}, mask: {mask_diagnosis}\")\n",
        "                if img_label == pred.item():\n",
        "                    counter_4_diff_pred_and_mask_model_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_model_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_model_correct\")\n",
        "                elif img_label == mask_diagnosis:\n",
        "                    counter_4_diff_pred_and_mask_mask_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_mask_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_mask_correct\")\n",
        "\n",
        "            # 予測結果と実際のラベルを比較して、TP, TN, FP, FNのカウントを更新\n",
        "            if img_label == 1 and pred.item() == 1:\n",
        "                TP += 1\n",
        "            elif img_label == 0 and pred.item() == 0:\n",
        "                TN += 1\n",
        "            elif img_label == 0 and pred.item() == 1:\n",
        "                FP += 1\n",
        "            elif img_label == 1 and pred.item() == 0:\n",
        "                FN += 1\n",
        "\n",
        "            print(f\"{i}\")\n",
        "            print(f\"{os.path.basename(img_path)}\")\n",
        "            print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "            print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "            ###GradCAM画像を表示（表示しないときはコメントアウトする）\n",
        "            #cv2_imshow(result_with_mask)\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "            if pred.item() != img_label:\n",
        "                misclassified_indices.append(i)\n",
        "\n",
        "            # メモリ解放のための操作\n",
        "            del pilr_image, numpy_image, cv2_image, image_tensor, output, pred\n",
        "            del cam_class_0, cam_class_1, mask_0, mask_1, mask_0_resized, mask_1_resized\n",
        "            del heatmap_0, heatmap_1, heatmap_0_resized, heatmap_1_resized, overlayed_image_0, overlayed_image_1\n",
        "            del common_mask_0, common_mask_1, result_with_mask\n",
        "\n",
        "            # 明示的にガベージコレクションを実行\n",
        "            gc.collect()\n",
        "\n",
        "total_images = len(image_indices)\n",
        "ratio_0 = counter_0_overall_correct*100/total_images\n",
        "ratio_1 = counter_1*100/total_images\n",
        "ratio_2 = counter_2*100/total_images\n",
        "print(f\"Model accuracy: {ratio_0:.3f}% ({counter_0_overall_correct}/{total_images})\")\n",
        "print(f\"Model and GradCAM judgement match: {ratio_1:.3f}% ({counter_1}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_3_same_pred_and_mask_correct*100/counter_1:.3f}% ({counter_3_same_pred_and_mask_correct}/{counter_1})\")\n",
        "print(f\"Model and GradCAM judgement do not match: {ratio_2:.3f}% ({counter_2}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_4_diff_pred_and_mask_model_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_model_correct}/{counter_2})\")\n",
        "print(f\"    Mask accuracy: {counter_4_diff_pred_and_mask_mask_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_mask_correct}/{counter_2})\")\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")\n",
        "print(f\"mismatched_indices: {mismatched_indices}\")\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")\n",
        "\n",
        "# 感度と特異度の計算\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m-m3ZFMou287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have defined your lists and calculated their lengths, for example:\n",
        "pred_0_model_correct_count = len(mismatched_indices_pred_0_model_correct)\n",
        "pred_0_mask_correct_count = len(mismatched_indices_pred_0_mask_correct)\n",
        "pred_1_model_correct_count = len(mismatched_indices_pred_1_model_correct)\n",
        "pred_1_mask_correct_count = len(mismatched_indices_pred_1_mask_correct)\n",
        "\n",
        "# 1. Create a matrix of the counts:\n",
        "matrix = [\n",
        "    [pred_1_model_correct_count, pred_1_mask_correct_count],\n",
        "    [pred_0_model_correct_count, pred_0_mask_correct_count],\n",
        "]\n",
        "\n",
        "# 2. Use seaborn's heatmap to visualize the matrix:\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(font_scale=1.2) # for label size\n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"model_correct\", \"mask_incorrect\"],\n",
        "            yticklabels=[\"model_pred_1\", \"model_pred_0\"])\n",
        "plt.title(\"In case model and mask diagnosis not consistent\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-HDn_QLB690x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_model_pred_0_inconsistent = pred_0_model_correct_count/(pred_0_model_correct_count+pred_0_mask_correct_count)\n",
        "Accuracy_model_pred_1_inconsistent = pred_1_model_correct_count/(pred_1_model_correct_count+pred_1_mask_correct_count)\n",
        "print(f\"Accuracy_model_pred_1_inconsistent: {Accuracy_model_pred_1_inconsistent*100:.2f}% ({pred_1_model_correct_count}/{pred_1_model_correct_count+pred_1_mask_correct_count})\")\n",
        "print(f\"Accuracy_model_pred_0_inconsistent {Accuracy_model_pred_0_inconsistent*100:.2f}% ({pred_0_model_correct_count}/{pred_0_model_correct_count+pred_0_mask_correct_count})\")\n",
        "Accuracy_model_pred_0_consistent = counter_5_matched_pred_0_correct/(counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect)\n",
        "Accuracy_model_pred_1_consistent = counter_5_matched_pred_1_correct/(counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect)\n",
        "print(f\"Accuracy_model_pred_1_consistent: {Accuracy_model_pred_1_consistent*100:.2f}% ({counter_5_matched_pred_1_correct}/{counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect})\")\n",
        "print(f\"Accuracy_model_pred_0_consistent {Accuracy_model_pred_0_consistent*100:.2f}% ({counter_5_matched_pred_0_correct}/{counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect})\")\n",
        "\n",
        "#モデルと注目点が合致する症例に限定した場合\n",
        "TP = counter_5_matched_pred_1_correct\n",
        "FP = counter_5_matched_pred_1_incorrect\n",
        "FN = counter_5_matched_pred_0_incorrect\n",
        "TN = counter_5_matched_pred_0_correct\n",
        "\n",
        "# 感度と特異度の計算\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")"
      ],
      "metadata": {
        "id": "abEcpyatIMFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ビデオ解析**"
      ],
      "metadata": {
        "id": "Qkcv1Poa8iyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the MP4 file and the directory to save frames\n",
        "video_path = \"/content/drive/MyDrive/Deep_learning/GravAI_iOS/test_movie/Grav_1.mp4\"\n",
        "save_path = \"/content/temp\"\n",
        "\n",
        "# Check if the save directory exists; if so, remove it\n",
        "if os.path.exists(save_path):\n",
        "    os.rmtree(save_path)\n",
        "\n",
        "# Create the directory\n",
        "os.mkdir(save_path)\n",
        "\n",
        "# Load the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frame counter\n",
        "count = 0\n",
        "\n",
        "# Check if the video is opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Read until video is completed\n",
        "while cap.isOpened():\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        # Save a frame every 10 frames\n",
        "        if count % 10 == 0:\n",
        "            frame_name = os.path.join(save_path, f\"frame_{count}.jpg\")\n",
        "            cv2.imwrite(frame_name, frame)\n",
        "            print(f\"Saved {frame_name}\")\n",
        "\n",
        "        count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "WI0X3akb7fLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iccKxhGN7fRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVESbPIt6WJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbE-WDwM7fUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kkcpd3f67fXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1SgwLyvRaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###叩き台（ボツ）のコード達\n",
        "\n",
        "Google mediapipeは顔面メッシュが検出できない症例が多いため採用せず"
      ],
      "metadata": {
        "id": "7GOZBL9t-pOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# # 読み込む画像ファイル\n",
        "# IMAGE_FILES = val_list[0]\n",
        "\n",
        "# # Google Mediapipe periocular landmarks #\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.5) as face_mesh:\n",
        "\n",
        "#     image = cv2.imread(IMAGE_FILES)\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       exit(\"No landmarks detected\")\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#         466, 388, 387, 386, 385, 384, 398,\n",
        "#         263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     mask = np.zeros_like(image)\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#         points_right = []\n",
        "#         points_left = []\n",
        "\n",
        "#         for idx in landmarks_list_right:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_right.append([loc_x, loc_y])\n",
        "\n",
        "#         for idx in landmarks_list_left:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_left.append([loc_x, loc_y])\n",
        "\n",
        "#         cv2.fillPoly(mask, [np.array(points_right)], (255,255,255))\n",
        "#         cv2.fillPoly(mask, [np.array(points_left)], (255,255,255))\n",
        "\n",
        "#     masked_image = cv2.bitwise_and(mask, np.ones_like(image) * 255)\n",
        "#     cv2_imshow(masked_image)\n"
      ],
      "metadata": {
        "id": "vkojIsOFugFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe opencv-python --q\n",
        "\n",
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# #########################################################\n",
        "# # Google Mediapipe periocular landmarks ※landmarkを検出できない症例がある#\n",
        "# #########################################################\n",
        "\n",
        "# mp_drawing = mp.solutions.drawing_utils\n",
        "# mp_drawing_styles = mp.solutions.drawing_styles\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# # For static images:\n",
        "\n",
        "# import glob\n",
        "# val_list = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid/*\")[0:100]\n",
        "# IMAGE_FILES = val_list\n",
        "# print(IMAGE_FILES)\n",
        "\n",
        "# drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.15,\n",
        "#     min_tracking_confidence=0.1) as face_mesh:\n",
        "\n",
        "#   for idx, file in enumerate(IMAGE_FILES):\n",
        "#     print(file)\n",
        "#     image = cv2.imread(file)\n",
        "#     # Convert the BGR image to RGB before processing.\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#     # Print and draw face mesh landmarks on the image.\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       print(\"no landmark detected\")\n",
        "#       continue\n",
        "#     annotated_image = image.copy()\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246,\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#     466, 388, 387, 386, 385, 384, 398,\n",
        "#     263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#       for idx in landmarks_list_right:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (255,0,255), 2)\n",
        "\n",
        "#       for idx in landmarks_list_left:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (0,255,255), 2)\n",
        "\n",
        "#     cv2_imshow(annotated_image)\n"
      ],
      "metadata": {
        "id": "3yqasLAdGt9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "##############################################\n",
        "# Inference MobileNetV3 with GradCAM ※メモリ対策バージョン#\n",
        "##############################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1, heatmap_0, heatmap_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "kghHA3ksgYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with 黒塗りGradCAM_白黒バージョン追加 #\n",
        "####################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0])\n",
        "        cv2_imshow(result)\n"
      ],
      "metadata": {
        "id": "0eu241VBwUdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with 黒塗りGradCAM #\n",
        "####################################\n",
        "\"\"\"\n",
        "GradCAMの注目点を黒塗りで隠してinferenceしたときに、どのように結果が変わるかを確認\n",
        "（結果としてあまり参考にならなさそう）\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Resize the masks\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # 黒く塗りつぶした画像をTensorに変換\n",
        "        blacked_out_tensor_0 = test_data_transforms(Image.fromarray(blacked_out_image_0)).unsqueeze(0).to(device)\n",
        "        blacked_out_tensor_1 = test_data_transforms(Image.fromarray(blacked_out_image_1)).unsqueeze(0).to(device)\n",
        "\n",
        "        # モデルで評価\n",
        "        output_0 = model_ft(blacked_out_tensor_0)\n",
        "        _, pred_0 = torch.max(output_0, 1)\n",
        "\n",
        "        prob_0 = nn.Softmax(dim=1)(output_0)\n",
        "        prob_0 = prob_0[0][1].cpu().detach()\n",
        "        prob_0 = \"{:.3f}\".format(prob_0.item())\n",
        "\n",
        "        output_1 = model_ft(blacked_out_tensor_1)\n",
        "        _, pred_1 = torch.max(output_1, 1)\n",
        "\n",
        "        prob_1 = nn.Softmax(dim=1)(output_1)\n",
        "        prob_1 = prob_1[0][1].cpu().detach()\n",
        "        prob_1 = \"{:.3f}\".format(prob_1.item())\n",
        "\n",
        "        # 結果を出力\n",
        "        print(f\"Blacked-out image (Class 1): prob: {prob_1}, pred: {pred_1.item()}\")\n",
        "        print(f\"Blacked-out image (Class 0): prob: {prob_0}, pred: {pred_0.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "wCvSB8X5OOkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAMで提示された関心領域のマスクと、HaarCascadeで検出された眼周囲（縦はそのまま、横は幅を半分にした長方形）との重複領域を計算する。\n",
        "GradCAMについてはGravと判定する場合とContと判定する場合とで両方とも計算を行う。\n",
        "重複領域がGrav>Contであればmask_diagnosis = \"Grav\"、重複領域がGrav<Contであればmask_diagnosis = \"Cont\"とする。\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####眼周囲のマスクを作成する\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # リストをNumPyの配列に変換\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPyの表示オプションを変更\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####ここまで\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "Mtc7lBGOdha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAMで提示された関心領域のマスクと、HaarCascadeで検出された眼周囲（縦はそのまま、横は幅を半分にした長方形）との重複領域を計算する。\n",
        "GradCAMについてはGravと判定する場合とContと判定する場合とで両方とも計算を行う。\n",
        "重複領域がGrav>Contであればmask_diagnosis = \"Grav\"、重複領域がGrav<Contであればmask_diagnosis = \"Cont\"とする。\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####眼周囲のマスクを作成する\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # リストをNumPyの配列に変換\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPyの表示オプションを変更\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####ここまで\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "ap-a6G9ou27W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "####. HaarCascadeのテスト用 #####\n",
        "###########################\n",
        "\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def crop_bilateral(in_path, class_num, showImage=True):\n",
        "#     img_resized_list, side_list = [], []\n",
        "\n",
        "#     img = cv2.imread(in_path)\n",
        "#     img2 = img.copy()\n",
        "\n",
        "#     # Convert to grayscale for eye detection\n",
        "#     grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "#     # Create a black mask of the same size as the grayscale image\n",
        "#     mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "#     # Detect eyes\n",
        "#     eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "#     print(\"\\nimage path = \", in_path)\n",
        "\n",
        "#     if len(eye_list) >= 1:\n",
        "#         print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "#         for (x, y, w, h) in eye_list:\n",
        "#             # Fill the detected eye region with value 255\n",
        "#             mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "#     else:\n",
        "#         print(\"no eye detected\")\n",
        "\n",
        "#     print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "#     # Show the mask if required using matplotlib for inline display in Colab\n",
        "#     if showImage:\n",
        "#         plt.imshow(mask, cmap='gray')\n",
        "#         plt.axis('off')  # Hide axis\n",
        "#         plt.show()\n",
        "\n",
        "#     return mask\n",
        "\n",
        "# # Assuming you have initialized eye_cascade before\n",
        "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "# # Test\n",
        "# # Assuming val_list and class_num are previously defined\n",
        "# test_path = val_list[0]\n",
        "# crop_bilateral(test_path, class_num=0)\n"
      ],
      "metadata": {
        "id": "0G6x2rv9WnIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Interference MobileNetV3 (GradCAMなしバージョン)#\n",
        "########################################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, test_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0) #val_datasetを1枚ずつにしてtest_loadeerに格納\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      #target = target.squeeze(1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "\n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "      probs.append(float(prob)) #予測確率\n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference MobileNet3"
      ],
      "metadata": {
        "id": "XvjkKZez_IrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # Import garbage collector module\n",
        "import os\n",
        "\n",
        "######################################\n",
        "# Inference MobileNetV3 ※GradCAMなし#\n",
        "######################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "paths, targets, probs, preds = [], [], [], []\n",
        "\n",
        "image_indices = list(range(0, 665))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        paths.append(str(os.path.basename(img_path)))\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "JFL4WogdahVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'paths': paths,\n",
        "    'Target': y_label,\n",
        "    'Probability': y_prob,\n",
        "    'Prediction': y_pred\n",
        "})\n",
        "pd.set_option('display.max_rows', 700)\n",
        "\n",
        "file_path = \"/content/result_list.csv\"\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hbDCHbieT3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "itGmZ613_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# #################################################\n",
        "# threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "# #################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "# Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "# Compute the ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc_score(y_label, y_prob):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "# plt.plot(fpr, tpr, marker='o')\n",
        "# plt.xlabel('FPR')\n",
        "# plt.ylabel('TPR')\n",
        "# plt.grid()\n",
        "# print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# confusion matrixをheatmapで表示\n",
        "cm = confusion_matrix(y_label, y_pred, labels=[1, 0])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['TED', 'control'],\n",
        "            yticklabels=['TED', 'control'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CAhawNTE8k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n"
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "id": "AAUdjy9A0YZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "ZjV_xXLpd5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}