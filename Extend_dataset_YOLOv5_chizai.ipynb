{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_YOLOv5_chizai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetYOLOv5_for_chizai**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "30e4512a-0e92-443f-c4cb-cb48bee423bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import pandas as pd\n",
        "import csv\n",
        "from random import randint\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# #„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "random_seed = 3 #shuffle„ÅÆ„Ç∑„Éº„Éâ\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colab„Çí„Éû„Ç¶„É≥„Éà"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545e1db7-8ed9-4de9-f266-2c5abddbceb5"
      },
      "source": [
        "'''\n",
        "„ÉªYOLOv5„ÅÆ„É¢„Éá„É´„ÇíÁî®„ÅÑ„Å¶ÁõÆ„ÇíÂàá„ÇäÊäú„Åè\n",
        "„ÉªÊ®™ÂπÖ„Çí2ÂÄç„ÄÅÁ∏¶ÂπÖ„Çí‰∏ä„Å´1ÂÄçËøΩÂä†/‰∏ã„Å´0.5ÂÄçËøΩÂä†„Åó„Åü‰∏°Áúº„ÅÆÁîªÂÉè„ÅåÂê´„Åæ„Çå„Çã„Çà„ÅÜ„Å´Âàá„ÇäÂèñ„ÇãÔºàÁõÆ„ÅÆÂÖ®ÂπÖ„ÄÅÁúâÊØõ„ÅåÂê´„Åæ„Çå„Çã„Çà„ÅÜ„Å´Ôºâ\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Inference of sample images**\n",
        "\n",
        "yolov5n_130epch.pt (developed using Osaka Univ dataset)"
      ],
      "metadata": {
        "id": "W8Kk6suE7POP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish/runs/train/exp2/weights/best.pt\"\n",
        "#weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5n_130epch.pt\"\n",
        "\n",
        "# Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà\n",
        "dataset_olympia_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/grav\"\n",
        "dataset_olympia_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/olympia/cont\"\n",
        "dataset_handai_grav = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/grav\"\n",
        "dataset_handai_cont = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_dataset_for_YOLO/test/handai/cont\""
      ],
      "metadata": {
        "id": "LY1JefL167J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "%pip install -qr requirements.txt\n",
        "display = utils.notebook_init()\n"
      ],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "5FWS5BJJ67MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "\n",
        "\n",
        "image_path = glob.glob(f\"{dataset_olympia_cont}/*\")\n",
        "start_index = 1\n",
        "end_index = 5\n",
        "\n",
        "class_names = {0: \"cont\", 1: \"grav\"}\n",
        "\n",
        "for i in range(start_index, end_index + 1):\n",
        "    img = image_path[i]\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" % (class_name, prob * 100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "    img_cv2 = cv2.resize(img_cv2, (640, int(img_cv2.shape[0] * 640 / img_cv2.shape[1])))  # Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height)) / 2\n",
        "    padding_y = (img_width - min(img_width, img_height)) / 2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "hzTyoHPw_U-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-GradCAM (Êñ∞„Éê„Éº„Ç∏„Éß„É≥)**"
      ],
      "metadata": {
        "id": "xtC0Ozyf_W7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5-GradCAM (2Âπ¥Ââç‰ª•Ââç„ÅÆ„Éê„Éº„Ç∏„Éß„É≥Â∞ÇÁî®)**\n",
        "https://colab.research.google.com/github/pooya-mohammadi/yolov5-gradcam/blob/master/main.ipynb#scrollTo=2z2oxEMRCBOk\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n"
      ],
      "metadata": {
        "id": "nshZUVxm_CrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###„É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø\n",
        "import torch\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
        "hubconf_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\"\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "model = torch.hub.load(hubconf_path, 'custom', model_path, source='local')\n",
        "print(model(torch.randn(1,3,640,640)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkMr1F8b26Kj",
        "outputId": "e66d68cd-dd36-4884-d3be-f70fbc70553d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5-iFish\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 190.6/190.6 kB 4.6 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 62.7/62.7 kB 24.8 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.40 smmap-5.0.1\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 6.2s, installed 1 package: ['gitpython>=3.1.30']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ 2269e2e Python-3.10.12 torch-2.1.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4.88041e+00, 1.35679e-01, 3.84668e+01,  ..., 5.65154e-08, 3.26122e-01, 6.49781e-01],\n",
            "         [1.33316e+01, 5.77327e-01, 3.85642e+01,  ..., 4.33717e-08, 3.43050e-01, 6.59148e-01],\n",
            "         [2.12945e+01, 8.89058e-01, 3.81220e+01,  ..., 1.40185e-07, 3.51219e-01, 6.28838e-01],\n",
            "         ...,\n",
            "         [5.55764e+02, 6.07017e+02, 2.99500e+02,  ..., 1.45832e-06, 6.80712e-01, 2.18833e-01],\n",
            "         [5.82885e+02, 6.07734e+02, 2.86606e+02,  ..., 1.20806e-07, 6.77674e-01, 1.65309e-01],\n",
            "         [6.18567e+02, 6.11243e+02, 2.79418e+02,  ..., 3.64277e-08, 6.29886e-01, 1.68246e-01]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "#!pip install -U opencv-python --q\n",
        "# restart runtime required??"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwfgTUYT67Sk",
        "outputId": "47369edd-906a-40b0-dcd1-b8486321cc0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/pooya-mohammadi/deep_utils.git\n",
            "  Cloning https://github.com/pooya-mohammadi/deep_utils.git to /tmp/pip-req-build-9bx19a2u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pooya-mohammadi/deep_utils.git /tmp/pip-req-build-9bx19a2u\n",
            "  Resolved https://github.com/pooya-mohammadi/deep_utils.git to commit 562e6af2dd13b34cfad3585838845dfd7fd72513\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.10/dist-packages (from deep-utils==1.3.10) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deep-utils==1.3.10) (2023.11.17)\n",
            "Building wheels for collected packages: deep-utils\n",
            "  Building wheel for deep-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deep-utils: filename=deep_utils-1.3.10-py3-none-any.whl size=535938 sha256=d62e69bbeb4c6ed14d967c4e89837b963b9b831d11535f3ab37d1561cd0d736b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fx512y9m/wheels/8f/0a/f4/5e2b92d9573699e3e30ce319a4b06218eb281695935d0b8b54\n",
            "Successfully built deep-utils\n",
            "Installing collected packages: deep-utils\n",
            "Successfully installed deep-utils-1.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWfeBZbD67Uz",
        "outputId": "b472dc32-c721-4f30-84f8-1f66aa332a20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 134 (delta 29), reused 36 (delta 13), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (134/134), 6.95 MiB | 28.45 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%cd /content/yolov5-gradcam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE5fpiWP67W-",
        "outputId": "8ed9b3c1-49ef-4019-ea8d-83030d03ec91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5-gradcam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "%matplotlib inline\n",
        "\n",
        "sys.path.append('/content/yolov5-gradcam')  # yolov5-gradcam„Éá„Ç£„É¨„ÇØ„Éà„É™„Çí„Éë„Çπ„Å´ËøΩÂä†\n",
        "\n",
        "from models.gradcam import YOLOV5GradCAM\n",
        "from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "\n",
        "import cv2\n",
        "from deep_utils import Box, split_extension\n",
        "\n",
        "# Set parameters directly\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/super_extend_for_YOLO_training/yolov5n-iFish_120epoch.pt\"\n",
        "#model_path = \"yolov5n.pt\"\n",
        "#img_path = \"/content/drive/MyDrive/AI_laboratory_course/goldenretriever-3724972_640.jpg\"\n",
        "img_path = \"/content/drive/MyDrive/AI_laboratory_course/puppies.jpg\"\n",
        "output_dir = \"out\"\n",
        "img_size = 640\n",
        "target_layer = \"model_23_cv3_act\" #„Éá„Éï„Ç©„É´„Éà„ÅØYOLOv5s„Å†„Åå„Å©„ÅÆ„É¢„Éá„É´„Åß„ÇÇÂêå„Åò„Å£„ÅΩ„ÅÑ\n",
        "method = \"gradcam\"\n",
        "device = \"cpu\"\n",
        "names = [\"cont\", \"grav\"]\n",
        "#names = None\n",
        "\n",
        "def get_res_img(bbox, mask, res_img):\n",
        "    mask = mask.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n",
        "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "    n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "    res_img = res_img / 255\n",
        "    res_img = cv2.add(res_img, n_heatmat)\n",
        "    res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.imshow\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "    res_img = Box.put_text(res_img, cls_name, (x1, y1))\n",
        "    return res_img\n",
        "\n",
        "def concat_images(images):\n",
        "    w, h = images[0].shape[:2]\n",
        "    width = w\n",
        "    height = h * len(images)\n",
        "    base_img = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "    for i, img in enumerate(images):\n",
        "        base_img[:, h * i:h * (i + 1), ...] = img\n",
        "    return base_img\n",
        "\n",
        "def main(img_path):\n",
        "    print(names)\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    tic = time.time()\n",
        "    masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "    print(\"total time:\", round(time.time() - tic, 4))\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]  # convert to bgr\n",
        "    images = [result]\n",
        "    # for i, mask in enumerate(masks):\n",
        "    #     res_img = result.copy()\n",
        "    #     bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "    #     res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "    #     res_img = put_text_box(bbox, cls_name, res_img)\n",
        "    #     images.append(res_img)\n",
        "\n",
        "\n",
        "    for i, mask in enumerate(masks):\n",
        "        res_img = result.copy()\n",
        "        bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "        res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "        res_img = put_text_box(bbox, cls_name, res_img)\n",
        "        show_image(res_img)  # Display each image individually\n",
        "    final_image = concat_images(images)\n",
        "    show_image(final_image)  # Display the image instead of saving it\n",
        "\n",
        "\n",
        "def folder_main(folder_path):\n",
        "    input_size = (img_size, img_size)\n",
        "    print('[INFO] Loading the model')\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    for item in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, item)\n",
        "        img = cv2.imread(img_path)\n",
        "        torch_img = model.preprocessing(img[..., ::-1])\n",
        "        if method == 'gradcam':\n",
        "            saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "        tic = time.time()\n",
        "        masks, logits, [boxes, _, class_names, _] = saliency_method(torch_img)\n",
        "        print(\"total time:\", round(time.time() - tic, 4))\n",
        "        result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "        result = result[..., ::-1]  # convert to bgr\n",
        "        images = [result]\n",
        "        for i, mask in enumerate(masks):\n",
        "            res_img = result.copy()\n",
        "            bbox, cls_name = boxes[0][i], class_names[0][i]\n",
        "            res_img, heat_map = get_res_img(bbox, mask, res_img)\n",
        "            res_img = put_text_box(bbox, cls_name, res_img)\n",
        "            images.append(res_img)\n",
        "        final_image = concat_images(images)\n",
        "        show_image(final_image)  # Display the image instead of saving it\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path)\n",
        "    else:\n",
        "        main(img_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXPh1IB4VqcM",
        "outputId": "d5fe233d-e65f-4cf7-e07c-6b3552fff1ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cont', 'grav']\n",
            "[INFO] Loading the model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3e2019f0cd56>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mfolder_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-3e2019f0cd56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Loading the model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOV5TorchObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtorch_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gradcam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5-gradcam/models/yolo_v5_object_detector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_weight, device, img_size, names, mode, confidence, iou_thresh, agnostic_nms)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miou_thresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magnostic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magnostic_nms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Model is loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5-gradcam/models/experimental.py\u001b[0m in \u001b[0;36mattempt_load\u001b[0;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfuse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fused or un-fused model in eval mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'DetectionModel' on <module 'models.yolo' from '/content/yolov5-gradcam/models/yolo.py'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSkqOCk1IZ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbaoVy-bIZ9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76nkcUUIzUey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AoBfT0SzUgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwDnNonIzUi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNTnFR0MzUkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UvEHekMzUnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lr2FoWajIZ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofa0v0XSIaBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPP8B8rbIaDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_dataset„Çí colab‰∏ä„ÅÆ„Éï„Ç©„É´„ÉÄ„Å´Â±ïÈñã\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 trainingÁî®„Éï„Ç©„É´„ÉÄ„Çí‰ΩúÊàê**\n",
        "\n",
        "dataset„Çítrain„Å®val„Å´ÂàÜ„Åë„Çã\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLO„Éï„Ç©„É´„ÉÄ„Å´„Åô„Åß„Å´Â±ïÈñã„Åï„Çå„Å¶„ÅÑ„Çã„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁî®„ÅÑ„Çã\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_test„Å´ÂêçÂâç„Åå‰∏ÄËá¥„Åô„Çãtxt„Éï„Ç°„Ç§„É´„ÇíÊäú„ÅçÂá∫„Åô\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "\n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# „Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ„Éë„Çπ\n",
        "directory_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont/images_cropped\"\n",
        "\n",
        "# „Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆjpg„Éï„Ç°„Ç§„É´„Çí„É™„Çπ„Éà„Ç¢„ÉÉ„Éó\n",
        "jpg_files = [f for f in os.listdir(directory_path) if f.endswith('.JPG')]\n",
        "\n",
        "# 1„Å§„ÅÆjpg„Éï„Ç°„Ç§„É´„ÇíË°®Á§∫\n",
        "if len(jpg_files) > 0:\n",
        "    file_to_display = jpg_files[0]  # 1„Å§ÁõÆ„ÅÆ„Éï„Ç°„Ç§„É´„ÇíË°®Á§∫\n",
        "    file_path = os.path.join(directory_path, file_to_display)\n",
        "    display(Image(filename=file_path))\n",
        "else:\n",
        "    print(\"ÊåáÂÆö„Åï„Çå„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„Å´jpg„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ\")\n"
      ],
      "metadata": {
        "id": "OiVhW7PAZhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆpreprocess**"
      ],
      "metadata": {
        "id": "RPDTAk_iNfY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5Âêë„Åë„Å´GroupKfold„Åß‰ªïÂàÜ„Åë„Çâ„Çå„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åå„ÅÇ„Çã„ÅÆ„Åß„Åì„Çå„ÇíÁî®„ÅÑ„Çã„ÄÄ„ÄÄ#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels\n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### „Éï„Ç©„É´„ÉÄ„ÅÆ„É™„Çª„ÉÉ„Éà #######\n",
        "\n",
        "# # MobileNetÁî®„Å´224pxÂõõÊñπ„Å´ÊàêÂΩ¢„Åó„Å¶„Åä„Åè\n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "# if os.path.exists(dst_folder):\n",
        "#     shutil.rmtree(dst_folder)\n",
        "# os.makedirs(f\"{dst_folder}/train\")\n",
        "# os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #Âá¶ÁêÜÊôÇÈñì„ÅÆË®àÊ∏¨\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:\n",
        "          print(i)\n",
        "          print(in_path + '/' + i)\n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "\n",
        "          #Âàá„ÇäÂèñ„Å£„ÅüÁîªÂÉè„ÇíË°®Á§∫\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #Âá¶ÁêÜ„Åô„ÇãDirectory„ÅÆË®≠ÂÆö\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #„Åì„Åì„Å´„Éï„Ç©„É´„ÉÄÁï™Âè∑„ÇíË®òËºâ„Åô„Çã (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ÂÖÉÁîªÂÉè„Éï„Ç©„É´„ÉÄ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#ÂÖÉÁîªÂÉè„Éï„Ç©„É´„ÉÄ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#‰øùÂ≠òÂÖà„Éï„Ç©„É´„ÉÄ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #ÁîªÂÉè„ÅÆ„Çµ„Ç§„Ç∫\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    Èï∑ÊñπÂΩ¢„ÅÆÂÖÉÁîªÂÉè„ÇíÈï∑Ëæ∫„Çí1Ëæ∫„Å®„Åô„ÇãÊ≠£ÊñπÂΩ¢„Å´Ë≤º„Çä‰ªò„Åë„ÄÅÁ©∫ÁôΩ„ÇíÈªí„ÅèÂ°ó„Çä„Å§„Å∂„Åô\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#Â∞ëÊï∞„ÅÆÁîªÂÉè„ÇíÂèØË¶ñÂåñ\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            #ÊôÆÈÄö„ÅØ„Åì„Å°„Çâ„Çí‰Ωø„ÅÜ\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##„Éë„É©„É°„Éº„Çø„ÅÆL1„Éé„É´„É†„ÅÆÁµ∂ÂØæÂÄ§„ÇíÊêçÂ§±Èñ¢Êï∞„Å´Ë∂≥„Åô##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##„Éë„É©„É°„Éº„Çø„ÅÆL2„Éé„É´„É†„ÅÆ‰∫å‰πó„ÇíÊêçÂ§±Èñ¢Êï∞„Å´Ë∂≥„Åô##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()\n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#„Ç∞„É©„Éï„ÅÆÂ§ñÂΩ¢„Çí‰ΩúÊàê\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # ÂêÑ„Éó„É≠„ÉÉ„Éà„ÅÆËâ≤\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "\n",
        "        #„Åù„Çå„Åû„Çå„ÅÆÁîªÂÉè„Å´„Åä„Åë„ÇãÈôΩÊÄß„ÅÆÁ¢∫Áéá„Å´„Å§„ÅÑ„Å¶„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "\n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "\n",
        "    #„Åù„Çå„Åû„Çå„ÅÆÁîªÂÉè„Å´„Åä„Åë„ÇãÈôΩÊÄß„ÅÆÁ¢∫Áéá„Å´„Å§„ÅÑ„Å¶„É™„Çπ„Éà„Çí‰ΩúÊàê\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU‰ΩøÁî®\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#ÊêçÂ§±Èñ¢Êï∞„ÇíÂÆöÁæ©\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min')\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_path„ÅØÁîªÂÉè„ÇíÊ†ºÁ¥ç„Åó„Å¶„ÅÑ„Çã„Éï„Ç©„É´„ÉÄ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "eI2_SlJUcqDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3**"
      ],
      "metadata": {
        "id": "BU8Bw69eN4gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "91wReBK3OCjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model È£õ„Å∞„Åó„Å¶‰∏ã„Åï„ÅÑ\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆË™≠„ÅøËæº„Åø\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "##################\n",
        "## Define GradCAM ##\n",
        "##################\n",
        "\n",
        "def compute_gradcam(model, image_tensor, target_class=None):\n",
        "    model.eval()\n",
        "\n",
        "    features = None\n",
        "    grads = None\n",
        "\n",
        "    def hook_feature(module, input, output):\n",
        "        nonlocal features\n",
        "        features = output\n",
        "\n",
        "    def hook_grad(module, grad_in, grad_out):\n",
        "        nonlocal grads\n",
        "        grads = grad_out[0]\n",
        "\n",
        "    final_conv = model.blocks[-1]  # „Åì„ÅÆÈÉ®ÂàÜ„ÅØ„É¢„Éá„É´„ÅÆÊßãÈÄ†„Å´„Çà„ÇäË™øÊï¥„ÅåÂøÖË¶Å\n",
        "    final_conv.register_forward_hook(hook_feature)\n",
        "    final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax().item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        activations = features.cpu().numpy()\n",
        "        grad_values = grads.cpu().numpy()\n",
        "        weights = np.mean(grad_values, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[0, i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (image_tensor.shape[2], image_tensor.shape[3]))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "\n",
        "    # Memory clean-up\n",
        "    del features, grads, activations, grad_values, weights\n",
        "    torch.cuda.empty_cache()  # If you're using CUDA\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "hzvNpNeravOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dlib„ÇíÁî®„ÅÑ„ÅüÁúºÂë®Âõ≤Êäú„ÅçÂá∫„Åó\n",
        "\n",
        "„Åì„Å°„Çâ„ÅÆÊñπ„Ååmediapipe„Çà„ÇäÊ§úÂá∫Áéá„ÄÅÁ≤æÂ∫¶„ÅåÈ´ò„ÅÑ"
      ],
      "metadata": {
        "id": "mqchGqI3SZT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib --q\n",
        "!pip install opencv-python --q\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# „Ç´„Çπ„Ç±„Éº„Éâ„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "# „Ç´„Çπ„Ç±„Éº„ÉâÂàÜÈ°ûÂô®„ÅÆÁâπÂæ¥ÈáèÂèñÂæó\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n"
      ],
      "metadata": {
        "id": "zq_FRvcq0Ilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "# mismatched = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = mismatched\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 30))\n",
        "\n",
        "\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "counter_0_overall_correct = 0\n",
        "counter_1 = 0  # img_label == pred.item()\n",
        "counter_2 = 0  # img_label != pred_item\n",
        "counter_3_same_pred_and_mask_correct = 0  # pred.item() == mask_diagnosis and img_label == pred.item()\n",
        "counter_3_same_pred_and_mask_incorrect = 0  # pred.item() == mask_diagnosis and img_lanbel != pred.label\n",
        "counter_4_diff_pred_and_mask_model_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_4_diff_pred_and_mask_mask_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_5_matched_pred_1_correct = 0\n",
        "counter_5_matched_pred_0_correct = 0\n",
        "counter_5_matched_pred_1_incorrect = 0\n",
        "counter_5_matched_pred_0_incorrect = 0\n",
        "misclassified_indices, mismatched_indices = [], []\n",
        "mismatched_indices_pred_1, mismatched_indices_pred_0 = [], []\n",
        "mismatched_indices_pred_1_model_correct, mismatched_indices_pred_0_model_correct = [], []\n",
        "mismatched_indices_pred_1_mask_correct, mismatched_indices_pred_0_mask_correct = [], []\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "\n",
        "def crop_bilateral(in_path):\n",
        "    img = cv2.imread(in_path)\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    mask = np.zeros_like(grayscale_img)\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "    for (x, y, w, h) in eye_list:\n",
        "        mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "    return mask\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        # if img_label == 1:  #sort by label\n",
        "        # #if img_label == 0:\n",
        "        # #if img_label == 0 or img_label == 1:\n",
        "            #GradCAM\n",
        "            pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "            numpy_image = np.array(pilr_image)\n",
        "            cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "            image_tensor = test_data_transforms(pilr_image)\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "            output = model_ft(image_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = nn.Softmax(dim=1)(output)[0][1].cpu().detach().item()\n",
        "\n",
        "            cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "            cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "\n",
        "            #Ê≥®ÁõÆÈÉ®‰Ωç„ÇíÔºíÂÄ§Âåñ„Åó„Å¶Ë°®Á§∫\n",
        "            mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "            mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "            mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            #HaarCascade„Å´„Çà„ÇãÁúºÂë®Âõ≤Êäú„ÅçÂá∫„Åó\n",
        "            periocular_mask = crop_bilateral(img_path)\n",
        "\n",
        "            #GradCAM„ÄÅÁúºÂë®Âõ≤È†òÂüü„ÅÆÂÖ±ÈÄöÁÇπ„Çí„Éû„Çπ„ÇØÂåñ„Åô„Çã\n",
        "            common_mask_0 = cv2.bitwise_and(mask_0_resized, periocular_mask)\n",
        "            common_mask_1 = cv2.bitwise_and(mask_1_resized, periocular_mask)\n",
        "\n",
        "            area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "            area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "            mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "\n",
        "\n",
        "            # Convert the common masks to 3-channel images for visualization\n",
        "            common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "            common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "            heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "            heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "\n",
        "            heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "            overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "            # 1. 3„ÉÅ„É£„É≥„Éç„É´„ÅÆÁîªÂÉè„Å´Â§âÊèõ\n",
        "            periocular_colored_mask = cv2.cvtColor(periocular_mask, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            # 2. overlayed_image_1„Å´periocular_mask„Çí„Ç™„Éº„Éê„Éº„É¨„Ç§\n",
        "            overlayed_image_1_with_mask = cv2.addWeighted(overlayed_image_1, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # 3. overlayed_image_0„Å´periocular_mask„Çí„Ç™„Éº„Éê„Éº„É¨„Ç§\n",
        "            overlayed_image_0_with_mask = cv2.addWeighted(overlayed_image_0, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # ÁµêÊûú„ÅÆË°®Á§∫\n",
        "            result_with_mask = np.hstack([cv2_image, overlayed_image_1_with_mask, overlayed_image_0_with_mask, common_mask_1, common_mask_0])\n",
        "\n",
        "            # Inside your loop\n",
        "            if img_label == pred.item():\n",
        "                counter_0_overall_correct += 1\n",
        "\n",
        "            if pred.item() == mask_diagnosis:\n",
        "                counter_1 += 1\n",
        "                if img_label == pred.item():\n",
        "                    counter_3_same_pred_and_mask_correct += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_correct += 1\n",
        "                        print(\"match_pred_1_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_correct\")\n",
        "                elif img_label != pred.item():\n",
        "                    counter_3_same_pred_and_mask_incorrect += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_incorrect += 1\n",
        "                        print(\"match_pred_1_incorrect\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_incorrect\")\n",
        "\n",
        "            if pred.item() != mask_diagnosis:\n",
        "                counter_2 += 1\n",
        "                mismatched_indices.append(i)\n",
        "                #print(f\"pred: {pred.item()}, mask: {mask_diagnosis}\")\n",
        "                if img_label == pred.item():\n",
        "                    counter_4_diff_pred_and_mask_model_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_model_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_model_correct\")\n",
        "                elif img_label == mask_diagnosis:\n",
        "                    counter_4_diff_pred_and_mask_mask_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_mask_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_mask_correct\")\n",
        "\n",
        "            # ‰∫àÊ∏¨ÁµêÊûú„Å®ÂÆüÈöõ„ÅÆ„É©„Éô„É´„ÇíÊØîËºÉ„Åó„Å¶„ÄÅTP, TN, FP, FN„ÅÆ„Ç´„Ç¶„É≥„Éà„ÇíÊõ¥Êñ∞\n",
        "            if img_label == 1 and pred.item() == 1:\n",
        "                TP += 1\n",
        "            elif img_label == 0 and pred.item() == 0:\n",
        "                TN += 1\n",
        "            elif img_label == 0 and pred.item() == 1:\n",
        "                FP += 1\n",
        "            elif img_label == 1 and pred.item() == 0:\n",
        "                FN += 1\n",
        "\n",
        "            print(f\"{i}\")\n",
        "            print(f\"{os.path.basename(img_path)}\")\n",
        "            print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "            print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "            ###GradCAMÁîªÂÉè„ÇíË°®Á§∫ÔºàË°®Á§∫„Åó„Å™„ÅÑ„Å®„Åç„ÅØ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà„Åô„ÇãÔºâ\n",
        "            #cv2_imshow(result_with_mask)\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "            if pred.item() != img_label:\n",
        "                misclassified_indices.append(i)\n",
        "\n",
        "            # „É°„É¢„É™Ëß£Êîæ„ÅÆ„Åü„ÇÅ„ÅÆÊìç‰Ωú\n",
        "            del pilr_image, numpy_image, cv2_image, image_tensor, output, pred\n",
        "            del cam_class_0, cam_class_1, mask_0, mask_1, mask_0_resized, mask_1_resized\n",
        "            del heatmap_0, heatmap_1, heatmap_0_resized, heatmap_1_resized, overlayed_image_0, overlayed_image_1\n",
        "            del common_mask_0, common_mask_1, result_with_mask\n",
        "\n",
        "            # ÊòéÁ§∫ÁöÑ„Å´„Ç¨„Éô„Éº„Ç∏„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„ÇíÂÆüË°å\n",
        "            gc.collect()\n",
        "\n",
        "total_images = len(image_indices)\n",
        "ratio_0 = counter_0_overall_correct*100/total_images\n",
        "ratio_1 = counter_1*100/total_images\n",
        "ratio_2 = counter_2*100/total_images\n",
        "print(f\"Model accuracy: {ratio_0:.3f}% ({counter_0_overall_correct}/{total_images})\")\n",
        "print(f\"Model and GradCAM judgement match: {ratio_1:.3f}% ({counter_1}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_3_same_pred_and_mask_correct*100/counter_1:.3f}% ({counter_3_same_pred_and_mask_correct}/{counter_1})\")\n",
        "print(f\"Model and GradCAM judgement do not match: {ratio_2:.3f}% ({counter_2}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_4_diff_pred_and_mask_model_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_model_correct}/{counter_2})\")\n",
        "print(f\"    Mask accuracy: {counter_4_diff_pred_and_mask_mask_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_mask_correct}/{counter_2})\")\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")\n",
        "print(f\"mismatched_indices: {mismatched_indices}\")\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")\n",
        "\n",
        "# ÊÑüÂ∫¶„Å®ÁâπÁï∞Â∫¶„ÅÆË®àÁÆó\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m-m3ZFMou287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have defined your lists and calculated their lengths, for example:\n",
        "pred_0_model_correct_count = len(mismatched_indices_pred_0_model_correct)\n",
        "pred_0_mask_correct_count = len(mismatched_indices_pred_0_mask_correct)\n",
        "pred_1_model_correct_count = len(mismatched_indices_pred_1_model_correct)\n",
        "pred_1_mask_correct_count = len(mismatched_indices_pred_1_mask_correct)\n",
        "\n",
        "# 1. Create a matrix of the counts:\n",
        "matrix = [\n",
        "    [pred_1_model_correct_count, pred_1_mask_correct_count],\n",
        "    [pred_0_model_correct_count, pred_0_mask_correct_count],\n",
        "]\n",
        "\n",
        "# 2. Use seaborn's heatmap to visualize the matrix:\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(font_scale=1.2) # for label size\n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"model_correct\", \"mask_incorrect\"],\n",
        "            yticklabels=[\"model_pred_1\", \"model_pred_0\"])\n",
        "plt.title(\"In case model and mask diagnosis not consistent\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-HDn_QLB690x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_model_pred_0_inconsistent = pred_0_model_correct_count/(pred_0_model_correct_count+pred_0_mask_correct_count)\n",
        "Accuracy_model_pred_1_inconsistent = pred_1_model_correct_count/(pred_1_model_correct_count+pred_1_mask_correct_count)\n",
        "print(f\"Accuracy_model_pred_1_inconsistent: {Accuracy_model_pred_1_inconsistent*100:.2f}% ({pred_1_model_correct_count}/{pred_1_model_correct_count+pred_1_mask_correct_count})\")\n",
        "print(f\"Accuracy_model_pred_0_inconsistent {Accuracy_model_pred_0_inconsistent*100:.2f}% ({pred_0_model_correct_count}/{pred_0_model_correct_count+pred_0_mask_correct_count})\")\n",
        "Accuracy_model_pred_0_consistent = counter_5_matched_pred_0_correct/(counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect)\n",
        "Accuracy_model_pred_1_consistent = counter_5_matched_pred_1_correct/(counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect)\n",
        "print(f\"Accuracy_model_pred_1_consistent: {Accuracy_model_pred_1_consistent*100:.2f}% ({counter_5_matched_pred_1_correct}/{counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect})\")\n",
        "print(f\"Accuracy_model_pred_0_consistent {Accuracy_model_pred_0_consistent*100:.2f}% ({counter_5_matched_pred_0_correct}/{counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect})\")\n",
        "\n",
        "#„É¢„Éá„É´„Å®Ê≥®ÁõÆÁÇπ„ÅåÂêàËá¥„Åô„ÇãÁóá‰æã„Å´ÈôêÂÆö„Åó„ÅüÂ†¥Âêà\n",
        "TP = counter_5_matched_pred_1_correct\n",
        "FP = counter_5_matched_pred_1_incorrect\n",
        "FN = counter_5_matched_pred_0_incorrect\n",
        "TN = counter_5_matched_pred_0_correct\n",
        "\n",
        "# ÊÑüÂ∫¶„Å®ÁâπÁï∞Â∫¶„ÅÆË®àÁÆó\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")"
      ],
      "metadata": {
        "id": "abEcpyatIMFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**„Éì„Éá„Ç™Ëß£Êûê**"
      ],
      "metadata": {
        "id": "Qkcv1Poa8iyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the MP4 file and the directory to save frames\n",
        "video_path = \"/content/drive/MyDrive/Deep_learning/GravAI_iOS/test_movie/Grav_1.mp4\"\n",
        "save_path = \"/content/temp\"\n",
        "\n",
        "# Check if the save directory exists; if so, remove it\n",
        "if os.path.exists(save_path):\n",
        "    os.rmtree(save_path)\n",
        "\n",
        "# Create the directory\n",
        "os.mkdir(save_path)\n",
        "\n",
        "# Load the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frame counter\n",
        "count = 0\n",
        "\n",
        "# Check if the video is opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Read until video is completed\n",
        "while cap.isOpened():\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        # Save a frame every 10 frames\n",
        "        if count % 10 == 0:\n",
        "            frame_name = os.path.join(save_path, f\"frame_{count}.jpg\")\n",
        "            cv2.imwrite(frame_name, frame)\n",
        "            print(f\"Saved {frame_name}\")\n",
        "\n",
        "        count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "WI0X3akb7fLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iccKxhGN7fRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVESbPIt6WJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbE-WDwM7fUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kkcpd3f67fXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1SgwLyvRaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Âè©„ÅçÂè∞Ôºà„Éú„ÉÑÔºâ„ÅÆ„Ç≥„Éº„ÉâÈÅî\n",
        "\n",
        "Google mediapipe„ÅØÈ°îÈù¢„É°„ÉÉ„Ç∑„É•„ÅåÊ§úÂá∫„Åß„Åç„Å™„ÅÑÁóá‰æã„ÅåÂ§ö„ÅÑ„Åü„ÇÅÊé°Áî®„Åõ„Åö"
      ],
      "metadata": {
        "id": "7GOZBL9t-pOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# # Ë™≠„ÅøËæº„ÇÄÁîªÂÉè„Éï„Ç°„Ç§„É´\n",
        "# IMAGE_FILES = val_list[0]\n",
        "\n",
        "# # Google Mediapipe periocular landmarks #\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.5) as face_mesh:\n",
        "\n",
        "#     image = cv2.imread(IMAGE_FILES)\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       exit(\"No landmarks detected\")\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#         466, 388, 387, 386, 385, 384, 398,\n",
        "#         263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     mask = np.zeros_like(image)\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#         points_right = []\n",
        "#         points_left = []\n",
        "\n",
        "#         for idx in landmarks_list_right:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_right.append([loc_x, loc_y])\n",
        "\n",
        "#         for idx in landmarks_list_left:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_left.append([loc_x, loc_y])\n",
        "\n",
        "#         cv2.fillPoly(mask, [np.array(points_right)], (255,255,255))\n",
        "#         cv2.fillPoly(mask, [np.array(points_left)], (255,255,255))\n",
        "\n",
        "#     masked_image = cv2.bitwise_and(mask, np.ones_like(image) * 255)\n",
        "#     cv2_imshow(masked_image)\n"
      ],
      "metadata": {
        "id": "vkojIsOFugFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe opencv-python --q\n",
        "\n",
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# #########################################################\n",
        "# # Google Mediapipe periocular landmarks ‚Äªlandmark„ÇíÊ§úÂá∫„Åß„Åç„Å™„ÅÑÁóá‰æã„Åå„ÅÇ„Çã#\n",
        "# #########################################################\n",
        "\n",
        "# mp_drawing = mp.solutions.drawing_utils\n",
        "# mp_drawing_styles = mp.solutions.drawing_styles\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# # For static images:\n",
        "\n",
        "# import glob\n",
        "# val_list = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid/*\")[0:100]\n",
        "# IMAGE_FILES = val_list\n",
        "# print(IMAGE_FILES)\n",
        "\n",
        "# drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.15,\n",
        "#     min_tracking_confidence=0.1) as face_mesh:\n",
        "\n",
        "#   for idx, file in enumerate(IMAGE_FILES):\n",
        "#     print(file)\n",
        "#     image = cv2.imread(file)\n",
        "#     # Convert the BGR image to RGB before processing.\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#     # Print and draw face mesh landmarks on the image.\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       print(\"no landmark detected\")\n",
        "#       continue\n",
        "#     annotated_image = image.copy()\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246,\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#     466, 388, 387, 386, 385, 384, 398,\n",
        "#     263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#       for idx in landmarks_list_right:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (255,0,255), 2)\n",
        "\n",
        "#       for idx in landmarks_list_left:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (0,255,255), 2)\n",
        "\n",
        "#     cv2_imshow(annotated_image)\n"
      ],
      "metadata": {
        "id": "3yqasLAdGt9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "##############################################\n",
        "# Inference MobileNetV3 with GradCAM ‚Äª„É°„É¢„É™ÂØæÁ≠ñ„Éê„Éº„Ç∏„Éß„É≥#\n",
        "##############################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1, heatmap_0, heatmap_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "kghHA3ksgYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with ÈªíÂ°ó„ÇäGradCAM_ÁôΩÈªí„Éê„Éº„Ç∏„Éß„É≥ËøΩÂä† #\n",
        "####################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Êó¢Â≠ò„ÅÆ„Éû„Çπ„ÇØ„ÇíÂèçËª¢„Åï„Åõ„Çã\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # „Éû„Çπ„ÇØ„Çí„É™„Çµ„Ç§„Ç∫\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # ÈªíÂ°ó„ÇäÈÉ®ÂàÜ‰ª•Â§ñ„ÇíÁôΩ„ÅßÂ°ó„Çä„Å§„Å∂„Åô\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2Ê¨°ÂÖÉ„ÅÆÁôΩ„Éû„Çπ„ÇØ„Çí3Ê¨°ÂÖÉ„Å´Â§âÊèõ\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0])\n",
        "        cv2_imshow(result)\n"
      ],
      "metadata": {
        "id": "0eu241VBwUdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with ÈªíÂ°ó„ÇäGradCAM #\n",
        "####################################\n",
        "\"\"\"\n",
        "GradCAM„ÅÆÊ≥®ÁõÆÁÇπ„ÇíÈªíÂ°ó„Çä„ÅßÈö†„Åó„Å¶inference„Åó„Åü„Å®„Åç„Å´„ÄÅ„Å©„ÅÆ„Çà„ÅÜ„Å´ÁµêÊûú„ÅåÂ§â„Çè„Çã„Åã„ÇíÁ¢∫Ë™ç\n",
        "ÔºàÁµêÊûú„Å®„Åó„Å¶„ÅÇ„Åæ„ÇäÂèÇËÄÉ„Å´„Å™„Çâ„Å™„Åï„Åù„ÅÜÔºâ\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Resize the masks\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # Èªí„ÅèÂ°ó„Çä„Å§„Å∂„Åó„ÅüÁîªÂÉè„ÇíTensor„Å´Â§âÊèõ\n",
        "        blacked_out_tensor_0 = test_data_transforms(Image.fromarray(blacked_out_image_0)).unsqueeze(0).to(device)\n",
        "        blacked_out_tensor_1 = test_data_transforms(Image.fromarray(blacked_out_image_1)).unsqueeze(0).to(device)\n",
        "\n",
        "        # „É¢„Éá„É´„ÅßË©ï‰æ°\n",
        "        output_0 = model_ft(blacked_out_tensor_0)\n",
        "        _, pred_0 = torch.max(output_0, 1)\n",
        "\n",
        "        prob_0 = nn.Softmax(dim=1)(output_0)\n",
        "        prob_0 = prob_0[0][1].cpu().detach()\n",
        "        prob_0 = \"{:.3f}\".format(prob_0.item())\n",
        "\n",
        "        output_1 = model_ft(blacked_out_tensor_1)\n",
        "        _, pred_1 = torch.max(output_1, 1)\n",
        "\n",
        "        prob_1 = nn.Softmax(dim=1)(output_1)\n",
        "        prob_1 = prob_1[0][1].cpu().detach()\n",
        "        prob_1 = \"{:.3f}\".format(prob_1.item())\n",
        "\n",
        "        # ÁµêÊûú„ÇíÂá∫Âäõ\n",
        "        print(f\"Blacked-out image (Class 1): prob: {prob_1}, pred: {pred_1.item()}\")\n",
        "        print(f\"Blacked-out image (Class 0): prob: {prob_0}, pred: {pred_0.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "wCvSB8X5OOkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAM„ÅßÊèêÁ§∫„Åï„Çå„ÅüÈñ¢ÂøÉÈ†òÂüü„ÅÆ„Éû„Çπ„ÇØ„Å®„ÄÅHaarCascade„ÅßÊ§úÂá∫„Åï„Çå„ÅüÁúºÂë®Âõ≤ÔºàÁ∏¶„ÅØ„Åù„ÅÆ„Åæ„Åæ„ÄÅÊ®™„ÅØÂπÖ„ÇíÂçäÂàÜ„Å´„Åó„ÅüÈï∑ÊñπÂΩ¢Ôºâ„Å®„ÅÆÈáçË§áÈ†òÂüü„ÇíË®àÁÆó„Åô„Çã„ÄÇ\n",
        "GradCAM„Å´„Å§„ÅÑ„Å¶„ÅØGrav„Å®Âà§ÂÆö„Åô„ÇãÂ†¥Âêà„Å®Cont„Å®Âà§ÂÆö„Åô„ÇãÂ†¥Âêà„Å®„Åß‰∏°Êñπ„Å®„ÇÇË®àÁÆó„ÇíË°å„ÅÜ„ÄÇ\n",
        "ÈáçË§áÈ†òÂüü„ÅåGrav>Cont„Åß„ÅÇ„Çå„Å∞mask_diagnosis = \"Grav\"„ÄÅÈáçË§áÈ†òÂüü„ÅåGrav<Cont„Åß„ÅÇ„Çå„Å∞mask_diagnosis = \"Cont\"„Å®„Åô„Çã„ÄÇ\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Êó¢Â≠ò„ÅÆ„Éû„Çπ„ÇØ„ÇíÂèçËª¢„Åï„Åõ„Çã\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # „Éû„Çπ„ÇØ„Çí„É™„Çµ„Ç§„Ç∫\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####ÁúºÂë®Âõ≤„ÅÆ„Éû„Çπ„ÇØ„Çí‰ΩúÊàê„Åô„Çã\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('ÁõÆ„Åå' + str(len(eye_list)) + 'ÂÄãÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # „É™„Çπ„Éà„ÇíNumPy„ÅÆÈÖçÂàó„Å´Â§âÊèõ\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPy„ÅÆË°®Á§∫„Ç™„Éó„Ç∑„Éß„É≥„ÇíÂ§âÊõ¥\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####„Åì„Åì„Åæ„Åß\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # ÈªíÂ°ó„ÇäÈÉ®ÂàÜ‰ª•Â§ñ„ÇíÁôΩ„ÅßÂ°ó„Çä„Å§„Å∂„Åô\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2Ê¨°ÂÖÉ„ÅÆÁôΩ„Éû„Çπ„ÇØ„Çí3Ê¨°ÂÖÉ„Å´Â§âÊèõ\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "Mtc7lBGOdha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAM„ÅßÊèêÁ§∫„Åï„Çå„ÅüÈñ¢ÂøÉÈ†òÂüü„ÅÆ„Éû„Çπ„ÇØ„Å®„ÄÅHaarCascade„ÅßÊ§úÂá∫„Åï„Çå„ÅüÁúºÂë®Âõ≤ÔºàÁ∏¶„ÅØ„Åù„ÅÆ„Åæ„Åæ„ÄÅÊ®™„ÅØÂπÖ„ÇíÂçäÂàÜ„Å´„Åó„ÅüÈï∑ÊñπÂΩ¢Ôºâ„Å®„ÅÆÈáçË§áÈ†òÂüü„ÇíË®àÁÆó„Åô„Çã„ÄÇ\n",
        "GradCAM„Å´„Å§„ÅÑ„Å¶„ÅØGrav„Å®Âà§ÂÆö„Åô„ÇãÂ†¥Âêà„Å®Cont„Å®Âà§ÂÆö„Åô„ÇãÂ†¥Âêà„Å®„Åß‰∏°Êñπ„Å®„ÇÇË®àÁÆó„ÇíË°å„ÅÜ„ÄÇ\n",
        "ÈáçË§áÈ†òÂüü„ÅåGrav>Cont„Åß„ÅÇ„Çå„Å∞mask_diagnosis = \"Grav\"„ÄÅÈáçË§áÈ†òÂüü„ÅåGrav<Cont„Åß„ÅÇ„Çå„Å∞mask_diagnosis = \"Cont\"„Å®„Åô„Çã„ÄÇ\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Êó¢Â≠ò„ÅÆ„Éû„Çπ„ÇØ„ÇíÂèçËª¢„Åï„Åõ„Çã\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # „Éû„Çπ„ÇØ„Çí„É™„Çµ„Ç§„Ç∫\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####ÁúºÂë®Âõ≤„ÅÆ„Éû„Çπ„ÇØ„Çí‰ΩúÊàê„Åô„Çã\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('ÁõÆ„Åå' + str(len(eye_list)) + 'ÂÄãÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # „É™„Çπ„Éà„ÇíNumPy„ÅÆÈÖçÂàó„Å´Â§âÊèõ\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPy„ÅÆË°®Á§∫„Ç™„Éó„Ç∑„Éß„É≥„ÇíÂ§âÊõ¥\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####„Åì„Åì„Åæ„Åß\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # ÈªíÂ°ó„ÇäÈÉ®ÂàÜ‰ª•Â§ñ„ÇíÁôΩ„ÅßÂ°ó„Çä„Å§„Å∂„Åô\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2Ê¨°ÂÖÉ„ÅÆÁôΩ„Éû„Çπ„ÇØ„Çí3Ê¨°ÂÖÉ„Å´Â§âÊèõ\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "ap-a6G9ou27W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "####. HaarCascade„ÅÆ„ÉÜ„Çπ„ÉàÁî® #####\n",
        "###########################\n",
        "\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def crop_bilateral(in_path, class_num, showImage=True):\n",
        "#     img_resized_list, side_list = [], []\n",
        "\n",
        "#     img = cv2.imread(in_path)\n",
        "#     img2 = img.copy()\n",
        "\n",
        "#     # Convert to grayscale for eye detection\n",
        "#     grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "#     # Create a black mask of the same size as the grayscale image\n",
        "#     mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "#     # Detect eyes\n",
        "#     eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "#     print(\"\\nimage path = \", in_path)\n",
        "\n",
        "#     if len(eye_list) >= 1:\n",
        "#         print('ÁõÆ„Åå' + str(len(eye_list)) + 'ÂÄãÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü')\n",
        "#         for (x, y, w, h) in eye_list:\n",
        "#             # Fill the detected eye region with value 255\n",
        "#             mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "#     else:\n",
        "#         print(\"no eye detected\")\n",
        "\n",
        "#     print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "#     # Show the mask if required using matplotlib for inline display in Colab\n",
        "#     if showImage:\n",
        "#         plt.imshow(mask, cmap='gray')\n",
        "#         plt.axis('off')  # Hide axis\n",
        "#         plt.show()\n",
        "\n",
        "#     return mask\n",
        "\n",
        "# # Assuming you have initialized eye_cascade before\n",
        "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "# # Test\n",
        "# # Assuming val_list and class_num are previously defined\n",
        "# test_path = val_list[0]\n",
        "# crop_bilateral(test_path, class_num=0)\n"
      ],
      "metadata": {
        "id": "0G6x2rv9WnIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Interference MobileNetV3 (GradCAM„Å™„Åó„Éê„Éº„Ç∏„Éß„É≥)#\n",
        "########################################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, test_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0) #val_dataset„Çí1Êûö„Åö„Å§„Å´„Åó„Å¶test_loadeer„Å´Ê†ºÁ¥ç\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      #target = target.squeeze(1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "\n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "      probs.append(float(prob)) #‰∫àÊ∏¨Á¢∫Áéá\n",
        "      preds.append(int(pred))  #‰∫àÊ∏¨ÁµêÊûú\n",
        "      targets.append(int(target)) #„É©„Éô„É´\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference MobileNet3"
      ],
      "metadata": {
        "id": "XvjkKZez_IrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # Import garbage collector module\n",
        "import os\n",
        "\n",
        "######################################\n",
        "# Inference MobileNetV3 ‚ÄªGradCAM„Å™„Åó#\n",
        "######################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "paths, targets, probs, preds = [], [], [], []\n",
        "\n",
        "image_indices = list(range(0, 665))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        paths.append(str(os.path.basename(img_path)))\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "JFL4WogdahVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'paths': paths,\n",
        "    'Target': y_label,\n",
        "    'Probability': y_prob,\n",
        "    'Prediction': y_pred\n",
        "})\n",
        "pd.set_option('display.max_rows', 700)\n",
        "\n",
        "file_path = \"/content/result_list.csv\"\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hbDCHbieT3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "itGmZ613_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# #################################################\n",
        "# threshold = 0.5 #Âà§ÂÆöÂü∫Ê∫ñ„ÄÇ„Åì„Åì„ÅØÂÖà„Å´ÂÖ•Âäõ„Åó„Å¶„Åä„Åè\n",
        "# #################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "# Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "# Compute the ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc_score(y_label, y_prob):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "# plt.plot(fpr, tpr, marker='o')\n",
        "# plt.xlabel('FPR')\n",
        "# plt.ylabel('TPR')\n",
        "# plt.grid()\n",
        "# print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# confusion matrix„Çíheatmap„ÅßË°®Á§∫\n",
        "cm = confusion_matrix(y_label, y_pred, labels=[1, 0])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['TED', 'control'],\n",
        "            yticklabels=['TED', 'control'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CAhawNTE8k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n"
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5„Éà„É¨„Éº„Éã„É≥„Ç∞Áî®\n",
        "#„ÇÇ„Åódst_folder„Åå„ÅÇ„Çå„Å∞ÂâäÈô§„Åó„Å¶Êñ∞„Åó„Åè‰Ωú„ÇäÁõ¥„Åô\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "id": "AAUdjy9A0YZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # classÂêç„ÇíÂÆöÁæ©"
      ],
      "metadata": {
        "id": "giDFflceMi9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "ZjV_xXLpd5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.py„Çírename„Åó„Å¶gdrive„Å´ÁßªÂãï„Åó„Å¶„Åä„Åè\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folderÂÜÖÂÖ®ÈÉ®)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# Ê®™ÂπÖ„Çí640px„Å´„É™„Çµ„Ç§„Ç∫„Åó„Åü„Éá„Éº„Çø„Çª„ÉÉ„Éà\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# „Éë„Çπ„ÇíÊåáÂÆö„Åô„Çã\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#„Çµ„Éù„Éº„Éà„Éë„ÉÉ„ÉÅ„ÅÆ„Ç§„É≥„Éù„Éº„Éà\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2„ÅßÈñã„Åè\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, ‰∏ä‰∏ãpadding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # „Éê„ÉÉ„ÉÅÂØæÂøú\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"Ë®∫Êñ≠„ÅØ %s„ÄÅÁ¢∫Áéá„ÅØ%.1fÔºÖ„Åß„Åô„ÄÇ\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640„Å´padding„Åï„Çå„Å¶„ÅÑ„ÇãÂàÜ„ÅÆÂ∫ßÊ®ô„ÇíË∂≥„Åô)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅßÁîªÂÉè„ÇíÂàá„ÇäÊäú„Åè„Äç\n",
        "\n",
        "    if x1 < 0: #Ë≤†„ÅÆÂ†¥Âêà„ÅÆ„Ç®„É©„ÉºÂõûÈÅø\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # Âàá„ÇäÊäú„ÅÑ„ÅüÁîªÂÉè„Çí‰øùÂ≠ò„Åô„Çã\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcampÁî®csv„ÅÆimage_path„ÇíÊîπÂ§â)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}