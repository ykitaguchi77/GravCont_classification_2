{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_MobileNet%EF%BC%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetMobileNet_for_ios**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "9e413749-8d30-42c6-8b6b-d66743e08e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!pip install albumentations==0.4.6\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from torch_optimizer) (1.13.1+cu116)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (1.10.1)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from albumentations==0.4.6) (4.7.0.72)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (8.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.16.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.9/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (23.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2023.3.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.39.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.15.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=79e341d407de18e5ec6a2b0ae9443f010ae01d7a6f0587894465aca94a2ccd2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ab/bb/5c36149e73c852c0fc8f278f457dd5b22df911cd65a542b16e\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed albumentations-0.4.6\n",
            "Thu Mar 23 13:16:17 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    24W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee4ee40-19df-43b2-9dd4-1edaa19be030"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d5da6b-7112-4f5a-e2b3-3ae29ec1b6c4"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.96"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "    \n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "5dc0c944-e948-44a1-f34a-a98e824ca034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels \n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet用に224px四方に成形しておく\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "if os.path.exists(dst_folder): \n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/train\")\n",
        "os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:      \n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "          \n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])      \n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "        \n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            \n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()   \n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "           \n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(num_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)  \n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min') \n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI2_SlJUcqDX",
        "outputId": "949ec283-3a61-4c27-b79c-b93a3380a882"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ranger_adabelief\n",
            "  Downloading ranger_adabelief-0.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from ranger_adabelief) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->ranger_adabelief) (4.5.0)\n",
            "Installing collected packages: ranger_adabelief\n",
            "Successfully installed ranger_adabelief-0.1.0\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "2649\n",
            "664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b554ccf-db49-40db-a4e4-b04288463af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Epoch: [ 0/40\n",
            "train_loss: 0.61255 train_acc: 0.67610\n",
            "valid_loss: 0.42031 valid_acc: 0.85241\n",
            "Validation loss decreased (inf --> 0.420309).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 1/40\n",
            "train_loss: 0.37488 train_acc: 0.83994\n",
            "valid_loss: 0.25871 valid_acc: 0.89006\n",
            "Validation loss decreased (0.420309 --> 0.258712).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 2/40\n",
            "train_loss: 0.30978 train_acc: 0.86146\n",
            "valid_loss: 0.20457 valid_acc: 0.92620\n",
            "Validation loss decreased (0.258712 --> 0.204568).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 3/40\n",
            "train_loss: 0.24727 train_acc: 0.89958\n",
            "valid_loss: 0.24185 valid_acc: 0.90813\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 4/40\n",
            "train_loss: 0.20818 train_acc: 0.91242\n",
            "valid_loss: 0.23571 valid_acc: 0.90512\n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 5/40\n",
            "train_loss: 0.18544 train_acc: 0.92676\n",
            "valid_loss: 0.29689 valid_acc: 0.88404\n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 6/40\n",
            "train_loss: 0.14568 train_acc: 0.93847\n",
            "valid_loss: 0.20536 valid_acc: 0.91867\n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 7/40\n",
            "train_loss: 0.15159 train_acc: 0.94035\n",
            "valid_loss: 0.21522 valid_acc: 0.90964\n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 8/40\n",
            "train_loss: 0.12689 train_acc: 0.94790\n",
            "valid_loss: 0.22458 valid_acc: 0.91717\n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 9/40\n",
            "train_loss: 0.10117 train_acc: 0.95583\n",
            "valid_loss: 0.42749 valid_acc: 0.86747\n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [10/40\n",
            "train_loss: 0.11081 train_acc: 0.95696\n",
            "valid_loss: 0.23160 valid_acc: 0.92018\n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [11/40\n",
            "train_loss: 0.09709 train_acc: 0.96300\n",
            "valid_loss: 0.25000 valid_acc: 0.91416\n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [12/40\n",
            "train_loss: 0.08705 train_acc: 0.96716\n",
            "valid_loss: 0.44884 valid_acc: 0.85090\n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [13/40\n",
            "train_loss: 0.09019 train_acc: 0.96451\n",
            "valid_loss: 0.24390 valid_acc: 0.91416\n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [14/40\n",
            "train_loss: 0.07210 train_acc: 0.97508\n",
            "valid_loss: 0.32671 valid_acc: 0.89608\n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [15/40\n",
            "train_loss: 0.07177 train_acc: 0.97131\n",
            "valid_loss: 0.24731 valid_acc: 0.91717\n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [16/40\n",
            "train_loss: 0.08003 train_acc: 0.97169\n",
            "valid_loss: 0.30967 valid_acc: 0.90211\n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [17/40\n",
            "train_loss: 0.06363 train_acc: 0.97357\n",
            "valid_loss: 0.23251 valid_acc: 0.91114\n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [18/40\n",
            "train_loss: 0.06233 train_acc: 0.97848\n",
            "valid_loss: 0.32752 valid_acc: 0.91717\n",
            "EarlyStopping counter: 16 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [19/40\n",
            "train_loss: 0.06333 train_acc: 0.97395\n",
            "valid_loss: 0.23595 valid_acc: 0.92018\n",
            "EarlyStopping counter: 17 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [20/40\n",
            "train_loss: 0.04753 train_acc: 0.98188\n",
            "valid_loss: 0.34644 valid_acc: 0.90060\n",
            "EarlyStopping counter: 18 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [21/40\n",
            "train_loss: 0.05112 train_acc: 0.98037\n",
            "valid_loss: 0.46414 valid_acc: 0.89157\n",
            "EarlyStopping counter: 19 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [22/40\n",
            "train_loss: 0.06942 train_acc: 0.97131\n",
            "valid_loss: 0.22961 valid_acc: 0.93524\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug2.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)  \n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug2.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH",
        "outputId": "57d5718b-178c-41f8-ceb8-28544b24d685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# Interference MobileNetV3 #\n",
        "######################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0)\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:  \n",
        "      #target = target.squeeze(1)     \n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1) \n",
        "    \n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "      \n",
        "      probs.append(prob) #予測確率\n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#################################################\n",
        "threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "#################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "#Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_pred)     \n",
        "plt.plot(fpr, tpr, marker='o')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.grid()\n",
        "print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "#ヒートマップを作成\n",
        "arr_2d = np.round(confusion_arr/5).astype(int) #5foldの合計をfold数で割って平均を出す、整数に丸めて整数型にする\n",
        "df_matrix = pd.DataFrame(data=arr_2d, index=[\"Normal\", \"TED\"], columns=[\"Normal\", \"TED\"])\n",
        "print(df_matrix)\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(df_matrix, annot=True,fmt=\"d\", cmap='Blues')\n",
        "#plt.savefig(r\"C:\\Users\\ykita\\Downloads\\per_image_confusion_matrix.png\", dpi=700)\n",
        "plt.show()\n",
        "plt.close('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "1CAhawNTE8k8",
        "outputId": "2e138a01-9658-49bb-9d1f-0b33582b1da7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9081325301204819\n",
            "289 43 18 314\n",
            "confusion matrix = \n",
            " [[314  18]\n",
            " [ 43 289]]\n",
            "Accuracy : 0.9081325301204819\n",
            "Precision (true positive rate) : 0.9413680781758957\n",
            "Recall (sensitivity): 0.8704819277108434\n",
            "Specificity : 0.9457831325301205\n",
            "F1 score : 0.9045383411580594\n",
            "Area_under_ROC : 0.908132530120482\n",
            "\n",
            "        Normal  TED\n",
            "Normal       0    0\n",
            "TED          0    0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n",
        "    "
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh",
        "outputId": "852987b9-62c5-4da4-de93-4fa915bad38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2854.09 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 40/40 [00:00<00:00, 76.30 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 127.54 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1664.59 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")            \n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\") \n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69277f-119c-41dc-d348-1bccda7d470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img) \n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device) \n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img) \n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "    \n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]  \n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}