{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/LivePortrait.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# リポジトリクローン🕊"
      ],
      "metadata": {
        "id": "YLFciB35vNnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfrvbe_7k-a-",
        "outputId": "84b55a81-afdf-4988-a946-8173e94fb7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LivePortrait'...\n",
            "remote: Enumerating objects: 990, done.\u001b[K\n",
            "remote: Counting objects: 100% (607/607), done.\u001b[K\n",
            "remote: Compressing objects: 100% (360/360), done.\u001b[K\n",
            "remote: Total 990 (delta 379), reused 288 (delta 247), pack-reused 383 (from 1)\u001b[K\n",
            "Receiving objects: 100% (990/990), 38.70 MiB | 23.13 MiB/s, done.\n",
            "Resolving deltas: 100% (469/469), done.\n",
            "/content/LivePortrait\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KwaiVGI/LivePortrait\n",
        "%cd LivePortrait"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# パッケージインストール🕊"
      ],
      "metadata": {
        "id": "tJXMSFysvQYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q onnx\n",
        "!pip install -q onnxruntime-gpu\n",
        "!pip install -q tyro\n",
        "!pip install -q gradio\n",
        "!pip install -q pykalman\n",
        "!pip install -q colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6e5mM7mD9C",
        "outputId": "077d1038-a00b-4c11-9d35-da5cba854cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.2/226.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 重みダウンロード🕊"
      ],
      "metadata": {
        "id": "pvD0rBmHvTUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude \"*.git*\" \"README.md\" \"docs\" --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd1vi883lIQ-",
        "outputId": "9688d406-2983-4b98-8ea1-35abd3d8b2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LivePortrait/pretrained_weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推論実行🕊"
      ],
      "metadata": {
        "id": "W5GJf7VqvVbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py \\\n",
        "    -s=assets/examples/source/s10.jpg \\\n",
        "    -d=assets/examples/driving/d0.mp4 \\\n",
        "    -o=animations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC7SFP64rpY9",
        "outputId": "de1ae3a2-14a9-4c23-8d2c-4c727ab08d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LivePortrait/src/utils/helper.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(ckpt_path, map_location=lambda storage, loc: storage))\n",
            "\u001b[2;36m[14:00:16]\u001b[0m\u001b[2;36m \u001b[0mLoad appearance_feature_extractor from                        \u001b]8;id=594794;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=574931;file:///content/LivePortrait/src/live_portrait_wrapper.py#46\u001b\\\u001b[2m46\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[35mdels/\u001b[0m\u001b[95mappearance_feature_extractor.pth\u001b[0m done.                   \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad motion_extractor from                                    \u001b]8;id=836631;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=382409;file:///content/LivePortrait/src/live_portrait_wrapper.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[35mdels/\u001b[0m\u001b[95mmotion_extractor.pth\u001b[0m done.                               \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m[14:00:17]\u001b[0m\u001b[2;36m \u001b[0mLoad warping_module from                                      \u001b]8;id=982682;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=736240;file:///content/LivePortrait/src/live_portrait_wrapper.py#52\u001b\\\u001b[2m52\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[35mdels/\u001b[0m\u001b[95mwarping_module.pth\u001b[0m done.                                 \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m[14:00:18]\u001b[0m\u001b[2;36m \u001b[0mLoad spade_generator from                                     \u001b]8;id=192258;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=207368;file:///content/LivePortrait/src/live_portrait_wrapper.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[35mdels/\u001b[0m\u001b[95mspade_generator.pth\u001b[0m done.                                \u001b[2m                           \u001b[0m\n",
            "/content/LivePortrait/src/utils/helper.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad stitching_retargeting_module from                        \u001b]8;id=429835;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=331447;file:///content/LivePortrait/src/live_portrait_wrapper.py#59\u001b\\\u001b[2m59\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/retarge\u001b[0m \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[35mting_models/\u001b[0m\u001b[95mstitching_retargeting_module.pth\u001b[0m done.            \u001b[2m                           \u001b[0m\n",
            "\u001b[2;36m[14:00:20]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m2.\u001b[0m126s                               \u001b]8;id=60602;file:///content/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=731751;file:///content/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m[14:00:25]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m4.\u001b[0m660s                            \u001b]8;id=947107;file:///content/LivePortrait/src/utils/human_landmark_runner.py\u001b\\\u001b[2mhuman_landmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=337955;file:///content/LivePortrait/src/utils/human_landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad source image from assets/examples/source/s10.jpg        \u001b]8;id=610740;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=295358;file:///content/LivePortrait/src/live_portrait_pipeline.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad driving video from: assets/examples/driving/d0.mp4,    \u001b]8;id=553423;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=914629;file:///content/LivePortrait/src/live_portrait_pipeline.py#133\u001b\\\u001b[2m133\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0mFPS is \u001b[1;36m25\u001b[0m                                                   \u001b[2m                             \u001b[0m\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mStart making driving motion template\u001b[33m...\u001b[0m                     \u001b]8;id=130929;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=163402;file:///content/LivePortrait/src/live_portrait_pipeline.py#144\u001b\\\u001b[2m144\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2KMaking motion templates... \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[2;36m[14:00:30]\u001b[0m\u001b[2;36m \u001b[0mDump motion template to assets/examples/driving/d0.pkl      \u001b]8;id=591568;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=848061;file:///content/LivePortrait/src/live_portrait_pipeline.py#172\u001b\\\u001b[2m172\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mPrepared pasteback mask done.                               \u001b]8;id=562222;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=55411;file:///content/LivePortrait/src/live_portrait_pipeline.py#183\u001b\\\u001b[2m183\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m[14:00:31]\u001b[0m\u001b[2;36m \u001b[0mThe animated video consists of \u001b[1;36m78\u001b[0m frames.                   \u001b]8;id=567664;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=332510;file:///content/LivePortrait/src/live_portrait_pipeline.py#270\u001b\\\u001b[2m270\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2K🚀Animating... \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:17\u001b[0m\n",
            "\u001b[2KConcatenating result... \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[2KWriting \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[2KWriting \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[1;34m[swscaler @ 0x5f87000] \u001b[0m\u001b[0;33mWarning: data is not aligned! This can lead to a speed loss\n",
            "\u001b[2KWriting \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[1A\u001b[2K\u001b[2;36m[14:00:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32mAnimated template: assets/examples/driving/d0.pkl, you can \u001b[0m \u001b]8;id=83885;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=396379;file:///content/LivePortrait/src/live_portrait_pipeline.py#503\u001b\\\u001b[2m503\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m           \u001b[0m\u001b[1;32mspecify `-d` argument with this template path next time to \u001b[0m \u001b[2m                             \u001b[0m\n",
            "\u001b[2;36m           \u001b[0m\u001b[1;32mavoid cropping video, motion making and protecting privacy.\u001b[0m \u001b[2m                             \u001b[0m\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mAnimated video: animations/s10--d0.mp4                      \u001b]8;id=783769;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=596072;file:///content/LivePortrait/src/live_portrait_pipeline.py#504\u001b\\\u001b[2m504\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mAnimated video with concat: animations/s10--d0_concat.mp4   \u001b]8;id=235605;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=110308;file:///content/LivePortrait/src/live_portrait_pipeline.py#505\u001b\\\u001b[2m505\u001b[0m\u001b]8;;\u001b\\\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradioアプリ起動🕊　※app.py改造"
      ],
      "metadata": {
        "id": "9dhBAZg_sRkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "\"\"\"\n",
        "The entrance of the gradio for human\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tyro\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "import os.path as osp\n",
        "from src.utils.helper import load_description\n",
        "from src.gradio_pipeline import GradioPipeline\n",
        "from src.config.crop_config import CropConfig\n",
        "from src.config.argument_config import ArgumentConfig\n",
        "from src.config.inference_config import InferenceConfig\n",
        "\n",
        "\n",
        "def partial_fields(target_class, kwargs):\n",
        "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
        "\n",
        "\n",
        "def fast_check_ffmpeg():\n",
        "    try:\n",
        "        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Colaboratory用修正\n",
        "import sys\n",
        "sys.argv = [arg for arg in sys.argv if not (arg.startswith(\"-f\") or \"json\" in arg)]\n",
        "\n",
        "# set tyro theme\n",
        "tyro.extras.set_accent_color(\"bright_cyan\")\n",
        "args = tyro.cli(ArgumentConfig)\n",
        "\n",
        "ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n",
        "if osp.exists(ffmpeg_dir):\n",
        "    os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n",
        "\n",
        "if not fast_check_ffmpeg():\n",
        "    raise ImportError(\n",
        "        \"FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html\"\n",
        "    )\n",
        "# specify configs for inference\n",
        "inference_cfg = partial_fields(InferenceConfig, args.__dict__)  # use attribute of args to initial InferenceConfig\n",
        "crop_cfg = partial_fields(CropConfig, args.__dict__)  # use attribute of args to initial CropConfig\n",
        "# global_tab_selection = None\n",
        "\n",
        "gradio_pipeline = GradioPipeline(\n",
        "    inference_cfg=inference_cfg,\n",
        "    crop_cfg=crop_cfg,\n",
        "    args=args\n",
        ")\n",
        "\n",
        "if args.gradio_temp_dir not in (None, ''):\n",
        "    os.environ[\"GRADIO_TEMP_DIR\"] = args.gradio_temp_dir\n",
        "    os.makedirs(args.gradio_temp_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_video(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_video(*args, **kwargs)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_image_retargeting(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_image_retargeting(*args, **kwargs)\n",
        "\n",
        "\n",
        "def gpu_wrapped_execute_video_retargeting(*args, **kwargs):\n",
        "    return gradio_pipeline.execute_video_retargeting(*args, **kwargs)\n",
        "\n",
        "\n",
        "def reset_sliders(*args, **kwargs):\n",
        "    return 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5, True, True\n",
        "\n",
        "\n",
        "# assets\n",
        "title_md = \"assets/gradio/gradio_title.md\"\n",
        "example_portrait_dir = \"assets/examples/source\"\n",
        "example_video_dir = \"assets/examples/driving\"\n",
        "data_examples_i2v = [\n",
        "    [osp.join(example_portrait_dir, \"s9.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s6.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s10.jpg\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s5.jpg\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s7.jpg\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False],\n",
        "    [osp.join(example_portrait_dir, \"s2.jpg\"), osp.join(example_video_dir, \"d13.mp4\"), True, True, True, True],\n",
        "]\n",
        "data_examples_v2v = [\n",
        "    [osp.join(example_portrait_dir, \"s13.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s14.mp4\"), osp.join(example_video_dir, \"d18.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s15.mp4\"), osp.join(example_video_dir, \"d19.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    [osp.join(example_portrait_dir, \"s18.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, 3e-7],\n",
        "    # [osp.join(example_portrait_dir, \"s19.mp4\"), osp.join(example_video_dir, \"d6.mp4\"), True, True, True, False, False, 3e-7],\n",
        "    [osp.join(example_portrait_dir, \"s20.mp4\"), osp.join(example_video_dir, \"d0.mp4\"), True, True, True, False, 3e-7],\n",
        "]\n",
        "#################### interface logic ####################\n",
        "\n",
        "# Define components first\n",
        "retargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.5, step=0.05, label=\"crop scale\")\n",
        "video_retargeting_source_scale = gr.Number(minimum=1.8, maximum=3.2, value=2.3, step=0.05, label=\"crop scale\")\n",
        "driving_smooth_observation_variance_retargeting = gr.Number(value=3e-6, label=\"motion smooth strength\", minimum=1e-11, maximum=1e-2, step=1e-8)\n",
        "video_retargeting_silence = gr.Checkbox(value=False, label=\"keeping the lip silent\")\n",
        "eye_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target eyes-open ratio\")\n",
        "lip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\n",
        "video_lip_retargeting_slider = gr.Slider(minimum=0, maximum=0.8, step=0.01, label=\"target lip-open ratio\")\n",
        "head_pitch_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative pitch\")\n",
        "head_yaw_slider = gr.Slider(minimum=-25, maximum=25, value=0, step=1, label=\"relative yaw\")\n",
        "head_roll_slider = gr.Slider(minimum=-15.0, maximum=15.0, value=0, step=1, label=\"relative roll\")\n",
        "mov_x = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"x-axis movement\")\n",
        "mov_y = gr.Slider(minimum=-0.19, maximum=0.19, value=0.0, step=0.01, label=\"y-axis movement\")\n",
        "mov_z = gr.Slider(minimum=0.9, maximum=1.2, value=1.0, step=0.01, label=\"z-axis movement\")\n",
        "lip_variation_zero = gr.Slider(minimum=-0.09, maximum=0.09, value=0, step=0.01, label=\"pouting\")\n",
        "lip_variation_one = gr.Slider(minimum=-20.0, maximum=15.0, value=0, step=0.01, label=\"pursing 😐\")\n",
        "lip_variation_two = gr.Slider(minimum=0.0, maximum=15.0, value=0, step=0.01, label=\"grin 😁\")\n",
        "lip_variation_three = gr.Slider(minimum=-90.0, maximum=120.0, value=0, step=1.0, label=\"lip close <-> open\")\n",
        "smile = gr.Slider(minimum=-0.3, maximum=1.3, value=0, step=0.01, label=\"smile 😄\")\n",
        "wink = gr.Slider(minimum=0, maximum=39, value=0, step=0.01, label=\"wink 😉\")\n",
        "eyebrow = gr.Slider(minimum=-30, maximum=30, value=0, step=0.01, label=\"eyebrow 🤨\")\n",
        "eyeball_direction_x = gr.Slider(minimum=-30.0, maximum=30.0, value=0, step=0.01, label=\"eye gaze (horizontal) 👀\")\n",
        "eyeball_direction_y = gr.Slider(minimum=-63.0, maximum=63.0, value=0, step=0.01, label=\"eye gaze (vertical) 🙄\")\n",
        "retargeting_input_image = gr.Image(type=\"filepath\")\n",
        "retargeting_input_video = gr.Video()\n",
        "output_image = gr.Image(type=\"numpy\")\n",
        "output_image_paste_back = gr.Image(type=\"numpy\")\n",
        "retargeting_output_image = gr.Image(type=\"numpy\")\n",
        "retargeting_output_image_paste_back = gr.Image(type=\"numpy\")\n",
        "output_video = gr.Video(autoplay=False)\n",
        "output_video_paste_back = gr.Video(autoplay=False)\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont(\"Plus Jakarta Sans\")])) as demo:\n",
        "    gr.HTML(load_description(title_md))\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_upload.md\"))\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"🖼️ Source Image\") as tab_image:\n",
        "                    with gr.Accordion(open=True, label=\"Source Image\"):\n",
        "                        source_image_input = gr.Image(type=\"filepath\")\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_portrait_dir, \"s9.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s6.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s10.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s5.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s7.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s12.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s22.jpg\")],\n",
        "                                [osp.join(example_portrait_dir, \"s23.jpg\")],\n",
        "                            ],\n",
        "                            inputs=[source_image_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                with gr.TabItem(\"🎞️ Source Video\") as tab_video:\n",
        "                    with gr.Accordion(open=True, label=\"Source Video\"):\n",
        "                        source_video_input = gr.Video()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_portrait_dir, \"s13.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s14.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s15.mp4\")],\n",
        "                                [osp.join(example_portrait_dir, \"s18.mp4\")],\n",
        "                                # [osp.join(example_portrait_dir, \"s19.mp4\")],\n",
        "                                [osp.join(example_portrait_dir, \"s20.mp4\")],\n",
        "                            ],\n",
        "                            inputs=[source_video_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                tab_selection = gr.Textbox(visible=False)\n",
        "                tab_image.select(lambda: \"Image\", None, tab_selection)\n",
        "                tab_video.select(lambda: \"Video\", None, tab_selection)\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Source Image or Video\"):\n",
        "                with gr.Row():\n",
        "                    flag_do_crop_input = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "                    scale = gr.Number(value=2.3, label=\"source crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio = gr.Number(value=0.0, label=\"source crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio = gr.Number(value=-0.125, label=\"source crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "        with gr.Column():\n",
        "            with gr.Tabs():\n",
        "                with gr.TabItem(\"🎞️ Driving Video\") as v_tab_video:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Video\"):\n",
        "                        driving_video_input = gr.Video()\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d0.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d18.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d19.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d14.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d6.mp4\")],\n",
        "                                [osp.join(example_video_dir, \"d20.mp4\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "                with gr.TabItem(\"🖼️ Driving Image\") as v_tab_image:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Image\"):\n",
        "                        driving_image_input = gr.Image(type=\"filepath\")\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d30.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d9.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d19.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d8.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d12.jpg\")],\n",
        "                                [osp.join(example_video_dir, \"d38.jpg\")],\n",
        "                            ],\n",
        "                            inputs=[driving_image_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                with gr.TabItem(\"📁 Driving Pickle\") as v_tab_pickle:\n",
        "                    with gr.Accordion(open=True, label=\"Driving Pickle\"):\n",
        "                        driving_video_pickle_input = gr.File(type=\"filepath\", file_types=[\".pkl\"])\n",
        "                        gr.Examples(\n",
        "                            examples=[\n",
        "                                [osp.join(example_video_dir, \"d1.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d2.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d5.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d7.pkl\")],\n",
        "                                [osp.join(example_video_dir, \"d8.pkl\")],\n",
        "                            ],\n",
        "                            inputs=[driving_video_pickle_input],\n",
        "                            cache_examples=False,\n",
        "                        )\n",
        "\n",
        "                v_tab_selection = gr.Textbox(visible=False)\n",
        "                v_tab_video.select(lambda: \"Video\", None, v_tab_selection)\n",
        "                v_tab_image.select(lambda: \"Image\", None, v_tab_selection)\n",
        "                v_tab_pickle.select(lambda: \"Pickle\", None, v_tab_selection)\n",
        "            # with gr.Accordion(open=False, label=\"Animation Instructions\"):\n",
        "                # gr.Markdown(load_description(\"assets/gradio/gradio_description_animation.md\"))\n",
        "            with gr.Accordion(open=True, label=\"Cropping Options for Driving Video\"):\n",
        "                with gr.Row():\n",
        "                    flag_crop_driving_video_input = gr.Checkbox(value=False, label=\"do crop (driving)\")\n",
        "                    scale_crop_driving_video = gr.Number(value=2.2, label=\"driving crop scale\", minimum=1.8, maximum=3.2, step=0.05)\n",
        "                    vx_ratio_crop_driving_video = gr.Number(value=0.0, label=\"driving crop x\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "                    vy_ratio_crop_driving_video = gr.Number(value=-0.1, label=\"driving crop y\", minimum=-0.5, maximum=0.5, step=0.01)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Accordion(open=True, label=\"Animation Options\"):\n",
        "            with gr.Row():\n",
        "                flag_normalize_lip = gr.Checkbox(value=False, label=\"normalize lip\")\n",
        "                flag_relative_input = gr.Checkbox(value=True, label=\"relative motion\")\n",
        "                flag_remap_input = gr.Checkbox(value=True, label=\"paste-back\")\n",
        "                flag_stitching_input = gr.Checkbox(value=True, label=\"stitching\")\n",
        "                animation_region = gr.Radio([\"exp\", \"pose\", \"lip\", \"eyes\", \"all\"], value=\"all\", label=\"animation region\")\n",
        "                driving_option_input = gr.Radio(['expression-friendly', 'pose-friendly'], value=\"expression-friendly\", label=\"driving option (i2v)\")\n",
        "                driving_multiplier = gr.Number(value=1.0, label=\"driving multiplier (i2v)\", minimum=0.0, maximum=2.0, step=0.02)\n",
        "                driving_smooth_observation_variance = gr.Number(value=3e-7, label=\"motion smooth strength (v2v)\", minimum=1e-11, maximum=1e-2, step=1e-8)\n",
        "\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_animate_clear.md\"))\n",
        "    with gr.Row():\n",
        "        process_button_animation = gr.Button(\"🚀 Animate\", variant=\"primary\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_video_i2v = gr.Video(autoplay=False, label=\"The animated video in the original image space\")\n",
        "        with gr.Column():\n",
        "            output_video_concat_i2v = gr.Video(autoplay=False, label=\"The animated video\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            output_image_i2i = gr.Image(type=\"numpy\", label=\"The animated image in the original image space\", visible=False)\n",
        "        with gr.Column():\n",
        "            output_image_concat_i2i = gr.Image(type=\"numpy\", label=\"The animated image\", visible=False)\n",
        "    with gr.Row():\n",
        "        process_button_reset = gr.ClearButton([source_image_input, source_video_input, driving_video_pickle_input, driving_video_input, driving_image_input, output_video_i2v, output_video_concat_i2v, output_image_i2i, output_image_concat_i2i], value=\"🧹 Clear\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Examples\n",
        "        gr.Markdown(\"## You could also choose the examples below by one click ⬇️\")\n",
        "    with gr.Row():\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"🖼️ Portrait Animation\"):\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_i2v,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_image_input,\n",
        "                        driving_video_input,\n",
        "                        flag_relative_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back],\n",
        "                    examples_per_page=len(data_examples_i2v),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "            with gr.TabItem(\"🎞️ Portrait Video Editing\"):\n",
        "                gr.Examples(\n",
        "                    examples=data_examples_v2v,\n",
        "                    fn=gpu_wrapped_execute_video,\n",
        "                    inputs=[\n",
        "                        source_video_input,\n",
        "                        driving_video_input,\n",
        "                        flag_relative_input,\n",
        "                        flag_do_crop_input,\n",
        "                        flag_remap_input,\n",
        "                        flag_crop_driving_video_input,\n",
        "                        driving_smooth_observation_variance,\n",
        "                    ],\n",
        "                    outputs=[output_image, output_image_paste_back],\n",
        "                    examples_per_page=len(data_examples_v2v),\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "\n",
        "    # Retargeting Image\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting.md\"), visible=True)\n",
        "    with gr.Row(visible=True):\n",
        "        flag_do_crop_input_retargeting_image = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "        flag_stitching_retargeting_input = gr.Checkbox(value=True, label=\"stitching\")\n",
        "        retargeting_source_scale.render()\n",
        "        eye_retargeting_slider.render()\n",
        "        lip_retargeting_slider.render()\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Facial movement sliders\"):\n",
        "                with gr.Row(visible=True):\n",
        "                    head_pitch_slider.render()\n",
        "                    head_yaw_slider.render()\n",
        "                    head_roll_slider.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    mov_x.render()\n",
        "                    mov_y.render()\n",
        "                    mov_z.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Facial expression sliders\"):\n",
        "                with gr.Row(visible=True):\n",
        "                    lip_variation_zero.render()\n",
        "                    lip_variation_one.render()\n",
        "                    lip_variation_two.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    lip_variation_three.render()\n",
        "                    smile.render()\n",
        "                    wink.render()\n",
        "                with gr.Row(visible=True):\n",
        "                    eyebrow.render()\n",
        "                    eyeball_direction_x.render()\n",
        "                    eyeball_direction_y.render()\n",
        "    with gr.Row(visible=True):\n",
        "        reset_button = gr.Button(\"🔄 Reset\")\n",
        "        reset_button.click(\n",
        "            fn=reset_sliders,\n",
        "            inputs=None,\n",
        "            outputs=[\n",
        "                head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n",
        "                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n",
        "                retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n",
        "            ]\n",
        "        )\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Image Input\"):\n",
        "                retargeting_input_image.render()\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [osp.join(example_portrait_dir, \"s9.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s6.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s10.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s5.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s7.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s12.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s22.jpg\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s23.jpg\")],\n",
        "                        [osp.join(example_portrait_dir, \"s42.jpg\")],\n",
        "                    ],\n",
        "                    inputs=[retargeting_input_image],\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n",
        "                retargeting_output_image.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n",
        "                retargeting_output_image_paste_back.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_reset_retargeting = gr.ClearButton(\n",
        "            [\n",
        "                retargeting_input_image,\n",
        "                retargeting_output_image,\n",
        "                retargeting_output_image_paste_back,\n",
        "            ],\n",
        "            value=\"🧹 Clear\"\n",
        "        )\n",
        "\n",
        "    # Retargeting Video\n",
        "    gr.Markdown(load_description(\"assets/gradio/gradio_description_retargeting_video.md\"), visible=True)\n",
        "    with gr.Row(visible=True):\n",
        "        flag_do_crop_input_retargeting_video = gr.Checkbox(value=True, label=\"do crop (source)\")\n",
        "        video_retargeting_source_scale.render()\n",
        "        video_lip_retargeting_slider.render()\n",
        "        driving_smooth_observation_variance_retargeting.render()\n",
        "        video_retargeting_silence.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_retargeting_video = gr.Button(\"🚗 Retargeting Video\", variant=\"primary\")\n",
        "    with gr.Row(visible=True):\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Video Input\"):\n",
        "                retargeting_input_video.render()\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [osp.join(example_portrait_dir, \"s13.mp4\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s18.mp4\")],\n",
        "                        # [osp.join(example_portrait_dir, \"s20.mp4\")],\n",
        "                        [osp.join(example_portrait_dir, \"s29.mp4\")],\n",
        "                        [osp.join(example_portrait_dir, \"s32.mp4\")],\n",
        "                        [osp.join(example_video_dir, \"d3.mp4\")],\n",
        "                    ],\n",
        "                    inputs=[retargeting_input_video],\n",
        "                    cache_examples=False,\n",
        "                )\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Retargeting Result\"):\n",
        "                output_video.render()\n",
        "        with gr.Column():\n",
        "            with gr.Accordion(open=True, label=\"Paste-back Result\"):\n",
        "                output_video_paste_back.render()\n",
        "    with gr.Row(visible=True):\n",
        "        process_button_reset_retargeting = gr.ClearButton(\n",
        "            [\n",
        "                video_lip_retargeting_slider,\n",
        "                retargeting_input_video,\n",
        "                output_video,\n",
        "                output_video_paste_back\n",
        "            ],\n",
        "            value=\"🧹 Clear\"\n",
        "        )\n",
        "\n",
        "    # binding functions for buttons\n",
        "    process_button_animation.click(\n",
        "        fn=gpu_wrapped_execute_video,\n",
        "        inputs=[\n",
        "            source_image_input,\n",
        "            source_video_input,\n",
        "            driving_video_input,\n",
        "            driving_image_input,\n",
        "            driving_video_pickle_input,\n",
        "            flag_normalize_lip,\n",
        "            flag_relative_input,\n",
        "            flag_do_crop_input,\n",
        "            flag_remap_input,\n",
        "            flag_stitching_input,\n",
        "            animation_region,\n",
        "            driving_option_input,\n",
        "            driving_multiplier,\n",
        "            flag_crop_driving_video_input,\n",
        "            scale,\n",
        "            vx_ratio,\n",
        "            vy_ratio,\n",
        "            scale_crop_driving_video,\n",
        "            vx_ratio_crop_driving_video,\n",
        "            vy_ratio_crop_driving_video,\n",
        "            driving_smooth_observation_variance,\n",
        "            tab_selection,\n",
        "            v_tab_selection,\n",
        "        ],\n",
        "        outputs=[output_video_i2v, output_video_i2v, output_video_concat_i2v, output_video_concat_i2v, output_image_i2i, output_image_i2i, output_image_concat_i2i, output_image_concat_i2i],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "\n",
        "    retargeting_input_image.change(\n",
        "        fn=gradio_pipeline.init_retargeting_image,\n",
        "        inputs=[retargeting_source_scale, eye_retargeting_slider, lip_retargeting_slider, retargeting_input_image],\n",
        "        outputs=[eye_retargeting_slider, lip_retargeting_slider]\n",
        "    )\n",
        "\n",
        "    sliders = [eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z, lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y]\n",
        "    for slider in sliders:\n",
        "        # NOTE: gradio >= 4.0.0 may cause slow response\n",
        "        slider.change(\n",
        "            fn=gpu_wrapped_execute_image_retargeting,\n",
        "            inputs=[\n",
        "                eye_retargeting_slider, lip_retargeting_slider, head_pitch_slider, head_yaw_slider, head_roll_slider, mov_x, mov_y, mov_z,\n",
        "                lip_variation_zero, lip_variation_one, lip_variation_two, lip_variation_three, smile, wink, eyebrow, eyeball_direction_x, eyeball_direction_y,\n",
        "                retargeting_input_image, retargeting_source_scale, flag_stitching_retargeting_input, flag_do_crop_input_retargeting_image\n",
        "            ],\n",
        "            outputs=[retargeting_output_image, retargeting_output_image_paste_back],\n",
        "        )\n",
        "\n",
        "    process_button_retargeting_video.click(\n",
        "        fn=gpu_wrapped_execute_video_retargeting,\n",
        "        inputs=[video_lip_retargeting_slider, retargeting_input_video, video_retargeting_source_scale, driving_smooth_observation_variance_retargeting, video_retargeting_silence, flag_do_crop_input_retargeting_video],\n",
        "        outputs=[output_video, output_video_paste_back],\n",
        "        show_progress=True\n",
        "    )\n",
        "\n",
        "# Colaboratory用修正\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "_aReKccXlPUh",
        "outputId": "85b3e8cc-cdb0-40e1-a9da-461bf06236b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/LivePortrait/src/utils/helper.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(ckpt_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:35]\u001b[0m\u001b[2;36m \u001b[0mLoad appearance_feature_extractor from                                       \u001b]8;id=100085;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=668921;file:///content/LivePortrait/src/live_portrait_wrapper.py#46\u001b\\\u001b[2m46\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mappearance\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95m_feature_extractor.pth\u001b[0m done.                                                 \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:35] </span>Load appearance_feature_extractor from                                       <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#46\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">appearance</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">_feature_extractor.pth</span> done.                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:36]\u001b[0m\u001b[2;36m \u001b[0mLoad motion_extractor from                                                   \u001b]8;id=974239;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=199432;file:///content/LivePortrait/src/live_portrait_wrapper.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mmotion_ext\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mractor.pth\u001b[0m done.                                                             \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:36] </span>Load motion_extractor from                                                   <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#49\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">motion_ext</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ractor.pth</span> done.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:37]\u001b[0m\u001b[2;36m \u001b[0mLoad warping_module from                                                     \u001b]8;id=983691;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=862307;file:///content/LivePortrait/src/live_portrait_wrapper.py#52\u001b\\\u001b[2m52\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mwarping_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mdule.pth\u001b[0m done.                                                               \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:37] </span>Load warping_module from                                                     <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#52\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">warping_mo</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dule.pth</span> done.                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad spade_generator from                                                    \u001b]8;id=103663;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=162574;file:///content/LivePortrait/src/live_portrait_wrapper.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mspade_gene\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mrator.pth\u001b[0m done.                                                              \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Load spade_generator from                                                    <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">spade_gene</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">rator.pth</span> done.                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/LivePortrait/src/utils/helper.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:38]\u001b[0m\u001b[2;36m \u001b[0mLoad stitching_retargeting_module from                                       \u001b]8;id=104040;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=38522;file:///content/LivePortrait/src/live_portrait_wrapper.py#59\u001b\\\u001b[2m59\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/\u001b[0m\u001b[95msti\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mtching_retargeting_module.pth\u001b[0m done.                                          \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:38] </span>Load stitching_retargeting_module from                                       <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#59\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sti</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tching_retargeting_module.pth</span> done.                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:39]\u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m0.\u001b[0m929s                                              \u001b]8;id=895172;file:///content/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=824103;file:///content/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:39] </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>929s                                              <a href=\"file:///content/LivePortrait/src/utils/face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/utils/face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[14:02:42]\u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m2.\u001b[0m975s                                           \u001b]8;id=723927;file:///content/LivePortrait/src/utils/human_landmark_runner.py\u001b\\\u001b[2mhuman_landmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=577207;file:///content/LivePortrait/src/utils/human_landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:02:42] </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.</span>975s                                           <a href=\"file:///content/LivePortrait/src/utils/human_landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">human_landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/utils/human_landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://520f69514f591be58b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://520f69514f591be58b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}