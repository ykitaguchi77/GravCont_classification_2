{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2864c2c72ba4b8ebe76dfbca3c3ec96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e66cb66453f74b068523ffa677e34755",
              "IPY_MODEL_9506a5054695405d9fd84f55e5f7b32b",
              "IPY_MODEL_3e241c6ee4fb4afc95a31069ff0d25d4"
            ],
            "layout": "IPY_MODEL_84d78b47d6cd425792de1d1ac5e329fc"
          }
        },
        "e66cb66453f74b068523ffa677e34755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ce6ea8d2ba4eafb891e69764b1b402",
            "placeholder": "​",
            "style": "IPY_MODEL_7b87923646404552bbc79ee4122a1769",
            "value": "model.safetensors: 100%"
          }
        },
        "9506a5054695405d9fd84f55e5f7b32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670a597d8af740f3b010a3365fa92a67",
            "max": 22058321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9eaa2d6321b42d1b5bc6c154461d609",
            "value": 22058321
          }
        },
        "3e241c6ee4fb4afc95a31069ff0d25d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef05a01afc44b9cafd859b8157b8e0a",
            "placeholder": "​",
            "style": "IPY_MODEL_26aeb2887c0f4149b7127675900f8c5b",
            "value": " 22.1M/22.1M [00:00&lt;00:00, 160MB/s]"
          }
        },
        "84d78b47d6cd425792de1d1ac5e329fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ce6ea8d2ba4eafb891e69764b1b402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b87923646404552bbc79ee4122a1769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "670a597d8af740f3b010a3365fa92a67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9eaa2d6321b42d1b5bc6c154461d609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fef05a01afc44b9cafd859b8157b8e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26aeb2887c0f4149b7127675900f8c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_CNN_chizai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetMobileNet_for_chizai**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "fd76de89-87b9-4e6c-bfe4-87c051b68865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer --q\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!pip install albumentations==0.4.6 --q\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Tue Oct 17 08:58:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c73adb-9934-438e-e76f-63b24a15ed7d"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "\n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "1bf12a8d-3243-4b7a-ea13-a33db0594ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# ディレクトリのパス\n",
        "directory_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont/images_cropped\"\n",
        "\n",
        "# ディレクトリ内のjpgファイルをリストアップ\n",
        "jpg_files = [f for f in os.listdir(directory_path) if f.endswith('.JPG')]\n",
        "\n",
        "# 1つのjpgファイルを表示\n",
        "if len(jpg_files) > 0:\n",
        "    file_to_display = jpg_files[0]  # 1つ目のファイルを表示\n",
        "    file_path = os.path.join(directory_path, file_to_display)\n",
        "    display(Image(filename=file_path))\n",
        "else:\n",
        "    print(\"指定されたディレクトリ内にjpgファイルが見つかりませんでした。\")\n"
      ],
      "metadata": {
        "id": "OiVhW7PAZhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels\n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet用に224px四方に成形しておく\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/train\")\n",
        "os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:\n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "\n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()\n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "\n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "\n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "\n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min')\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "eI2_SlJUcqDX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340,
          "referenced_widgets": [
            "c2864c2c72ba4b8ebe76dfbca3c3ec96",
            "e66cb66453f74b068523ffa677e34755",
            "9506a5054695405d9fd84f55e5f7b32b",
            "3e241c6ee4fb4afc95a31069ff0d25d4",
            "84d78b47d6cd425792de1d1ac5e329fc",
            "b1ce6ea8d2ba4eafb891e69764b1b402",
            "7b87923646404552bbc79ee4122a1769",
            "670a597d8af740f3b010a3365fa92a67",
            "c9eaa2d6321b42d1b5bc6c154461d609",
            "fef05a01afc44b9cafd859b8157b8e0a",
            "26aeb2887c0f4149b7127675900f8c5b"
          ]
        },
        "outputId": "c49ca9a6-9e54-4416-c7e7-8b0ab24d20b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2864c2c72ba4b8ebe76dfbca3c3ec96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0f7e1430547f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobilenetv3_large_100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#GPU使用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH",
        "outputId": "7988ca0a-c829-4422-8303-0c513ec4048c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-de7cd5f10342>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobilenetv3_large_100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#ネットワークの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dShbfug9CYD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "uJp342k-ICo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############\n",
        "# ## GradCAM ##\n",
        "# ############\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# def compute_gradcam(model, image_tensor, target_class=None):\n",
        "#     model.eval()\n",
        "\n",
        "#     # 最終畳み込み層の出力とその勾配を取得するためのhooks\n",
        "#     features_blobs = []\n",
        "#     grads_blobs = []\n",
        "\n",
        "#     def hook_feature(module, input, output):\n",
        "#         features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "#     def hook_grad(module, grad_in, grad_out):\n",
        "#         grads_blobs.append(grad_out[0].data.cpu().numpy())\n",
        "\n",
        "#     # 最後の畳み込み層を取得します (MobileNetV3の場合は、特定の層名を指定することができます。)\n",
        "#     final_conv = model.blocks[-1]  # この部分はモデルの構造により調整が必要\n",
        "#     final_conv.register_forward_hook(hook_feature)\n",
        "#     final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "#     # 順伝播と逆伝播を実行\n",
        "#     output = model(image_tensor)\n",
        "#     if target_class is None:\n",
        "#         target_class = output.argmax().item()\n",
        "#     model.zero_grad()\n",
        "#     one_hot = torch.zeros_like(output)\n",
        "#     one_hot[0][target_class] = 1\n",
        "#     output.backward(gradient=one_hot.to(device))\n",
        "\n",
        "#     # Grad-CAMの計算\n",
        "#     activations = features_blobs[0]\n",
        "#     grads = grads_blobs[0]\n",
        "#     weights = np.mean(grads, axis=(2, 3))[0, :]\n",
        "#     cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "#     for i, w in enumerate(weights):\n",
        "#         cam += w * activations[0, i, :, :]\n",
        "#     cam = np.maximum(cam, 0)\n",
        "#     cam = cv2.resize(cam, (numpy_image.shape[1], numpy_image.shape[0]))  # 元の画像のサイズに合わせて調整\n",
        "#     cam = cam - np.min(cam)\n",
        "#     cam = cam / np.max(cam)\n",
        "\n",
        "#     return cam\n"
      ],
      "metadata": {
        "id": "jM8OzXv1ICqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "############\n",
        "## GradCAM ##\n",
        "############\n",
        "\n",
        "def compute_gradcam(model, image_tensor, target_class=None):\n",
        "    model.eval()\n",
        "\n",
        "    features = None\n",
        "    grads = None\n",
        "\n",
        "    def hook_feature(module, input, output):\n",
        "        nonlocal features\n",
        "        features = output\n",
        "\n",
        "    def hook_grad(module, grad_in, grad_out):\n",
        "        nonlocal grads\n",
        "        grads = grad_out[0]\n",
        "\n",
        "    final_conv = model.blocks[-1]  # この部分はモデルの構造により調整が必要\n",
        "    final_conv.register_forward_hook(hook_feature)\n",
        "    final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax().item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        activations = features.cpu().numpy()\n",
        "        grad_values = grads.cpu().numpy()\n",
        "        weights = np.mean(grad_values, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[0, i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (image_tensor.shape[2], image_tensor.shape[3]))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "\n",
        "    # Memory clean-up\n",
        "    del features, grads, activations, grad_values, weights\n",
        "    torch.cuda.empty_cache()  # If you're using CUDA\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "hzvNpNeravOw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "###################################\n",
        "# Inference MobileNetV3 ※メモリ対策バージョン#\n",
        "###################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "image_indices = list(range(0, 665))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1, heatmap_0, heatmap_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "kghHA3ksgYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mtc7lBGOdha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Interference MobileNetV3 (GradCAMなしバージョン)#\n",
        "########################################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, test_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0) #val_datasetを1枚ずつにしてtest_loadeerに格納\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      #target = target.squeeze(1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "\n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "      probs.append(float(prob)) #予測確率\n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "309c9856-3514-4310-a87e-cef1ff667389"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fd611b78fbf7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#define dataset and dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_list_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#val_datasetを1枚ずつにしてtest_loadeerに格納\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleImageDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # Import garbage collector module\n",
        "import os\n",
        "\n",
        "######################################\n",
        "# Inference MobileNetV3 ※GradCAMなし#\n",
        "######################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "paths, targets, probs, preds = [], [], [], []\n",
        "\n",
        "image_indices = list(range(0, 665))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        paths.append(str(os.path.basename(img_path)))\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "JFL4WogdahVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'paths': paths,\n",
        "    'Target': y_label,\n",
        "    'Probability': y_prob,\n",
        "    'Prediction': y_pred\n",
        "})\n",
        "pd.set_option('display.max_rows', 700)\n",
        "\n",
        "file_path = \"/content/result_list.csv\"\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hbDCHbieT3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "itGmZ613_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# #################################################\n",
        "# threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "# #################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "# Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "# Compute the ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc_score(y_label, y_prob):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "# plt.plot(fpr, tpr, marker='o')\n",
        "# plt.xlabel('FPR')\n",
        "# plt.ylabel('TPR')\n",
        "# plt.grid()\n",
        "# print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# confusion matrixをheatmapで表示\n",
        "cm = confusion_matrix(y_label, y_pred, labels=[1, 0])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['TED', 'control'],\n",
        "            yticklabels=['TED', 'control'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CAhawNTE8k8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fbb04ea-69d6-4e3c-f621-d44685963df3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9292168674698795\n",
            "304 28 19 313\n",
            "confusion matrix = \n",
            " [[313  19]\n",
            " [ 28 304]]\n",
            "Accuracy : 0.9292168674698795\n",
            "Precision (true positive rate) : 0.9411764705882353\n",
            "Recall (sensitivity): 0.9156626506024096\n",
            "Specificity : 0.9427710843373494\n",
            "F1 score : 0.9282442748091603\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n"
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh",
        "outputId": "852987b9-62c5-4da4-de93-4fa915bad38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2854.09 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 40/40 [00:00<00:00, 76.30 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 127.54 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1664.59 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69277f-119c-41dc-d348-1bccda7d470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}