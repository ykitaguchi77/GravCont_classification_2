{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_CNN_chizai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetMobileNet_for_chizai**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "b0d1af09-ec80-4477-deab-f5d318ecaee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer --q\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "!pip install albumentations==0.4.6 --q\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "!pip install mediapipe opencv-python --q\n",
        "import mediapipe as mp\n",
        "\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 21 07:47:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0              33W /  70W |    171MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff708c1-148c-4c26-f59f-c1c72b426ae9"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka",
        "outputId": "33888986-e3c6-4c94-91b0-bd255e1cebe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残り時間 : 11.69"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "\n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "c668994f-5304-4169-e860-34e1d4d442f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# ディレクトリのパス\n",
        "directory_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont/images_cropped\"\n",
        "\n",
        "# ディレクトリ内のjpgファイルをリストアップ\n",
        "jpg_files = [f for f in os.listdir(directory_path) if f.endswith('.JPG')]\n",
        "\n",
        "# 1つのjpgファイルを表示\n",
        "if len(jpg_files) > 0:\n",
        "    file_to_display = jpg_files[0]  # 1つ目のファイルを表示\n",
        "    file_path = os.path.join(directory_path, file_to_display)\n",
        "    display(Image(filename=file_path))\n",
        "else:\n",
        "    print(\"指定されたディレクトリ内にjpgファイルが見つかりませんでした。\")\n"
      ],
      "metadata": {
        "id": "OiVhW7PAZhsu",
        "outputId": "40c26488-2cc2-483c-c84a-10fe73f615ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**データセットのpreprocess**"
      ],
      "metadata": {
        "id": "RPDTAk_iNfY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels\n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd",
        "outputId": "2e71d5b7-6470-48a1-adeb-3e281363f62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\\n-----dataset-----train-----images\\n              |         |--labels\\n              |--valid-----images\\n              |         |--labels\\n              |--dataset.yaml\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### フォルダのリセット #######\n",
        "\n",
        "# # MobileNet用に224px四方に成形しておく\n",
        "# dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "# if os.path.exists(dst_folder):\n",
        "#     shutil.rmtree(dst_folder)\n",
        "# os.makedirs(f\"{dst_folder}/train\")\n",
        "# os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:\n",
        "          print(i)\n",
        "          print(in_path + '/' + i)\n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "\n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()\n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "\n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "\n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "\n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min')\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "eI2_SlJUcqDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af1a778-3a8b-4623-b6bf-8714f6c857d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "2649\n",
            "664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train MobileNetV3**"
      ],
      "metadata": {
        "id": "BU8Bw69eN4gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA",
        "outputId": "4a0995d2-3088-442b-ede4-9ac6d9848124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fd3e3e62be3f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-f0dc8281509c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, patience, num_epochs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f0dc8281509c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mpilr_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtensor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilr_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "# Load the model\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/MobileNetV3_aug3.pth\"\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Load the saved state dict\n",
        "model_ft.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "DJGkYXxSExq7",
        "outputId": "94fb3a72-4d84-4e6b-d0e6-fd422796c519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Inference**"
      ],
      "metadata": {
        "id": "N7M5cQ2WCPLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "classes = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "# Assuming you have the 'val_loader', 'model_ft', and 'device' already set up\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Function to display an image\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)\n",
        "\n",
        "model_ft.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(val_loader):\n",
        "        if i >= 10:  # Process only the first 10 images\n",
        "            break\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_ft(inputs)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Display image and predictions\n",
        "        for j in range(inputs.size()[0]):\n",
        "            class_name = classes[preds[j]]\n",
        "            probability = probabilities[j][preds[j]].item()\n",
        "            print(f\"Image {j}: Predicted class - {class_name}, Probability - {probability:.2f}\")\n",
        "            print\n",
        "        imshow(torchvision.utils.make_grid(inputs.cpu()))\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "RY6Y3O17COfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "91wReBK3OCjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH",
        "outputId": "c4d9ac1a-28c5-4f73-ee8f-afb2b57907f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "##################\n",
        "## Define GradCAM ##\n",
        "##################\n",
        "\n",
        "def compute_gradcam(model, image_tensor, target_class=None):\n",
        "    model.eval()\n",
        "\n",
        "    features = None\n",
        "    grads = None\n",
        "\n",
        "    def hook_feature(module, input, output):\n",
        "        nonlocal features\n",
        "        features = output\n",
        "\n",
        "    def hook_grad(module, grad_in, grad_out):\n",
        "        nonlocal grads\n",
        "        grads = grad_out[0]\n",
        "\n",
        "    final_conv = model.blocks[-1]  # この部分はモデルの構造により調整が必要\n",
        "    final_conv.register_forward_hook(hook_feature)\n",
        "    final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax().item()\n",
        "\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        activations = features.cpu().numpy()\n",
        "        grad_values = grads.cpu().numpy()\n",
        "        weights = np.mean(grad_values, axis=(2, 3))[0, :]\n",
        "        cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[0, i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (image_tensor.shape[2], image_tensor.shape[3]))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "\n",
        "    # Memory clean-up\n",
        "    del features, grads, activations, grad_values, weights\n",
        "    torch.cuda.empty_cache()  # If you're using CUDA\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "hzvNpNeravOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dlibを用いた眼周囲抜き出し\n",
        "\n",
        "こちらの方がmediapipeより検出率、精度が高い"
      ],
      "metadata": {
        "id": "mqchGqI3SZT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib --q\n",
        "!pip install opencv-python --q\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bunzip2 shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# カスケードファイルのパス\n",
        "eye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_eye.xml'\n",
        "# righteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_righteye_2splits.xml'\n",
        "# lefteye_cascade_path = '/content/drive/My Drive/Deep_learning/haarcascade_lefteye_2splits.xml'\n",
        "\n",
        "# カスケード分類器の特徴量取得\n",
        "eye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# righteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n",
        "# lefteye_cascade = cv2.CascadeClassifier(eye_cascade_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq_FRvcq0Ilw",
        "outputId": "0a095046-6557-4b24-ceeb-7f258bd05c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 02:10:23--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  13.2MB/s    in 6.7s    \n",
            "\n",
            "2023-12-25 02:10:30 (9.07 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "# mismatched = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = mismatched\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 30))\n",
        "\n",
        "\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "counter_0_overall_correct = 0\n",
        "counter_1 = 0  # img_label == pred.item()\n",
        "counter_2 = 0  # img_label != pred_item\n",
        "counter_3_same_pred_and_mask_correct = 0  # pred.item() == mask_diagnosis and img_label == pred.item()\n",
        "counter_3_same_pred_and_mask_incorrect = 0  # pred.item() == mask_diagnosis and img_lanbel != pred.label\n",
        "counter_4_diff_pred_and_mask_model_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_4_diff_pred_and_mask_mask_correct = 0  # pred.item() != mask_diagnosis and img_label == pred.item()\n",
        "counter_5_matched_pred_1_correct = 0\n",
        "counter_5_matched_pred_0_correct = 0\n",
        "counter_5_matched_pred_1_incorrect = 0\n",
        "counter_5_matched_pred_0_incorrect = 0\n",
        "misclassified_indices, mismatched_indices = [], []\n",
        "mismatched_indices_pred_1, mismatched_indices_pred_0 = [], []\n",
        "mismatched_indices_pred_1_model_correct, mismatched_indices_pred_0_model_correct = [], []\n",
        "mismatched_indices_pred_1_mask_correct, mismatched_indices_pred_0_mask_correct = [], []\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "\n",
        "def crop_bilateral(in_path):\n",
        "    img = cv2.imread(in_path)\n",
        "    grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    mask = np.zeros_like(grayscale_img)\n",
        "    eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "    for (x, y, w, h) in eye_list:\n",
        "        mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "    return mask\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        # if img_label == 1:  #sort by label\n",
        "        # #if img_label == 0:\n",
        "        # #if img_label == 0 or img_label == 1:\n",
        "            #GradCAM\n",
        "            pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "            numpy_image = np.array(pilr_image)\n",
        "            cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "            image_tensor = test_data_transforms(pilr_image)\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "            output = model_ft(image_tensor)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            prob = nn.Softmax(dim=1)(output)[0][1].cpu().detach().item()\n",
        "\n",
        "            cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "            cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "\n",
        "            #注目部位を２値化して表示\n",
        "            mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "            mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "            mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            # inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            #HaarCascadeによる眼周囲抜き出し\n",
        "            periocular_mask = crop_bilateral(img_path)\n",
        "\n",
        "            #GradCAM、眼周囲領域の共通点をマスク化する\n",
        "            common_mask_0 = cv2.bitwise_and(mask_0_resized, periocular_mask)\n",
        "            common_mask_1 = cv2.bitwise_and(mask_1_resized, periocular_mask)\n",
        "\n",
        "            area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "            area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "            mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "\n",
        "\n",
        "            # Convert the common masks to 3-channel images for visualization\n",
        "            common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "            common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "            heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "            heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "\n",
        "            heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "            heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "            overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "            overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "            # 1. 3チャンネルの画像に変換\n",
        "            periocular_colored_mask = cv2.cvtColor(periocular_mask, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "            # 2. overlayed_image_1にperiocular_maskをオーバーレイ\n",
        "            overlayed_image_1_with_mask = cv2.addWeighted(overlayed_image_1, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # 3. overlayed_image_0にperiocular_maskをオーバーレイ\n",
        "            overlayed_image_0_with_mask = cv2.addWeighted(overlayed_image_0, 0.7, periocular_colored_mask, 0.3, 0)\n",
        "\n",
        "            # 結果の表示\n",
        "            result_with_mask = np.hstack([cv2_image, overlayed_image_1_with_mask, overlayed_image_0_with_mask, common_mask_1, common_mask_0])\n",
        "\n",
        "            # Inside your loop\n",
        "            if img_label == pred.item():\n",
        "                counter_0_overall_correct += 1\n",
        "\n",
        "            if pred.item() == mask_diagnosis:\n",
        "                counter_1 += 1\n",
        "                if img_label == pred.item():\n",
        "                    counter_3_same_pred_and_mask_correct += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_correct += 1\n",
        "                        print(\"match_pred_1_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_correct\")\n",
        "                elif img_label != pred.item():\n",
        "                    counter_3_same_pred_and_mask_incorrect += 1\n",
        "                    if pred.item() == 1:\n",
        "                        counter_5_matched_pred_1_incorrect += 1\n",
        "                        print(\"match_pred_1_incorrect\")\n",
        "                    elif pred.item() == 0:\n",
        "                        counter_5_matched_pred_0_correct += 1\n",
        "                        print(\"match_pred_0_incorrect\")\n",
        "\n",
        "            if pred.item() != mask_diagnosis:\n",
        "                counter_2 += 1\n",
        "                mismatched_indices.append(i)\n",
        "                #print(f\"pred: {pred.item()}, mask: {mask_diagnosis}\")\n",
        "                if img_label == pred.item():\n",
        "                    counter_4_diff_pred_and_mask_model_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_model_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_model_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_model_correct\")\n",
        "                elif img_label == mask_diagnosis:\n",
        "                    counter_4_diff_pred_and_mask_mask_correct +=1\n",
        "                    if pred.item() == 1:\n",
        "                        mismatched_indices_pred_1_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_1_mask_correct\")\n",
        "                    elif pred.item() == 0:\n",
        "                        mismatched_indices_pred_0_mask_correct.append(i)\n",
        "                        print(\"mismatch_pred_0_mask_correct\")\n",
        "\n",
        "            # 予測結果と実際のラベルを比較して、TP, TN, FP, FNのカウントを更新\n",
        "            if img_label == 1 and pred.item() == 1:\n",
        "                TP += 1\n",
        "            elif img_label == 0 and pred.item() == 0:\n",
        "                TN += 1\n",
        "            elif img_label == 0 and pred.item() == 1:\n",
        "                FP += 1\n",
        "            elif img_label == 1 and pred.item() == 0:\n",
        "                FN += 1\n",
        "\n",
        "            print(f\"{i}\")\n",
        "            print(f\"{os.path.basename(img_path)}\")\n",
        "            print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "            print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "            ###GradCAM画像を表示（表示しないときはコメントアウトする）\n",
        "            #cv2_imshow(result_with_mask)\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "            if pred.item() != img_label:\n",
        "                misclassified_indices.append(i)\n",
        "\n",
        "            # メモリ解放のための操作\n",
        "            del pilr_image, numpy_image, cv2_image, image_tensor, output, pred\n",
        "            del cam_class_0, cam_class_1, mask_0, mask_1, mask_0_resized, mask_1_resized\n",
        "            del heatmap_0, heatmap_1, heatmap_0_resized, heatmap_1_resized, overlayed_image_0, overlayed_image_1\n",
        "            del common_mask_0, common_mask_1, result_with_mask\n",
        "\n",
        "            # 明示的にガベージコレクションを実行\n",
        "            gc.collect()\n",
        "\n",
        "total_images = len(image_indices)\n",
        "ratio_0 = counter_0_overall_correct*100/total_images\n",
        "ratio_1 = counter_1*100/total_images\n",
        "ratio_2 = counter_2*100/total_images\n",
        "print(f\"Model accuracy: {ratio_0:.3f}% ({counter_0_overall_correct}/{total_images})\")\n",
        "print(f\"Model and GradCAM judgement match: {ratio_1:.3f}% ({counter_1}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_3_same_pred_and_mask_correct*100/counter_1:.3f}% ({counter_3_same_pred_and_mask_correct}/{counter_1})\")\n",
        "print(f\"Model and GradCAM judgement do not match: {ratio_2:.3f}% ({counter_2}/{total_images})\")\n",
        "print(f\"    Model accuracy: {counter_4_diff_pred_and_mask_model_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_model_correct}/{counter_2})\")\n",
        "print(f\"    Mask accuracy: {counter_4_diff_pred_and_mask_mask_correct*100/counter_2:.3f}% ({counter_4_diff_pred_and_mask_mask_correct}/{counter_2})\")\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")\n",
        "print(f\"mismatched_indices: {mismatched_indices}\")\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")\n",
        "\n",
        "# 感度と特異度の計算\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m-m3ZFMou287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have defined your lists and calculated their lengths, for example:\n",
        "pred_0_model_correct_count = len(mismatched_indices_pred_0_model_correct)\n",
        "pred_0_mask_correct_count = len(mismatched_indices_pred_0_mask_correct)\n",
        "pred_1_model_correct_count = len(mismatched_indices_pred_1_model_correct)\n",
        "pred_1_mask_correct_count = len(mismatched_indices_pred_1_mask_correct)\n",
        "\n",
        "# 1. Create a matrix of the counts:\n",
        "matrix = [\n",
        "    [pred_1_model_correct_count, pred_1_mask_correct_count],\n",
        "    [pred_0_model_correct_count, pred_0_mask_correct_count],\n",
        "]\n",
        "\n",
        "# 2. Use seaborn's heatmap to visualize the matrix:\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(font_scale=1.2) # for label size\n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"model_correct\", \"mask_incorrect\"],\n",
        "            yticklabels=[\"model_pred_1\", \"model_pred_0\"])\n",
        "plt.title(\"In case model and mask diagnosis not consistent\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-HDn_QLB690x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy_model_pred_0_inconsistent = pred_0_model_correct_count/(pred_0_model_correct_count+pred_0_mask_correct_count)\n",
        "Accuracy_model_pred_1_inconsistent = pred_1_model_correct_count/(pred_1_model_correct_count+pred_1_mask_correct_count)\n",
        "print(f\"Accuracy_model_pred_1_inconsistent: {Accuracy_model_pred_1_inconsistent*100:.2f}% ({pred_1_model_correct_count}/{pred_1_model_correct_count+pred_1_mask_correct_count})\")\n",
        "print(f\"Accuracy_model_pred_0_inconsistent {Accuracy_model_pred_0_inconsistent*100:.2f}% ({pred_0_model_correct_count}/{pred_0_model_correct_count+pred_0_mask_correct_count})\")\n",
        "Accuracy_model_pred_0_consistent = counter_5_matched_pred_0_correct/(counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect)\n",
        "Accuracy_model_pred_1_consistent = counter_5_matched_pred_1_correct/(counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect)\n",
        "print(f\"Accuracy_model_pred_1_consistent: {Accuracy_model_pred_1_consistent*100:.2f}% ({counter_5_matched_pred_1_correct}/{counter_5_matched_pred_1_correct + counter_5_matched_pred_1_incorrect})\")\n",
        "print(f\"Accuracy_model_pred_0_consistent {Accuracy_model_pred_0_consistent*100:.2f}% ({counter_5_matched_pred_0_correct}/{counter_5_matched_pred_0_correct + counter_5_matched_pred_0_incorrect})\")\n",
        "\n",
        "#モデルと注目点が合致する症例に限定した場合\n",
        "TP = counter_5_matched_pred_1_correct\n",
        "FP = counter_5_matched_pred_1_incorrect\n",
        "FN = counter_5_matched_pred_0_incorrect\n",
        "TN = counter_5_matched_pred_0_correct\n",
        "\n",
        "# 感度と特異度の計算\n",
        "# Sensitivity (True Positive Rate)\n",
        "sensitivity = TP / (TP + FN)\n",
        "# Specificity (True Negative Rate)\n",
        "specificity = TN / (TN + FP)\n",
        "# Precision\n",
        "precision = TP / (TP + FP)\n",
        "# F1 Score (F-value)\n",
        "f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "# Printing the results\n",
        "print(f\"Sensitivity (True Positive Rate): {sensitivity:.3f}\")\n",
        "print(f\"Specificity (True Negative Rate): {specificity:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"F1 Score (F-value): {f1_score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "# After the loop completes and all metrics are calculated, display the confusion matrix.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"          Predicted Grav  |  Predicted Cont\")\n",
        "print(f\"Actual 1:      {TP}      |      {FN}\")\n",
        "print(f\"Actual 0:      {FP}      |      {TN}\")"
      ],
      "metadata": {
        "id": "abEcpyatIMFI",
        "outputId": "f4b75056-3b34-4e3c-ac81-b58ee7316cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy_model_pred_1_inconsistent: 100.00% (4/4)\n",
            "Accuracy_model_pred_0_inconsistent 0.00% (0/2)\n",
            "Accuracy_model_pred_1_consistent: 100.00% (48/48)\n",
            "Accuracy_model_pred_0_consistent 100.00% (6/6)\n",
            "Sensitivity (True Positive Rate): 1.000\n",
            "Specificity (True Negative Rate): 1.000\n",
            "Precision: 1.000\n",
            "F1 Score (F-value): 1.000\n",
            "Confusion Matrix:\n",
            "          Predicted Grav  |  Predicted Cont\n",
            "Actual 1:      48      |      0\n",
            "Actual 0:      0      |      6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ビデオ解析**"
      ],
      "metadata": {
        "id": "Qkcv1Poa8iyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the MP4 file and the directory to save frames\n",
        "video_path = \"/content/drive/MyDrive/Deep_learning/GravAI_iOS/test_movie/Grav_1.mp4\"\n",
        "save_path = \"/content/temp\"\n",
        "\n",
        "# Check if the save directory exists; if so, remove it\n",
        "if os.path.exists(save_path):\n",
        "    os.rmtree(save_path)\n",
        "\n",
        "# Create the directory\n",
        "os.mkdir(save_path)\n",
        "\n",
        "# Load the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Frame counter\n",
        "count = 0\n",
        "\n",
        "# Check if the video is opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Read until video is completed\n",
        "while cap.isOpened():\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        # Save a frame every 10 frames\n",
        "        if count % 10 == 0:\n",
        "            frame_name = os.path.join(save_path, f\"frame_{count}.jpg\")\n",
        "            cv2.imwrite(frame_name, frame)\n",
        "            print(f\"Saved {frame_name}\")\n",
        "\n",
        "        count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "WI0X3akb7fLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iccKxhGN7fRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVESbPIt6WJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbE-WDwM7fUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kkcpd3f67fXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1SgwLyvRaKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###叩き台（ボツ）のコード達\n",
        "\n",
        "Google mediapipeは顔面メッシュが検出できない症例が多いため採用せず"
      ],
      "metadata": {
        "id": "7GOZBL9t-pOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# # 読み込む画像ファイル\n",
        "# IMAGE_FILES = val_list[0]\n",
        "\n",
        "# # Google Mediapipe periocular landmarks #\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.5) as face_mesh:\n",
        "\n",
        "#     image = cv2.imread(IMAGE_FILES)\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       exit(\"No landmarks detected\")\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#         466, 388, 387, 386, 385, 384, 398,\n",
        "#         263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     mask = np.zeros_like(image)\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#         points_right = []\n",
        "#         points_left = []\n",
        "\n",
        "#         for idx in landmarks_list_right:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_right.append([loc_x, loc_y])\n",
        "\n",
        "#         for idx in landmarks_list_left:\n",
        "#             loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#             loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#             points_left.append([loc_x, loc_y])\n",
        "\n",
        "#         cv2.fillPoly(mask, [np.array(points_right)], (255,255,255))\n",
        "#         cv2.fillPoly(mask, [np.array(points_left)], (255,255,255))\n",
        "\n",
        "#     masked_image = cv2.bitwise_and(mask, np.ones_like(image) * 255)\n",
        "#     cv2_imshow(masked_image)\n"
      ],
      "metadata": {
        "id": "vkojIsOFugFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe opencv-python --q\n",
        "\n",
        "# import mediapipe as mp\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import glob\n",
        "\n",
        "# #########################################################\n",
        "# # Google Mediapipe periocular landmarks ※landmarkを検出できない症例がある#\n",
        "# #########################################################\n",
        "\n",
        "# mp_drawing = mp.solutions.drawing_utils\n",
        "# mp_drawing_styles = mp.solutions.drawing_styles\n",
        "# mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# # For static images:\n",
        "\n",
        "# import glob\n",
        "# val_list = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid/*\")[0:100]\n",
        "# IMAGE_FILES = val_list\n",
        "# print(IMAGE_FILES)\n",
        "\n",
        "# drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
        "# with mp_face_mesh.FaceMesh(\n",
        "#     static_image_mode=True,\n",
        "#     max_num_faces=1,\n",
        "#     refine_landmarks=True,\n",
        "#     min_detection_confidence=0.15,\n",
        "#     min_tracking_confidence=0.1) as face_mesh:\n",
        "\n",
        "#   for idx, file in enumerate(IMAGE_FILES):\n",
        "#     print(file)\n",
        "#     image = cv2.imread(file)\n",
        "#     # Convert the BGR image to RGB before processing.\n",
        "#     results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#     # Print and draw face mesh landmarks on the image.\n",
        "#     if not results.multi_face_landmarks:\n",
        "#       print(\"no landmark detected\")\n",
        "#       continue\n",
        "#     annotated_image = image.copy()\n",
        "\n",
        "#     landmarks_list_right = [\n",
        "#         33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
        "#         173, 157, 158, 159, 160, 161, 246,\n",
        "#     ]\n",
        "\n",
        "#     landmarks_list_left = [\n",
        "#     466, 388, 387, 386, 385, 384, 398,\n",
        "#     263, 249, 390, 373, 374, 380, 381, 382, 362\n",
        "#     ]\n",
        "\n",
        "#     for face_landmarks in results.multi_face_landmarks:\n",
        "#       for idx in landmarks_list_right:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (255,0,255), 2)\n",
        "\n",
        "#       for idx in landmarks_list_left:\n",
        "#         loc_x = int(face_landmarks.landmark[idx].x * image.shape[1])\n",
        "#         loc_y = int(face_landmarks.landmark[idx].y * image.shape[0])\n",
        "#         #print(loc_x, loc_y)\n",
        "#         cv2.circle(annotated_image,(loc_x, loc_y), 2, (0,255,255), 2)\n",
        "\n",
        "#     cv2_imshow(annotated_image)\n"
      ],
      "metadata": {
        "id": "3yqasLAdGt9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "##############################################\n",
        "# Inference MobileNetV3 with GradCAM ※メモリ対策バージョン#\n",
        "##############################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1, heatmap_0, heatmap_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "kghHA3ksgYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with 黒塗りGradCAM_白黒バージョン追加 #\n",
        "####################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0])\n",
        "        cv2_imshow(result)\n"
      ],
      "metadata": {
        "id": "0eu241VBwUdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "\n",
        "####################################\n",
        "# Inference MobileNetV3 with 黒塗りGradCAM #\n",
        "####################################\n",
        "\"\"\"\n",
        "GradCAMの注目点を黒塗りで隠してinferenceしたときに、どのように結果が変わるかを確認\n",
        "（結果としてあまり参考にならなさそう）\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "\n",
        "# misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "# image_indices = misclassified\n",
        "\n",
        "#image_indices = list(range(0, 665))\n",
        "image_indices = list(range(0, 3))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 0, 1).astype(np.uint8)\n",
        "\n",
        "        # Resize the masks\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0])\n",
        "        cv2_imshow(result)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # 黒く塗りつぶした画像をTensorに変換\n",
        "        blacked_out_tensor_0 = test_data_transforms(Image.fromarray(blacked_out_image_0)).unsqueeze(0).to(device)\n",
        "        blacked_out_tensor_1 = test_data_transforms(Image.fromarray(blacked_out_image_1)).unsqueeze(0).to(device)\n",
        "\n",
        "        # モデルで評価\n",
        "        output_0 = model_ft(blacked_out_tensor_0)\n",
        "        _, pred_0 = torch.max(output_0, 1)\n",
        "\n",
        "        prob_0 = nn.Softmax(dim=1)(output_0)\n",
        "        prob_0 = prob_0[0][1].cpu().detach()\n",
        "        prob_0 = \"{:.3f}\".format(prob_0.item())\n",
        "\n",
        "        output_1 = model_ft(blacked_out_tensor_1)\n",
        "        _, pred_1 = torch.max(output_1, 1)\n",
        "\n",
        "        prob_1 = nn.Softmax(dim=1)(output_1)\n",
        "        prob_1 = prob_1[0][1].cpu().detach()\n",
        "        prob_1 = \"{:.3f}\".format(prob_1.item())\n",
        "\n",
        "        # 結果を出力\n",
        "        print(f\"Blacked-out image (Class 1): prob: {prob_1}, pred: {pred_1.item()}\")\n",
        "        print(f\"Blacked-out image (Class 0): prob: {prob_0}, pred: {pred_0.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "        # Append results\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        if int(pred) != int(img_label):\n",
        "            misclassified_indices.append(i)\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor, cam_class_0, cam_class_1\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "wCvSB8X5OOkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAMで提示された関心領域のマスクと、HaarCascadeで検出された眼周囲（縦はそのまま、横は幅を半分にした長方形）との重複領域を計算する。\n",
        "GradCAMについてはGravと判定する場合とContと判定する場合とで両方とも計算を行う。\n",
        "重複領域がGrav>Contであればmask_diagnosis = \"Grav\"、重複領域がGrav<Contであればmask_diagnosis = \"Cont\"とする。\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####眼周囲のマスクを作成する\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # リストをNumPyの配列に変換\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPyの表示オプションを変更\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####ここまで\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, blacked_out_image_1, blacked_out_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "Mtc7lBGOdha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import gc  # Import garbage collector module\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "####################################\n",
        "## Comparison GradCAM vs Periocular area ##\n",
        "####################################\n",
        "\"\"\"\n",
        "Protocol:\n",
        "GradCAMで提示された関心領域のマスクと、HaarCascadeで検出された眼周囲（縦はそのまま、横は幅を半分にした長方形）との重複領域を計算する。\n",
        "GradCAMについてはGravと判定する場合とContと判定する場合とで両方とも計算を行う。\n",
        "重複領域がGrav>Contであればmask_diagnosis = \"Grav\"、重複領域がGrav<Contであればmask_diagnosis = \"Cont\"とする。\n",
        "\"\"\"\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "targets, probs, preds, misclassified_indices = [], [], [], []\n",
        "\n",
        "misclassified = [17, 24, 25, 28, 68, 70, 101, 121, 200, 210, 211, 214, 215, 218, 221, 271, 291, 316, 323, 343, 344, 363, 384, 440, 448, 485, 526, 531, 544, 546, 558, 561, 568, 586, 617, 627, 629, 641, 649]\n",
        "image_indices = misclassified\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference of classification\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{i}\")\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        cam_class_0 = compute_gradcam(model_ft, image_tensor, target_class=0)\n",
        "        heatmap_0 = cv2.applyColorMap(np.uint8(255 * cam_class_0), cv2.COLORMAP_JET)\n",
        "        heatmap_0 = cv2.cvtColor(heatmap_0, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        cam_class_1 = compute_gradcam(model_ft, image_tensor, target_class=1)\n",
        "        heatmap_1 = cv2.applyColorMap(np.uint8(255 * cam_class_1), cv2.COLORMAP_JET)\n",
        "        heatmap_1 = cv2.cvtColor(heatmap_1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the heatmaps\n",
        "        heatmap_0_resized = cv2.resize(heatmap_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        heatmap_1_resized = cv2.resize(heatmap_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "        # Overlay original image with the heatmaps\n",
        "        overlayed_image_0 = cv2.addWeighted(cv2_image, 0.5, heatmap_0_resized, 0.5, 0)\n",
        "        overlayed_image_1 = cv2.addWeighted(cv2_image, 0.5, heatmap_1_resized, 0.5, 0)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute Grad-CAM for class 0\n",
        "        mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # Compute Grad-CAM for class 1\n",
        "        mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # 既存のマスクを反転させる\n",
        "        inverse_mask_0 = np.where(cam_class_0 > 0.5, 1, 0).astype(np.uint8)\n",
        "        inverse_mask_1 = np.where(cam_class_1 > 0.5, 1, 0).astype(np.uint8)\n",
        "\n",
        "        # マスクをリサイズ\n",
        "        mask_0_resized = cv2.resize(mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        mask_1_resized = cv2.resize(mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_0_resized = cv2.resize(inverse_mask_0, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "        inverse_mask_1_resized = cv2.resize(inverse_mask_1, (cv2_image.shape[1], cv2_image.shape[0]))\n",
        "\n",
        "\n",
        "        #####眼周囲のマスクを作成する\n",
        "        def crop_bilateral(in_path, class_num, showImage=False):\n",
        "            img_resized_list, side_list = [], []\n",
        "\n",
        "            img = cv2.imread(in_path)\n",
        "            img2 = img.copy()\n",
        "\n",
        "            # Convert to grayscale for eye detection\n",
        "            grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Create a black mask of the same size as the grayscale image\n",
        "            mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "            # Detect eyes\n",
        "            eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "            #print(\"\\nimage path = \", in_path)\n",
        "\n",
        "            if len(eye_list) >= 1:\n",
        "                #print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "                for (x, y, w, h) in eye_list:\n",
        "                    # Fill the detected eye region with value 255\n",
        "                    mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "            else:\n",
        "                print(\"no eye detected\")\n",
        "\n",
        "            #print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "            # Show the mask if required using matplotlib for inline display in Colab\n",
        "            if showImage:\n",
        "                plt.imshow(mask, cmap='gray')\n",
        "                plt.axis('off')  # Hide axis\n",
        "                plt.show()\n",
        "\n",
        "            return mask\n",
        "\n",
        "        # Assuming you have initialized eye_cascade before\n",
        "        # eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "        test_path = img_path\n",
        "        periocular_mask =crop_bilateral(test_path, class_num=0)\n",
        "\n",
        "\n",
        "        def show_list_in_numpy(py_list):\n",
        "            # リストをNumPyの配列に変換\n",
        "            np_array = np.array(py_list)\n",
        "            # NumPyの表示オプションを変更\n",
        "            np.set_printoptions(threshold=np.inf)\n",
        "            print(np_array)\n",
        "\n",
        "        #####ここまで\n",
        "\n",
        "        # For inverse_mask_0 and mask\n",
        "        common_mask_0 = cv2.bitwise_and(inverse_mask_0_resized, periocular_mask)\n",
        "\n",
        "        # For inverse_mask_1 and mask\n",
        "        common_mask_1 = cv2.bitwise_and(inverse_mask_1_resized, periocular_mask)\n",
        "\n",
        "        # Overlay original image with the masks\n",
        "        blacked_out_image_0 = cv2_image * np.stack([mask_0_resized]*3, axis=-1)\n",
        "        blacked_out_image_1 = cv2_image * np.stack([mask_1_resized]*3, axis=-1)\n",
        "\n",
        "        # 黒塗り部分以外を白で塗りつぶす\n",
        "        white_blacked_out_image_0 = (inverse_mask_0_resized * 255).astype(np.uint8)\n",
        "        white_blacked_out_image_1 = (inverse_mask_1_resized * 255).astype(np.uint8)\n",
        "\n",
        "        # 2次元の白マスクを3次元に変換\n",
        "        white_blacked_out_image_0 = np.stack([white_blacked_out_image_0]*3, axis=-1)\n",
        "        white_blacked_out_image_1 = np.stack([white_blacked_out_image_1]*3, axis=-1)\n",
        "\n",
        "        # Convert the common masks to 3-channel images for visualization\n",
        "        common_mask_0 = np.stack([common_mask_0]*3, axis=-1) * 255\n",
        "        common_mask_1 = np.stack([common_mask_1]*3, axis=-1) * 255\n",
        "\n",
        "        # Calculate the areas\n",
        "        area_common_mask_0 = np.sum(common_mask_0) / 255\n",
        "        area_common_mask_1 = np.sum(common_mask_1) / 255\n",
        "        # Determine the mask_diagnosis based on the areas\n",
        "        mask_diagnosis = 0 if area_common_mask_0 > area_common_mask_1 else 1\n",
        "\n",
        "        print(f\"mask_diagnosis = {mask_diagnosis}\")\n",
        "\n",
        "\n",
        "        # Concatenate the images\n",
        "        result = np.hstack([cv2_image, overlayed_image_1, overlayed_image_0, white_blacked_out_image_1, white_blacked_out_image_0, common_mask_1, common_mask_0])\n",
        "\n",
        "        cv2_imshow(result)\n",
        "        print(\"\")\n"
      ],
      "metadata": {
        "id": "ap-a6G9ou27W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "####. HaarCascadeのテスト用 #####\n",
        "###########################\n",
        "\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def crop_bilateral(in_path, class_num, showImage=True):\n",
        "#     img_resized_list, side_list = [], []\n",
        "\n",
        "#     img = cv2.imread(in_path)\n",
        "#     img2 = img.copy()\n",
        "\n",
        "#     # Convert to grayscale for eye detection\n",
        "#     grayscale_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "#     # Create a black mask of the same size as the grayscale image\n",
        "#     mask = np.zeros_like(grayscale_img)\n",
        "\n",
        "#     # Detect eyes\n",
        "#     eye_list = eye_cascade.detectMultiScale(grayscale_img, minSize=(40, 40))\n",
        "#     print(\"\\nimage path = \", in_path)\n",
        "\n",
        "#     if len(eye_list) >= 1:\n",
        "#         print('目が' + str(len(eye_list)) + '個検出されました')\n",
        "#         for (x, y, w, h) in eye_list:\n",
        "#             # Fill the detected eye region with value 255\n",
        "#             mask[y:y+h, int(x+w/4):int(x+3*w/4)] = 255\n",
        "#     else:\n",
        "#         print(\"no eye detected\")\n",
        "\n",
        "#     print(f\"eye_list: {eye_list}\")\n",
        "\n",
        "#     # Show the mask if required using matplotlib for inline display in Colab\n",
        "#     if showImage:\n",
        "#         plt.imshow(mask, cmap='gray')\n",
        "#         plt.axis('off')  # Hide axis\n",
        "#         plt.show()\n",
        "\n",
        "#     return mask\n",
        "\n",
        "# # Assuming you have initialized eye_cascade before\n",
        "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
        "\n",
        "# # Test\n",
        "# # Assuming val_list and class_num are previously defined\n",
        "# test_path = val_list[0]\n",
        "# crop_bilateral(test_path, class_num=0)\n"
      ],
      "metadata": {
        "id": "0G6x2rv9WnIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# Interference MobileNetV3 (GradCAMなしバージョン)#\n",
        "########################################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, test_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0) #val_datasetを1枚ずつにしてtest_loadeerに格納\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      #target = target.squeeze(1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "\n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "      probs.append(float(prob)) #予測確率\n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference MobileNet3"
      ],
      "metadata": {
        "id": "XvjkKZez_IrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # Import garbage collector module\n",
        "import os\n",
        "\n",
        "######################################\n",
        "# Inference MobileNetV3 ※GradCAMなし#\n",
        "######################################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval()  # prep model for evaluation\n",
        "paths, targets, probs, preds = [], [], [], []\n",
        "\n",
        "image_indices = list(range(0, 665))\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    if i in image_indices:\n",
        "        pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert the PIL Image to numpy array\n",
        "        numpy_image = np.array(pilr_image)\n",
        "        cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "        image_tensor = test_data_transforms(pilr_image)\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model_ft(image_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        prob = nn.Softmax(dim=1)(output)  # calculate probability\n",
        "        prob = prob[0][1].cpu().detach()  # probability of being positive\n",
        "        prob = \"{:.3f}\".format(prob.item())\n",
        "        print(f\"{os.path.basename(img_path)}\")\n",
        "        print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "        print(\"\")\n",
        "\n",
        "        # Append results\n",
        "        paths.append(str(os.path.basename(img_path)))\n",
        "        probs.append(float(prob))  # predicted probability\n",
        "        preds.append(int(pred))  # predicted result\n",
        "        targets.append(int(img_label))  # label\n",
        "\n",
        "        # Cleanup\n",
        "        del pilr_image, numpy_image, cv2_image, image_tensor\n",
        "        gc.collect()  # Run garbage collector\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n",
        "print(f\"misclassified_indices: {misclassified_indices}\")"
      ],
      "metadata": {
        "id": "JFL4WogdahVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'paths': paths,\n",
        "    'Target': y_label,\n",
        "    'Probability': y_prob,\n",
        "    'Prediction': y_pred\n",
        "})\n",
        "pd.set_option('display.max_rows', 700)\n",
        "\n",
        "file_path = \"/content/result_list.csv\"\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hbDCHbieT3Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "itGmZ613_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# #################################################\n",
        "# threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "# #################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "# Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "# Compute the ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc_score(y_label, y_prob):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "# plt.plot(fpr, tpr, marker='o')\n",
        "# plt.xlabel('FPR')\n",
        "# plt.ylabel('TPR')\n",
        "# plt.grid()\n",
        "# print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# confusion matrixをheatmapで表示\n",
        "cm = confusion_matrix(y_label, y_pred, labels=[1, 0])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['TED', 'control'],\n",
        "            yticklabels=['TED', 'control'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CAhawNTE8k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n"
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh",
        "outputId": "852987b9-62c5-4da4-de93-4fa915bad38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2854.09 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 40/40 [00:00<00:00, 76.30 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 127.54 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1664.59 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69277f-119c-41dc-d348-1bccda7d470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}