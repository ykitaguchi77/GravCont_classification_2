{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "feecb2d6ee5743538bcd76766eb1bb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c05fbaf63100404fb94de617440f5318",
              "IPY_MODEL_0de329e52bac4218a34000cc455849d4",
              "IPY_MODEL_28c728eacd734b00b42cac38d42a79f8"
            ],
            "layout": "IPY_MODEL_7769130d28ee4008909cc8c88ef4e6a8"
          }
        },
        "c05fbaf63100404fb94de617440f5318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d0982fb14e4cdca85208fd39de849a",
            "placeholder": "​",
            "style": "IPY_MODEL_26d18dfbc11a452e85095eaf18ae231d",
            "value": "model.safetensors: 100%"
          }
        },
        "0de329e52bac4218a34000cc455849d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab10368d94494d79bd5a8151ab2c45a3",
            "max": 22058321,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f70acb46ef894d2c823a78a654584c71",
            "value": 22058321
          }
        },
        "28c728eacd734b00b42cac38d42a79f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af934eaf1e554dd3bfbe8d6fb0d870ac",
            "placeholder": "​",
            "style": "IPY_MODEL_692fd6c59164463ab46b36395d77657a",
            "value": " 22.1M/22.1M [00:05&lt;00:00, 4.09MB/s]"
          }
        },
        "7769130d28ee4008909cc8c88ef4e6a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d0982fb14e4cdca85208fd39de849a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d18dfbc11a452e85095eaf18ae231d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab10368d94494d79bd5a8151ab2c45a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f70acb46ef894d2c823a78a654584c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af934eaf1e554dd3bfbe8d6fb0d870ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692fd6c59164463ab46b36395d77657a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_2/blob/main/Extend_dataset_CNN_chizai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GO extend datasetMobileNet_for_chizai**"
      ],
      "metadata": {
        "id": "LXR--60tO5mS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xvbecME5IS",
        "outputId": "8d86ae1f-fda4-4fcc-a396-87b9d4c925f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import codecs\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "import re\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import pickle\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torch_optimizer\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!pip install albumentations==0.4.6\n",
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "random_seed = 3 #shuffleのシード\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (2.0.1+cu118)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.5.0->torch_optimizer) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.6) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.6) (1.11.3)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.6) (6.0.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.6) (4.8.0.76)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.31.5)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.0.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Mon Oct 16 14:28:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZI4pHmFDXZ"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrhEditFGkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4efa48-da1d-4f23-cf11-cb616c8a1bad"
      },
      "source": [
        "'''\n",
        "・dlibを用いて目を切り抜く\n",
        "・横幅を2倍、縦幅を上に1倍追加/下に0.5倍追加した両眼の画像が含まれるように切り取る（目の全幅、眉毛が含まれるように）\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7fAuwXxNka"
      },
      "source": [
        "#残り時間確認\n",
        "!cat /proc/uptime | awk '{printf(\"残り時間 : %.2f\", 12-$1/60/60)}'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSfusHMWPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSA2Rm9MFXoZ"
      },
      "source": [
        "# # GO_extended_datasetを colab上のフォルダに展開\n",
        "# zip_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/GO_extended_dataset.zip'\n",
        "# !unzip $zip_path -d \"/content\"\n",
        "# in_path_list  = ['/content/GO_extended_dataset/Control_photo_1886mai', '/content/GO_extended_dataset/treatable']\n",
        "# #保存先フォルダ\n",
        "# out_path_list = ['/content/GO_extended_dataset/cont', '/content/GO_extended_dataset/grav']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MobileNetV3 training用フォルダを作成**\n",
        "\n",
        "datasetをtrainとvalに分ける\n",
        "\n",
        "https://book.st-hakky.com/docs/object-detection-yolov5-tutorial/\n",
        "\n"
      ],
      "metadata": {
        "id": "VPi74ZCZrVDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# periocular_for_YOLOフォルダにすでに展開されているデータセットを用いる\n",
        "dataset_dir = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO\"\n",
        "\n",
        "# def split_dataset(dataset_dir):\n",
        "#     img_list = glob.glob(f\"{dataset_dir}/images/*\")\n",
        "#     img_train, img_test = train_test_split(img_list, test_size=0.3, random_state=0)\n",
        "\n",
        "#     # img_train, img_testに名前が一致するtxtファイルを抜き出す\n",
        "#     label_train = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_train]\n",
        "#     label_test = [f\"{dataset_dir}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in img_test]\n",
        "\n",
        "#     print(f\"train: {len(label_train)},test: {len(label_test)}\")\n",
        "\n",
        "#     return img_train, img_test, label_train, label_test\n",
        "\n",
        "def make_path_list(dir, class_name):\n",
        "    image_list =  [file for file in glob.glob(f\"{dir}/{class_name}/images/*\") if os.path.isfile(file) == True ]\n",
        "    label_list =  [f\"{dir}/{class_name}/labels/{os.path.basename(i).split('.')[0]}.txt\" for i in image_list]\n",
        "\n",
        "    id_list = [os.path.basename(i).split(\"-\")[0].split(\".\")[0] for i in image_list]\n",
        "\n",
        "    index = {}\n",
        "    id_idx = []\n",
        "    for item in id_list:\n",
        "        if item in index:\n",
        "            id_idx.append(index[item])\n",
        "        else:\n",
        "            index[item] = len(index) + 1\n",
        "            id_idx.append(index[item])\n",
        "    id_idx = [int(i) for i in id_idx]\n",
        "\n",
        "    return image_list, label_list, id_idx\n",
        "\n",
        "grav_image_list, grav_label_list, grav_id_idx = make_path_list(dataset_dir, \"grav\")\n",
        "cont_image_list, cont_label_list, cont_id_idx = make_path_list(dataset_dir, \"cont\")\n",
        "\n",
        "print(f\"grav: {len(grav_image_list)}\")\n",
        "print(f\"cont: {len(cont_image_list)}\")"
      ],
      "metadata": {
        "id": "hHiTlYEnLx_u",
        "outputId": "673bf958-6244-4175-cbb9-35bacbc58678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grav: 1657\n",
            "cont: 1656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "# ディレクトリのパス\n",
        "directory_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO/cont/images_cropped\"\n",
        "\n",
        "# ディレクトリ内のjpgファイルをリストアップ\n",
        "jpg_files = [f for f in os.listdir(directory_path) if f.endswith('.JPG')]\n",
        "\n",
        "# 1つのjpgファイルを表示\n",
        "if len(jpg_files) > 0:\n",
        "    file_to_display = jpg_files[0]  # 1つ目のファイルを表示\n",
        "    file_path = os.path.join(directory_path, file_to_display)\n",
        "    display(Image(filename=file_path))\n",
        "else:\n",
        "    print(\"指定されたディレクトリ内にjpgファイルが見つかりませんでした。\")\n"
      ],
      "metadata": {
        "id": "OiVhW7PAZhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################\n",
        "# YOLOv5向けにGroupKfoldで仕分けられたデータセットがあるのでこれを用いる　　#\n",
        "######################################################\n",
        "\"\"\"\n",
        "\n",
        "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "-----dataset-----train-----images\n",
        "              |         |--labels\n",
        "              |--valid-----images\n",
        "              |         |--labels\n",
        "              |--dataset.yaml\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lwjK1LdfR4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet用に224px四方に成形しておく\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "os.makedirs(f\"{dst_folder}/train\")\n",
        "os.makedirs(f\"{dst_folder}/valid\")"
      ],
      "metadata": {
        "id": "miH-lJQvSwfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(in_path, out_path, processing_file):\n",
        "    #処理時間の計測\n",
        "    start = time.time()\n",
        "\n",
        "    l=0\n",
        "    for i in processing_file:\n",
        "          img = Image.open(in_path + '/' + i)\n",
        "          img_new = expand2square(img, (0, 0, 0)).resize((250, 250))\n",
        "          img_new.save(out_path +'/'+ i)\n",
        "          print(out_path +'/'+ i)\n",
        "\n",
        "          #切り取った画像を表示\n",
        "          #plt.imshow(np.asarray(img_new))\n",
        "          #plt.show()\n",
        "\n",
        "    print('Process done!!')\n",
        "    elapsed_time = time.time() - start\n",
        "    print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
        "\n",
        "def expand2square(pil_img, background_color):\n",
        "    width, height = pil_img.size\n",
        "    if width == height:\n",
        "        return pil_img\n",
        "    elif width > height:\n",
        "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
        "        result.paste(pil_img, (0, (width-height)//2))\n",
        "        return result\n",
        "    else:\n",
        "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
        "        result.paste(pil_img, (0, (height - width) // 2))\n",
        "        return result\n",
        "\n",
        "def showInfo(in_path):\n",
        "    #処理するDirectoryの設定\n",
        "    file = os.listdir(in_path)\n",
        "    print(len(file))\n",
        "\n",
        "    #ここにフォルダ番号を記載する (ex. [0:999])\n",
        "    processing_file = file[0:]\n",
        "    print(processing_file)\n",
        "    len(processing_file)\n",
        "    return processing_file"
      ],
      "metadata": {
        "id": "E-rHgY4lSwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)\n",
        "\n",
        "#元画像フォルダ\n",
        "in_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images'\n",
        "\n",
        "#保存先フォルダ\n",
        "out_path = '/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid'\n",
        "if not os.path.exists(out_path):\n",
        "    os.makedirs(out_path)\n",
        "\n",
        "processing_file = showInfo(in_path)\n",
        "convert(in_path, out_path, processing_file)"
      ],
      "metadata": {
        "id": "kkKYHDOASwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modules**"
      ],
      "metadata": {
        "id": "JKyZzzRveEVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PX = 224 #画像のサイズ\n",
        "TRAIN_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "TRAIN_CROP_SCALE =(0.9,1.0)\n",
        "#TRAIN_BRIGHTNESS_PARAM = 0.2\n",
        "#TRAIN_CONTRAST_PARAM = 0.1\n",
        "#TRAIN_SATURATION_PARAM = 0.1\n",
        "TRAIN_RANDOM_ROTATION = 3\n",
        "TRAIN_HUE_PARAM = 0.02\n",
        "VAL_NORMALIZE_PARAM = [0.494, 0.296, 0.197], [0.14,  0.114, 0.072]\n",
        "\n",
        "class Expand2square(object):\n",
        "    \"\"\"\n",
        "    長方形の元画像を長辺を1辺とする正方形に貼り付け、空白を黒く塗りつぶす\n",
        "    \"\"\"\n",
        "    def __init__(self, background_color):\n",
        "        self.background_color = background_color\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        width, height = pil_img.size\n",
        "        if width == height:\n",
        "            return pil_img\n",
        "        elif width > height:\n",
        "            result = Image.new(pil_img.mode, (width, width), self.background_color)\n",
        "            result.paste(pil_img, (0, (width-height)//2))\n",
        "            return result\n",
        "        else:\n",
        "            result = Image.new(pil_img.mode, (height, height), self.background_color)\n",
        "            result.paste(pil_img, (0, (height - width) // 2))\n",
        "            return result\n",
        "\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, img_list, label_list, transform):\n",
        "        self.transform = transform\n",
        "        self.img_list = img_list\n",
        "        self.label_list = label_list\n",
        "        self.item_dict = {}\n",
        "        self.age = []\n",
        "        #print(img_list)\n",
        "        #print(label_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.img_list[idx]\n",
        "        pilr_image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor_image = self.transform(pilr_image)\n",
        "        target = torch.tensor(self.label_list[idx])\n",
        "        return tensor_image, target\n",
        "\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Defining early stopping class\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # Set model to training mode\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            #普通はこちらを使う\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            # Runs the forward pass with autocasting.\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL1ノルムの絶対値を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l1_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l1_loss = l1_loss + abs(torch.norm(w))\n",
        "            # loss = loss + lam * l1_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            ##################################\n",
        "            ##パラメータのL2ノルムの二乗を損失関数に足す##\n",
        "            ##################################\n",
        "            # lam=1e-3\n",
        "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
        "            # for w in model_ft.parameters():\n",
        "            #     l2_loss = l2_loss + torch.norm(w)**2\n",
        "            # loss = loss + lam * l2_loss\n",
        "            ##################################\n",
        "            ##################################\n",
        "\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            # record training loss\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "\n",
        "        #print()\n",
        "        train_acc = running_corrects.item()/len(train_dataset)\n",
        "\n",
        "        #####################\n",
        "        # validate the model#\n",
        "        #####################\n",
        "\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            \"\"\"\n",
        "            print(\"preds:\"+str(preds))\n",
        "            print(\"labels:\"+str(labels))\n",
        "            print(\"running_corrects: \"+str(str(running_corrects)))\n",
        "            \"\"\"\n",
        "\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "            running_corrects += torch.sum(preds==labels)\n",
        "        val_acc = running_corrects.item()/len(val_dataset)\n",
        "\n",
        "\n",
        "\n",
        "        # print training/validation statistics\n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "\n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'train_acc: {train_acc:.5f}' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' +\n",
        "                     f'valid_acc: {val_acc:.5f}')\n",
        "        print(print_msg)\n",
        "\n",
        "        \"\"\"\n",
        "        #Scheduler step for ReduceLROnPlateau\n",
        "        scheduler.step(valid_loss)\n",
        "        \"\"\"\n",
        "\n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed,\n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return model, avg_train_losses, avg_valid_losses\n",
        "\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves,class_names):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == class_names[0]:\n",
        "                  y_true.append(0)\n",
        "            elif i == class_names[1]:\n",
        "                  y_true.append(1)\n",
        "\n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "\n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob, class_names):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == class_names[0]:\n",
        "              y_true.append(0)\n",
        "        elif i == class_names[1]:\n",
        "              y_true.append(1)\n",
        "\n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "\n",
        "    print(y_true)\n",
        "    print(len(y_true))\n",
        "    print(y_score)\n",
        "    print(len(y_score))\n",
        "\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTNNlLU_cp_b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "## Deplpy MobileNetV3\n",
        "##############################################\n",
        "\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#https://blog.knjcode.com/adabound-memo/\n",
        "#https://pypi.org/project/torch-optimizer/\n",
        "!pip install ranger_adabelief --q\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-8, betas=(0.9,0.999), weight_decay=1e-2, weight_decouple=True)\n",
        "\n",
        "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min')\n",
        "\n",
        "##############################################\n",
        "## Data augumentation\n",
        "##############################################\n",
        "\n",
        "TRAIN_RANDOM_ROTATION = 15\n",
        "TRAIN_CROP_SCALE = (0.8,1.1)\n",
        "TRAIN_CROP_RATE = (0.9, 1.11)\n",
        "PX = 224\n",
        "\n",
        "class GaussianBlur():\n",
        "    def __init__(self, kernel_size, sigma_min=0.1, sigma_max=2.0):\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        sigma = np.random.uniform(self.sigma_min, self.sigma_max)\n",
        "        img = cv2.GaussianBlur(np.array(img), (self.kernel_size, self.kernel_size), sigma)\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "train_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.RandomResizedCrop(PX, scale=TRAIN_CROP_SCALE, ratio=TRAIN_CROP_RATE),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomGrayscale(p=0.01),\n",
        "                transforms.RandomEqualize(p=0.01),\n",
        "                transforms.RandomPerspective(distortion_scale=0.6, p=0.1),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "val_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.RandomRotation(degrees=TRAIN_RANDOM_ROTATION),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "test_data_transforms = transforms.Compose([\n",
        "                #Expand2square((0,0,0)),\n",
        "                transforms.Resize(PX),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "##############################################\n",
        "## Dataset and dataloader\n",
        "##############################################\n",
        "train_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "val_csv_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "train_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/train\"\n",
        "val_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training/valid\"\n",
        "\n",
        "\n",
        "def extract_list(csv_path, parent_path): #parent_pathは画像を格納しているフォルダ\n",
        "    df = pd.read_csv(csv_path, index_col=None)\n",
        "    image_list = [os.path.join(parent_path, os.path.basename(i)) for i in df[\"image_path\"]]\n",
        "    label_list = df[\"label\"]\n",
        "    return image_list, label_list\n",
        "\n",
        "train_list, train_list_label = extract_list(train_csv_path, train_parent_path)\n",
        "val_list, val_list_label = extract_list(val_csv_path, val_parent_path)\n",
        "\n",
        "#define dataset and dataloader\n",
        "train_dataset = SimpleImageDataset(train_list, train_list_label, train_data_transforms)\n",
        "val_dataset = SimpleImageDataset(val_list, val_list_label, val_data_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True, pin_memory=True, num_workers=0)\n",
        "\n",
        "print(len(train_list))\n",
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "eI2_SlJUcqDX",
        "outputId": "0cdcfc41-fba8-45f6-a68d-8f68e3054172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "feecb2d6ee5743538bcd76766eb1bb98",
            "c05fbaf63100404fb94de617440f5318",
            "0de329e52bac4218a34000cc455849d4",
            "28c728eacd734b00b42cac38d42a79f8",
            "7769130d28ee4008909cc8c88ef4e6a8",
            "f1d0982fb14e4cdca85208fd39de849a",
            "26d18dfbc11a452e85095eaf18ae231d",
            "ab10368d94494d79bd5a8151ab2c45a3",
            "f70acb46ef894d2c823a78a654584c71",
            "af934eaf1e554dd3bfbe8d6fb0d870ac",
            "692fd6c59164463ab46b36395d77657a"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feecb2d6ee5743538bcd76766eb1bb98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "2649\n",
            "664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, num_epochs=40)\n"
      ],
      "metadata": {
        "id": "eyRGmXWCcqFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28102f7b-4c29-4693-af62-826aebbe5452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "Epoch: [ 0/40\n",
            "train_loss: 0.61335 train_acc: 0.66667\n",
            "valid_loss: 0.40717 valid_acc: 0.86145\n",
            "Validation loss decreased (inf --> 0.407168).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 1/40\n",
            "train_loss: 0.37395 train_acc: 0.84032\n",
            "valid_loss: 0.24501 valid_acc: 0.92169\n",
            "Validation loss decreased (0.407168 --> 0.245006).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 2/40\n",
            "train_loss: 0.28594 train_acc: 0.88411\n",
            "valid_loss: 0.23358 valid_acc: 0.91265\n",
            "Validation loss decreased (0.245006 --> 0.233578).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 3/40\n",
            "train_loss: 0.23888 train_acc: 0.89996\n",
            "valid_loss: 0.23424 valid_acc: 0.90813\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 4/40\n",
            "train_loss: 0.21445 train_acc: 0.90827\n",
            "valid_loss: 0.25156 valid_acc: 0.89910\n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 5/40\n",
            "train_loss: 0.17319 train_acc: 0.93394\n",
            "valid_loss: 0.21677 valid_acc: 0.91265\n",
            "Validation loss decreased (0.233578 --> 0.216766).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 6/40\n",
            "train_loss: 0.17908 train_acc: 0.92676\n",
            "valid_loss: 0.20358 valid_acc: 0.93072\n",
            "Validation loss decreased (0.216766 --> 0.203576).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [ 7/40\n",
            "train_loss: 0.14128 train_acc: 0.94677\n",
            "valid_loss: 0.25220 valid_acc: 0.90211\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 8/40\n",
            "train_loss: 0.12755 train_acc: 0.94866\n",
            "valid_loss: 0.35130 valid_acc: 0.86145\n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [ 9/40\n",
            "train_loss: 0.12434 train_acc: 0.95092\n",
            "valid_loss: 0.24372 valid_acc: 0.90361\n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [10/40\n",
            "train_loss: 0.10536 train_acc: 0.95998\n",
            "valid_loss: 0.25982 valid_acc: 0.90060\n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [11/40\n",
            "train_loss: 0.09588 train_acc: 0.95961\n",
            "valid_loss: 0.26691 valid_acc: 0.90361\n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [12/40\n",
            "train_loss: 0.09571 train_acc: 0.96225\n",
            "valid_loss: 0.19275 valid_acc: 0.93675\n",
            "Validation loss decreased (0.203576 --> 0.192750).  Saving model ...\n",
            "\n",
            "----------\n",
            "Epoch: [13/40\n",
            "train_loss: 0.07129 train_acc: 0.97622\n",
            "valid_loss: 0.32195 valid_acc: 0.90211\n",
            "EarlyStopping counter: 1 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [14/40\n",
            "train_loss: 0.06730 train_acc: 0.97697\n",
            "valid_loss: 0.30370 valid_acc: 0.90361\n",
            "EarlyStopping counter: 2 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [15/40\n",
            "train_loss: 0.05909 train_acc: 0.97924\n",
            "valid_loss: 0.23611 valid_acc: 0.92169\n",
            "EarlyStopping counter: 3 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [16/40\n",
            "train_loss: 0.06027 train_acc: 0.97584\n",
            "valid_loss: 0.36340 valid_acc: 0.87199\n",
            "EarlyStopping counter: 4 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [17/40\n",
            "train_loss: 0.06573 train_acc: 0.97773\n",
            "valid_loss: 0.26719 valid_acc: 0.92018\n",
            "EarlyStopping counter: 5 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [18/40\n",
            "train_loss: 0.06566 train_acc: 0.97508\n",
            "valid_loss: 0.28477 valid_acc: 0.92018\n",
            "EarlyStopping counter: 6 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [19/40\n",
            "train_loss: 0.05077 train_acc: 0.98075\n",
            "valid_loss: 0.25749 valid_acc: 0.91717\n",
            "EarlyStopping counter: 7 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [20/40\n",
            "train_loss: 0.04894 train_acc: 0.98301\n",
            "valid_loss: 0.26610 valid_acc: 0.91717\n",
            "EarlyStopping counter: 8 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [21/40\n",
            "train_loss: 0.06488 train_acc: 0.97886\n",
            "valid_loss: 0.27474 valid_acc: 0.92018\n",
            "EarlyStopping counter: 9 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [22/40\n",
            "train_loss: 0.04239 train_acc: 0.98301\n",
            "valid_loss: 0.35517 valid_acc: 0.90663\n",
            "EarlyStopping counter: 10 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [23/40\n",
            "train_loss: 0.04676 train_acc: 0.98226\n",
            "valid_loss: 0.28665 valid_acc: 0.92319\n",
            "EarlyStopping counter: 11 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [24/40\n",
            "train_loss: 0.03818 train_acc: 0.98716\n",
            "valid_loss: 0.32874 valid_acc: 0.91416\n",
            "EarlyStopping counter: 12 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [25/40\n",
            "train_loss: 0.03238 train_acc: 0.99018\n",
            "valid_loss: 0.28866 valid_acc: 0.91867\n",
            "EarlyStopping counter: 13 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [26/40\n",
            "train_loss: 0.04297 train_acc: 0.98414\n",
            "valid_loss: 0.26649 valid_acc: 0.92620\n",
            "EarlyStopping counter: 14 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [27/40\n",
            "train_loss: 0.02923 train_acc: 0.99245\n",
            "valid_loss: 0.30264 valid_acc: 0.91114\n",
            "EarlyStopping counter: 15 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [28/40\n",
            "train_loss: 0.05128 train_acc: 0.98075\n",
            "valid_loss: 0.33236 valid_acc: 0.90964\n",
            "EarlyStopping counter: 16 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [29/40\n",
            "train_loss: 0.03485 train_acc: 0.98981\n",
            "valid_loss: 0.28354 valid_acc: 0.92319\n",
            "EarlyStopping counter: 17 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [30/40\n",
            "train_loss: 0.05099 train_acc: 0.98188\n",
            "valid_loss: 0.27109 valid_acc: 0.90663\n",
            "EarlyStopping counter: 18 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [31/40\n",
            "train_loss: 0.04210 train_acc: 0.98414\n",
            "valid_loss: 0.25527 valid_acc: 0.92922\n",
            "EarlyStopping counter: 19 out of 20\n",
            "\n",
            "----------\n",
            "Epoch: [32/40\n",
            "train_loss: 0.02351 train_acc: 0.99245\n",
            "valid_loss: 0.27320 valid_acc: 0.92319\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "torch.save(model_ft.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "51XIFf-q-3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# Load model 飛ばして下さい\n",
        "##########################\n",
        "!pip install timm --q\n",
        "import timm\n",
        "\n",
        "model_ft = timm.create_model('mobilenetv3_large_100', pretrained=True)\n",
        "num_ftrs = model_ft.classifier.in_features\n",
        "model_ft.classifier = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "PATH = f\"{model_parent_path}/MobileNetV3_aug3.pth\"\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "WfcuMSDM-3gH",
        "outputId": "78147cc7-da8d-45c2-9847-cfc47f4ec5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dShbfug9CYD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "uJp342k-ICo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_gradcam(model, image_tensor, target_class=None):\n",
        "    model.eval()\n",
        "\n",
        "    # 最終畳み込み層の出力とその勾配を取得するためのhooks\n",
        "    features_blobs = []\n",
        "    grads_blobs = []\n",
        "\n",
        "    def hook_feature(module, input, output):\n",
        "        features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "    def hook_grad(module, grad_in, grad_out):\n",
        "        grads_blobs.append(grad_out[0].data.cpu().numpy())\n",
        "\n",
        "    # 最後の畳み込み層を取得します (MobileNetV3の場合は、特定の層名を指定することができます。)\n",
        "    final_conv = model.blocks[-1]  # この部分はモデルの構造により調整が必要\n",
        "    final_conv.register_forward_hook(hook_feature)\n",
        "    final_conv.register_backward_hook(hook_grad)\n",
        "\n",
        "    # 順伝播と逆伝播を実行\n",
        "    output = model(image_tensor)\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax().item()\n",
        "    model.zero_grad()\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot.to(device))\n",
        "\n",
        "    # Grad-CAMの計算\n",
        "    activations = features_blobs[0]\n",
        "    grads = grads_blobs[0]\n",
        "    weights = np.mean(grads, axis=(2, 3))[0, :]\n",
        "    cam = np.zeros_like(activations[0, 0, :, :], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * activations[0, i, :, :]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (numpy_image.shape[1], numpy_image.shape[0]))  # 元の画像のサイズに合わせて調整\n",
        "    cam = cam - np.min(cam)\n",
        "    cam = cam / np.max(cam)\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "jM8OzXv1ICqp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_list))"
      ],
      "metadata": {
        "id": "wY8lJI5QLzMv",
        "outputId": "24013912-f6f2-4a25-c490-9cafaf626405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "\n",
        "######################\n",
        "# Inference MobileNetV3 #\n",
        "######################\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "\n",
        "for i, (img_path, img_label) in enumerate(zip(val_list, val_list_label)):\n",
        "    pilr_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Convert and display the PIL Image\n",
        "    numpy_image = np.array(pilr_image)\n",
        "    cv2_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Grad-CAMの計算\n",
        "    cam = compute_gradcam(model_ft, image_tensor)\n",
        "\n",
        "    # ヒートマップを画像として可視化\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    result = np.hstack([cv2_image, heatmap])\n",
        "    cv2_imshow(result)\n",
        "\n",
        "    image_tensor = test_data_transforms(pilr_image)\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model_ft(image_tensor)\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "    prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "    prob = \"{:.3f}\".format(prob.item())\n",
        "    print(f\"target: {img_label}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "    probs.append(float(prob)) #予測確率\n",
        "    preds.append(int(pred))  #予測結果\n",
        "    targets.append(int(img_label)) #ラベル\n",
        "\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)\n"
      ],
      "metadata": {
        "id": "qKyHq5dTCYFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAtkdZLnCYHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# Interference MobileNetV3 #\n",
        "######################\n",
        "\n",
        "#define dataset and dataloader\n",
        "test_dataset = SimpleImageDataset(val_list, val_list_label, test_data_transforms)\n",
        "test_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False, pin_memory=True, num_workers=0) #val_datasetを1枚ずつにしてtest_loadeerに格納\n",
        "\n",
        "model_ft.to(device)\n",
        "model_ft.eval() # prep model for evaluation\n",
        "targets, probs, preds =[], [], []\n",
        "for image_tensor, target in test_loader:\n",
        "      #target = target.squeeze(1)\n",
        "      image_tensor = image_tensor.to(device)\n",
        "      target = target.to(device)\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model_ft(image_tensor)\n",
        "      _, pred = torch.max(output, 1)\n",
        "\n",
        "      prob = nn.Softmax(dim=1)(output) #calculate probalility\n",
        "      prob = prob[0][1].cpu().detach() #probalility of being positive\n",
        "      prob = \"{:.3f}\".format(prob.item())\n",
        "      print(f\"target: {target.item()}, prob: {prob}, pred: {pred.item()}\")\n",
        "\n",
        "\n",
        "      probs.append(float(prob)) #予測確率\n",
        "      preds.append(int(pred))  #予測結果\n",
        "      targets.append(int(target)) #ラベル\n",
        "y_label = np.array(targets)\n",
        "y_pred = np.array(preds)\n",
        "y_prob = np.array(probs)"
      ],
      "metadata": {
        "id": "QvHusSc_-3iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label\n",
        "y_pred\n",
        "y_prob"
      ],
      "metadata": {
        "id": "E0BPaOqaEnM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "itGmZ613_5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_n9QtxfuBE86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNEtLq1XBE-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjjCwloOBFBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# #################################################\n",
        "# threshold = 0.5 #判定基準。ここは先に入力しておく\n",
        "# #################################################\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "specificity = []\n",
        "f1score = []\n",
        "area_u_ROC = []\n",
        "\n",
        "#TP_list, FN_list, FP_list, FN_list = [], [], [], []\n",
        "#confusion_list = [[] for i in range(4)]  #[[TP],[FN],[FP],[FN]]\n",
        "confusion_arr = np.zeros((2,2))\n",
        "\n",
        "\n",
        "# X = y_prob\n",
        "# Y = y_label\n",
        "\n",
        "# Y_pred_proba = X\n",
        "# Y_pred = np.where(Y_pred_proba >= threshold, 1, 0)\n",
        "\n",
        "acc = accuracy_score(y_label, y_pred)\n",
        "print('Accuracy:',acc)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_label, y_pred).ravel()\n",
        "print(tp, fn, fp, tn)\n",
        "\n",
        "def specificity_score(label, pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(label, pred).flatten()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "print('confusion matrix = \\n', confusion_matrix(y_label, y_pred))\n",
        "print(f'Accuracy : {accuracy_score(y_label, y_pred)}')\n",
        "print(f'Precision (true positive rate) : {precision_score(y_label, y_pred)}')\n",
        "print(f'Recall (sensitivity): {recall_score(y_label, y_pred)}')\n",
        "print(f'Specificity : {specificity_score(y_label, y_pred)}')\n",
        "print(f'F1 score : {f1_score(y_label, y_pred)}')\n",
        "\n",
        "#ROC curve\n",
        "\n",
        "# Compute the ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_label, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc_score(y_label, y_prob):.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
        "# plt.plot(fpr, tpr, marker='o')\n",
        "# plt.xlabel('FPR')\n",
        "# plt.ylabel('TPR')\n",
        "# plt.grid()\n",
        "# print(f'Area_under_ROC : {roc_auc_score(y_label, y_pred)}')\n",
        "#plt.savefig('plots/roc_curve.png')\n",
        "\n",
        "accuracy.append(accuracy_score(y_label, y_pred))\n",
        "precision.append(precision_score(y_label, y_pred))\n",
        "recall.append(recall_score(y_label, y_pred))\n",
        "specificity.append(specificity_score(y_label, y_pred))\n",
        "f1score.append(f1_score(y_label, y_pred))\n",
        "area_u_ROC.append(roc_auc_score(y_label, y_pred))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# confusion matrixをheatmapで表示\n",
        "cm = confusion_matrix(y_label, y_pred, labels=[1, 0])\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n",
        "            xticklabels=['TED', 'control'],\n",
        "            yticklabels=['TED', 'control'])\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1CAhawNTE8k8",
        "outputId": "39392fc3-aaa3-435b-8874-7a05f70f9eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9262048192771084\n",
            "302 30 19 313\n",
            "confusion matrix = \n",
            " [[313  19]\n",
            " [ 30 302]]\n",
            "Accuracy : 0.9262048192771084\n",
            "Precision (true positive rate) : 0.940809968847352\n",
            "Recall (sensitivity): 0.9096385542168675\n",
            "Specificity : 0.9427710843373494\n",
            "F1 score : 0.9249617151607962\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAIjCAYAAAAk+FJEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6iklEQVR4nO3de3zP9f//8ft7Y+/NzsbYxIZp5tzp44OcSqTkVElUI5RPik+OqcioJjmnUjkMH6KDY5SEkmMHh9Asc05bGMMcZrbX7w8/729r1N6M1zO7XS8Xl0t7vV/v1+vx3qWPz63X+/l+vR2WZVkCAAAADORh9wAAAADA5RCrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwBwCTt37lTTpk0VGBgoh8Oh+fPnF+jx9+7dK4fDoYSEhAI97j9Zo0aN1KhRI7vHAGAYYhWAsXbt2qWnn35aFSpUkLe3twICAlSvXj2NGzdOZ86cuabnjo2N1datW/Xaa69pxowZuv3226/p+a6nTp06yeFwKCAg4JK/x507d8rhcMjhcGjkyJFuH/+3337TkCFDtHnz5gKYFkBhV8TuAQDgUhYvXqyHH35YTqdTTzzxhKpVq6Zz585p9erV6tevn7Zv367333//mpz7zJkzWrdunV566SU9++yz1+QcEREROnPmjIoWLXpNjv93ihQpotOnT2vRokVq165drsdmzpwpb29vnT179oqO/dtvvykuLk6RkZGqVatWvp/35ZdfXtH5ANzYiFUAxtmzZ4/at2+viIgIrVixQmFhYa7HevTooeTkZC1evPianf/w4cOSpKCgoGt2DofDIW9v72t2/L/jdDpVr149ffjhh3liddasWbr//vv16aefXpdZTp8+rWLFisnLy+u6nA/APwvLAAAYZ8SIEcrIyNDkyZNzhepFUVFR6tWrl+vn8+fPa9iwYapYsaKcTqciIyP14osvKjMzM9fzIiMj1aJFC61evVr/+te/5O3trQoVKmj69OmufYYMGaKIiAhJUr9+/eRwOBQZGSnpwtvnF//5j4YMGSKHw5Fr27Jly3TnnXcqKChIfn5+io6O1osvvuh6/HJrVlesWKH69evL19dXQUFBatWqlRITEy95vuTkZHXq1ElBQUEKDAxU586ddfr06cv/Yv+kQ4cO+vzzz5Wenu7a9v3332vnzp3q0KFDnv2PHj2qvn37qnr16vLz81NAQICaN2+uLVu2uPb5+uuvdccdd0iSOnfu7FpOcPF1NmrUSNWqVdOPP/6oBg0aqFixYq7fy5/XrMbGxsrb2zvP62/WrJmCg4P122+/5fu1AvjnIlYBGGfRokWqUKGC6tatm6/9u3btqsGDB+vWW2/VmDFj1LBhQ8XHx6t9+/Z59k1OTtZDDz2ke+65R6NGjVJwcLA6deqk7du3S5Latm2rMWPGSJIeffRRzZgxQ2PHjnVr/u3bt6tFixbKzMzU0KFDNWrUKLVs2VJr1qz5y+d99dVXatasmQ4dOqQhQ4aod+/eWrt2rerVq6e9e/fm2b9du3Y6efKk4uPj1a5dOyUkJCguLi7fc7Zt21YOh0Nz5851bZs1a5YqV66sW2+9Nc/+u3fv1vz589WiRQuNHj1a/fr109atW9WwYUNXOMbExGjo0KGSpKeeekozZszQjBkz1KBBA9dx0tLS1Lx5c9WqVUtjx45V48aNLznfuHHjVLJkScXGxio7O1uS9N577+nLL7/UW2+9pfDw8Hy/VgD/YBYAGOT48eOWJKtVq1b52n/z5s2WJKtr1665tvft29eSZK1YscK1LSIiwpJkrVq1yrXt0KFDltPptPr06ePatmfPHkuS9eabb+Y6ZmxsrBUREZFnhldeecX641+nY8aMsSRZhw8fvuzcF88xdepU17ZatWpZoaGhVlpammvbli1bLA8PD+uJJ57Ic74nn3wy1zHbtGljhYSEXPacf3wdvr6+lmVZ1kMPPWTdfffdlmVZVnZ2tlW6dGkrLi7ukr+Ds2fPWtnZ2Xleh9PptIYOHera9v333+d5bRc1bNjQkmRNnDjxko81bNgw17alS5dakqxXX33V2r17t+Xn52e1bt36b18jgBsHV1YBGOXEiROSJH9//3ztv2TJEklS7969c23v06ePJOVZ21qlShXVr1/f9XPJkiUVHR2t3bt3X/HMf3ZxreuCBQuUk5OTr+ekpKRo8+bN6tSpk4oXL+7aXqNGDd1zzz2u1/lH3bt3z/Vz/fr1lZaW5vod5keHDh309ddfKzU1VStWrFBqauollwBIF9a5enhc+L+N7OxspaWluZY4bNy4Md/ndDqd6ty5c772bdq0qZ5++mkNHTpUbdu2lbe3t9577718nwvAPx+xCsAoAQEBkqSTJ0/ma/99+/bJw8NDUVFRubaXLl1aQUFB2rdvX67t5cqVy3OM4OBgHTt27AonzuuRRx5RvXr11LVrV5UqVUrt27fXRx999JfhenHO6OjoPI/FxMToyJEjOnXqVK7tf34twcHBkuTWa7nvvvvk7++vOXPmaObMmbrjjjvy/C4vysnJ0ZgxY1SpUiU5nU6VKFFCJUuW1E8//aTjx4/n+5xlypRx68NUI0eOVPHixbV582aNHz9eoaGh+X4ugH8+YhWAUQICAhQeHq5t27a59bw/f8Dpcjw9PS+53bKsKz7HxfWUF/n4+GjVqlX66quv9Pjjj+unn37SI488onvuuSfPvlfjal7LRU6nU23bttW0adM0b968y15VlaTXX39dvXv3VoMGDfS///1PS5cu1bJly1S1atV8X0GWLvx+3LFp0yYdOnRIkrR161a3ngvgn49YBWCcFi1aaNeuXVq3bt3f7hsREaGcnBzt3Lkz1/bff/9d6enprk/2F4Tg4OBcn5y/6M9XbyXJw8NDd999t0aPHq2ff/5Zr732mlasWKGVK1de8tgX50xKSsrz2I4dO1SiRAn5+vpe3Qu4jA4dOmjTpk06efLkJT+UdtEnn3yixo0ba/LkyWrfvr2aNm2qJk2a5Pmd5Pc/HPLj1KlT6ty5s6pUqaKnnnpKI0aM0Pfff19gxwdgPmIVgHH69+8vX19fde3aVb///nuex3ft2qVx48ZJuvA2tqQ8n9gfPXq0JOn+++8vsLkqVqyo48eP66effnJtS0lJ0bx583Ltd/To0TzPvXhz/D/fTuuisLAw1apVS9OmTcsVf9u2bdOXX37pep3XQuPGjTVs2DBNmDBBpUuXvux+np6eea7afvzxxzp48GCubRej+lJh764BAwZo//79mjZtmkaPHq3IyEjFxsZe9vcI4MbDlwIAME7FihU1a9YsPfLII4qJicn1DVZr167Vxx9/rE6dOkmSatasqdjYWL3//vtKT09Xw4YN9d1332natGlq3br1ZW+LdCXat2+vAQMGqE2bNurZs6dOnz6td999VzfffHOuDxgNHTpUq1at0v3336+IiAgdOnRI77zzjm666Sbdeeedlz3+m2++qebNm6tOnTrq0qWLzpw5o7feekuBgYEaMmRIgb2OP/Pw8NDLL7/8t/u1aNFCQ4cOVefOnVW3bl1t3bpVM2fOVIUKFXLtV7FiRQUFBWnixIny9/eXr6+vateurfLly7s114oVK/TOO+/olVdecd1Ka+rUqWrUqJEGDRqkESNGuHU8AP9MXFkFYKSWLVvqp59+0kMPPaQFCxaoR48eeuGFF7R3716NGjVK48ePd+07adIkxcXF6fvvv9d///tfrVixQgMHDtTs2bMLdKaQkBDNmzdPxYoVU//+/TVt2jTFx8frgQceyDN7uXLlNGXKFPXo0UNvv/22GjRooBUrVigwMPCyx2/SpIm++OILhYSEaPDgwRo5cqT+/e9/a82aNW6H3rXw4osvqk+fPlq6dKl69eqljRs3avHixSpbtmyu/YoWLapp06bJ09NT3bt316OPPqpvvvnGrXOdPHlSTz75pG655Ra99NJLru3169dXr169NGrUKK1fv75AXhcAszksd1biAwAAANcRV1YBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgrBvyG6x86r5o9wgAUKBSl79q9wgAUKACffJ3zZQrqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCgAAAGMZEauWZenIkSNKS0uzexQAAAAYxNZYTU1N1RNPPKHg4GCVKlVKoaGhCg4O1pNPPqnff//dztEAAABggCJ2nfjEiROqW7euMjIy1LlzZ1WuXFmWZennn3/Whx9+qNWrV2vjxo3y8/Oza0QAAADYzLZYHTdunDw9PbV9+3aVLFky12Mvv/yy6tWrp/Hjx+vFF1+0aUIAAADYzbZlAIsXL9aLL76YJ1QlKTQ0VAMHDtSiRYtsmAwAAACmsC1Wf/nlF9WtW/eyj9etW1dJSUnXcSIAAACYxrZYPXHihIKCgi77eFBQkE6cOHH9BgIAAIBxbFuzalmWPDwu38oOh0OWZV3HiQCpW5va6tbmX4oIC5YkJe45pNenrNCX63+RJDm9imj4c/fp4SY15Czqqa827FSvkQt16FiGJKl6VGn1fbyh6taIUEiQr/alHNOk+d/p7Y/W2vaaAODPPvnoQ839eLZSfjsoSSpfMUpdn3pGde9sIEnKzMzUuFFv6MulS5R1Lkv/rltP/V8crJCQEnaOjULKYdlUhB4eHgoMDJTD4bjk45Zl6cSJE8rOznb72D51+VAWrsx99SorOydHyQfS5HBIj913q57vUF//7jRBiXsOaVzfVmpeN1rdXvtEJzLOakyflsrJsXRX9/ckSU/cf5tqVArT/K+369dD6fp39Qi9PaC1Xnr7C038dL3Nrw7/ZKnLX7V7BNxAvv1mpTw8PFS2XIQsWVq8cIH+N22KZsz+VBWjKmn4a0O05ttVGjz0dfn5+evN4cPk4fDQpGmz7B4dN5BAn/y9wW9brE6bNi1f+8XGxrp9bGIVBengFy/rxQmfa97KbTqw5CV1GvKR5q3cJkm6OaKktnz4vBp2e1ffbT9wyeeP6dNSlSNLqvlzk6/n2LjBEKu41po0+Leee76v7m7STE0b19Ow+Dd19z3NJEl79+xWuzb3a/L0D1W9Ri17B8UNI7+xatsygCuJUOB68vBw6MG7qsvX20sbth3QLZXLyKtoEa34Ptm1zy/7Dmt/6jHVrlbusrEa6OfUsRNnrtfYAOCW7OxsLV/2hc6cOa3qNWopMXG7zp/P0r9q13HtE1m+gkqHhWnrls3EKq4722L1u+++02233SZPT89LPp6ZmakFCxaoXbt2f3mczMxMZWZm5tpm5ZyXw8O2l4Z/uKoVSunr97vL26uIMs6c0yMD/6cdew+pZqUwZZ47r+MZZ3Ptf+hohkqFXPrLK/5drZweuruG2vTN3zsJAHC9JO/8RV2eeFTnzmXKx6eYRox+SxUqRumXpB0qWrSo/AMCcu1fvHgJpaUdsWlaFGa23Q2gTp06SktLc/0cEBCg3bt3u35OT0/Xo48++rfHiY+PV2BgYK4/5w+uuyYzo3D4Zf8R1Y59Sw26vasP5m3QBy8/rMqRoW4fp0qFUvrojcf12pQVWv5d8t8/AQCuo4jISP1vzlxNmTFHD7Zrr7jBA7V7F39XwTy2xeqfl8peaulsfpbTDhw4UMePH8/1p0iZOn/7POByss5na/fBo9qU9JsGT/xSW5NT1KNdXaUePSmnVxEF+nnn2j+0uJ9+T8vIta1yZKiWjO+iKQu/0xsJK6/n+ACQL0WLeqlsuQjFVKmqHj17q9LN0Zoza4ZCSpRQVlaWTv7p9pFHjx7hbgCwhW2xmh+Xu1PAHzmdTgUEBOT6wxIAFCQPD4ecRT21acdBncs6r8a3V3Q9VqlcCZUrHawN2/a7tsWUD9UXE7pq5pKNGvLeMjtGBgC35eRYOnfunGJiqqpIkaL6/rv/u4PJvr17lJqSouo1a9k3IAotqg74g6Hdm2rp+l90IDVd/sWceqRpTTW4pbweeD5BJ05lKmHRj3qj5306euKMTp46q9G9H9D6rftcH66qUqGUPn+ri77asFPjZ69WqeIX1rJm51g6kn7KzpcGAC5vjx+tOvXqq3TpcJ0+fUpLP/9MG3/4TuPf+UB+/v5q2aatxo4aroDAQPn6+mnk8FdVvUYtPlwFW9gaqz///LNSU1MlXXjLf8eOHcrIuPB26pEjLOLG9Vcy2E+TBz2s0iH+On7qrLYlp+qB5xNcdwDoP36xcixLH77eQc6iRf7/lwIscD2/TeNqCg32U4d7b1GHe29xbd+XckyVH3zzur8eALiUo0fTFPfyCzpy5LD8/PwVdfPNGv/OB6pdp54k6fm+A+Xh8NALfXrp3Llzri8FAOxg65cCXO5bqi5udzgcfCkAAIj7rAK48Rh/n9U9e/bYdWoAAAD8Q9gWq9OmTVPfvn1VrFgxu0YAAACA4Wy7G0BcXJxrfSoAAABwKcbcZxUAAAD4M1vvs5qf+6gCAACg8LL11lU333zz3wbr0aNHr9M0AAAAMI2tsRoXF6fAwEA7RwAAAIDBbI3V9u3bKzQ01M4RAAAAYDDb1qyyXhUAAAB/h7sBAAAAwFi2LQPIycmx69QAAAD4h7D11lUAAADAXyFWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYqkFhNT08viMMAAAAAubgdq2+88YbmzJnj+rldu3YKCQlRmTJltGXLlgIdDgAAAIWb27E6ceJElS1bVpK0bNkyLVu2TJ9//rmaN2+ufv36FfiAAAAAKLyKuPuE1NRUV6x+9tlnateunZo2barIyEjVrl27wAcEAABA4eX2ldXg4GAdOHBAkvTFF1+oSZMmkiTLspSdnV2w0wEAAKBQc/vKatu2bdWhQwdVqlRJaWlpat68uSRp06ZNioqKKvABAQAAUHi5HatjxoxRZGSkDhw4oBEjRsjPz0+SlJKSomeeeabABwQAAEDh5bAsy7J7iILmU/dFu0cAgAKVuvxVu0cAgAIV6JO/1aj5urK6cOHCfJ+4ZcuW+d4XAAAA+Cv5itXWrVvn62AOh4MPWQEAAKDA5CtWc3JyrvUcAAAAQB5X9XWrZ8+eLag5AAAAgDzcjtXs7GwNGzZMZcqUkZ+fn3bv3i1JGjRokCZPnlzgAwIAAKDwcjtWX3vtNSUkJGjEiBHy8vJyba9WrZomTZpUoMMBAACgcHM7VqdPn673339fHTt2lKenp2t7zZo1tWPHjgIdDgAAAIWb27F68ODBS35TVU5OjrKysgpkKAAAAEC6glitUqWKvv322zzbP/nkE91yyy0FMhQAAAAgXcHXrQ4ePFixsbE6ePCgcnJyNHfuXCUlJWn69On67LPPrsWMAAAAKKTcvrLaqlUrLVq0SF999ZV8fX01ePBgJSYmatGiRbrnnnuuxYwAAAAopNy+sipJ9evX17Jlywp6FgAAACCXK4pVSfrhhx+UmJgo6cI61ttuu63AhgIAAACkK4jVX3/9VY8++qjWrFmjoKAgSVJ6errq1q2r2bNn66abbiroGQEAAFBIub1mtWvXrsrKylJiYqKOHj2qo0ePKjExUTk5Oerateu1mBEAAACFlNtXVr/55hutXbtW0dHRrm3R0dF66623VL9+/QIdDgAAAIWb21dWy5Yte8mb/2dnZys8PLxAhgIAAACkK4jVN998U88995x++OEH17YffvhBvXr10siRIwt0OAAAABRuDsuyrL/bKTg4WA6Hw/XzqVOndP78eRUpcmEVwcV/9vX11dGjR6/dtPnkU/dFu0cAgAKVuvxVu0cAgAIV6JO/a6b5WrM6duzYq5kFAAAAuCL5itXY2NhrPQcAAACQxxV/KYAknT17VufOncu1LSAg4KoGAgAAAC5y+wNWp06d0rPPPqvQ0FD5+voqODg41x8AAACgoLgdq/3799eKFSv07rvvyul0atKkSYqLi1N4eLimT59+LWYEAABAIeX2MoBFixZp+vTpatSokTp37qz69esrKipKERERmjlzpjp27Hgt5gQAAEAh5PaV1aNHj6pChQqSLqxPvXirqjvvvFOrVq0q2OkAAABQqLkdqxUqVNCePXskSZUrV9ZHH30k6cIV16CgoAIdDgAAAIWb27HauXNnbdmyRZL0wgsv6O2335a3t7eef/559evXr8AHBAAAQOGVr2+w+iv79u3Tjz/+qKioKNWoUaOg5roqZ8/bPQEAFKzgO561ewQAKFBnNk3I135XdZ9VSYqIiFBERMTVHgYAAADII1+xOn78+HwfsGfPnlc8DAAAAPBH+VoGUL58+fwdzOHQ7t27r3qoq8UyAAA3GpYBALjRFOgygIuf/gcAAACuJ7fvBgAAAABcL8QqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYVxSr3377rR577DHVqVNHBw8elCTNmDFDq1evLtDhAAAAULi5HauffvqpmjVrJh8fH23atEmZmZmSpOPHj+v1118v8AEBAABQeLkdq6+++qomTpyoDz74QEWLFnVtr1evnjZu3FigwwEAAKBwcztWk5KS1KBBgzzbAwMDlZ6eXhAzAQAAAJKuIFZLly6t5OTkPNtXr16tChUqFMhQAAAAgHQFsdqtWzf16tVLGzZskMPh0G+//aaZM2eqb9+++s9//nMtZgQAAEAhVcTdJ7zwwgvKycnR3XffrdOnT6tBgwZyOp3q27evnnvuuWsxIwAAAAoph2VZ1pU88dy5c0pOTlZGRoaqVKkiPz+/gp7tip09b/cEAFCwgu941u4RAKBAndk0IV/7uX1l9SIvLy9VqVLlSp8OAAAA/C23Y7Vx48ZyOByXfXzFihVXNRAAAABwkduxWqtWrVw/Z2VlafPmzdq2bZtiY2MLai4AAADA/VgdM2bMJbcPGTJEGRkZVz0QAAAAcJHbt666nMcee0xTpkwpqMMBAAAABRer69atk7e3d0EdDgAAAHB/GUDbtm1z/WxZllJSUvTDDz9o0KBBBTYYAAAA4HasBgYG5vrZw8ND0dHRGjp0qJo2bVpggwEAAABuxWp2drY6d+6s6tWrKzg4+FrNBAAAAEhyc82qp6enmjZtqvT09Gs0DgAAAPB/3P6AVbVq1bR79+5rMQsAAACQi9ux+uqrr6pv37767LPPlJKSohMnTuT6AwAAABQUh2VZVn52HDp0qPr06SN/f///e/IfvnbVsiw5HA5lZ2cX/JRuOnve7gkAoGAF3/Gs3SMAQIE6s2lCvvbLd6x6enoqJSVFiYmJf7lfw4YN83Xia4lYBXCjIVYB3GjyG6v5vhvAxaY1IUYBAABQOLi1ZvWPb/sDAAAA15pb91m9+eab/zZYjx49elUDAQAAABe5FatxcXF5vsEKAAAAuFbcitX27dsrNDT0Ws0CAAAA5JLvNausVwUAAMD1lu9YzecdrgAAAIACk+9lADk5OddyDgAAACAPt79uFQAAALheiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxithx0uDgYDkcjnzte/To0Ws8DQAAAExlS6yOHTvWjtMCAADgH8aWWI2NjbXjtAAAAPiHsSVW/yw7O1vz589XYmKiJKlq1apq2bKlPD09bZ4MAAAAdrI9VpOTk3Xffffp4MGDio6OliTFx8erbNmyWrx4sSpWrGjzhAAAALCL7XcD6NmzpypWrKgDBw5o48aN2rhxo/bv36/y5curZ8+edo8HAAAAG9l+ZfWbb77R+vXrVbx4cde2kJAQDR8+XPXq1bNxMgAAANjN9iurTqdTJ0+ezLM9IyNDXl5eNkwEAAAAU9geqy1atNBTTz2lDRs2yLIsWZal9evXq3v37mrZsqXd4wEAAMBGtsfq+PHjVbFiRdWpU0fe3t7y9vZWvXr1FBUVpXHjxtk9HgAAAGxk65pVy7J04sQJzZ49WwcPHnTduiomJkZRUVF2jgYAAAAD2B6rUVFR2r59uypVqkSgAgAAIBdblwF4eHioUqVKSktLs3MMAAAAGMr2NavDhw9Xv379tG3bNrtHAQAAgGEclmVZdg4QHBys06dP6/z58/Ly8pKPj0+ux48ePer2Mc+eL6jpAMAMwXc8a/cIAFCgzmyakK/9bP9SgDFjxsjhcNg9BnBZP/7wvRKmTFbiz9t0+PBhjRn/tu66u4nr8bQjRzR29EitW7taJ0+e1K233a4XXhqkiIhI+4YGgP+v28N3qttD9RURfuHLdxJ3p+r19z/Xl2t+liQ92baeHml+u2pVvkkBfj4qXb+fjmecyXWMj8c+rZo3l1HJ4v46duK0Vm5I0svjFyjl8PHr/npQ+Ngeq506dbJ7BOAvnTlzWtHR0Wrd9kH17pX76pZlWfpvzx4qUqSIxr71jvz8/DR9WoKe7tJZcxcuVrFixWyaGgAuOPh7uga9tUDJ+w/LIYcee6C2Ph7zlP7dfrgSd6eqmHdRLVv7s5at/VnDera65DFWff+L3py8VKlHjis8NEjxz7fRrDe7qHGn0df51aAwsj1WPT09lZKSotDQ0Fzb09LSFBoaquzsbJsmAy64s35D3Vm/4SUf27dvr37aslmfLvhMUVGVJEkvDx6iuxrW0xdLFqvtQw9fz1EBII8lq3J/JmTI24vU7eE79a8a5ZW4O1UTZn0tSap/W6XLHuOtmStd/7w/5ZhGTl2mj0Z3U5EiHjp/PueazA1cZPsHrC63ZDYzM5OvW4Xxss6dkyQ5vZyubR4eHvLy8tKmjT/aNRYAXJKHh0MPN7tNvj5e2vDTnis6RnBAMbVvfrvWb9lDqOK6sO3K6vjx4yVJDodDkyZNkp+fn+ux7OxsrVq1SpUrV/7b42RmZiozMzPXNsvTKafTeZlnAAUnsnwFhYWFa/zYURr0ylD5+PhoxvQE/Z6aqsOHD9s9HgBIkqpGhevraX3k7VVEGWcy9UifD7Rjd6pbx3i1Zyt1b99Avj5Obfhpj9r2nHiNpgVysy1Wx4wZI+nCldWJEyfK09PT9ZiXl5ciIyM1ceLf/w8hPj5ecXFxuba9NOgVvTx4SIHOC1xK0aJFNXrcWxoy6CXVr/sveXp6qva/6+jO+g0u+64BAFxvv+z9XbXbxyvQz0dtmtyiD4Y+rqZdx7kVrGOmf6WE+etULqy4Xnq6uSYNe5xgxXVhW6zu2XPh7YfGjRtr7ty5Cg4OvqLjDBw4UL179861zfLkqiqunypVq+mjuQt08uRJZWVlqXjx4urY/mFVrVrN7tEAQJKUdT5buw8ckSRtSjyg26qWU49HG+m512bn+xhp6aeUln5KyfsPKWlPqpKXvqraNcpf8XICIL9s/4DVypUr/36nv+B05n3Ln/uswg7+/v6SLnzo6uft29TjuV42TwQAl+bhcMjpdeUJ4OFx4ZaTXkVtzwgUArb/W5adna2EhAQtX75chw4dUk5O7sXaK1assGky4ILTp05p//79rp8P/vqrdiQmKjAwUGHh4fpy6ecKDi6usLBw7dyZpBHxr6vxXU1Ut96dNk4NABcMfa6llq7ZrgMpx+Tv661Hmt+uBrdX0gPPvCNJKhXir1IhAapYroQkqVqlcJ08dVYHUo/p2InTuqNahG6rGqG1m3Yp/eRplb+ppF555n7t2n+Yq6q4LmyP1V69eikhIUH333+/qlWrxhcEwDjbt29T185PuH4eOSJektSyVRsNe324Dh8+rJEjhivtSJpKliypFi1b6enuz9g1LgDkUrK4nyYPe0KlSwToeMZZbdt5UA88845WbNghSer6UH293P0+1/5fTXlektRt8Az9b9EGnT6bpVZ31dTL3e+Xr4+XUo8c15drE/XGB1N0Lou3MnHt2f51qyVKlND06dN13333/f3O+cQyAAA3Gr5uFcCNJr9ft2r7fVa9vLwUFRVl9xgAAAAwkO2x2qdPH40bN47b/AAAACAP29esrl69WitXrtTnn3+uqlWrqmjRorkenzt3rk2TAQAAwG62x2pQUJDatGlj9xgAAAAwkO2xOnXqVLtHAAAAgKFsj9WLDh8+rKSkJElSdHS0SpYsafNEAAAAsJvtH7A6deqUnnzySYWFhalBgwZq0KCBwsPD1aVLF50+fdru8QAAAGAj22O1d+/e+uabb7Ro0SKlp6crPT1dCxYs0DfffKM+ffrYPR4AAABsZMSXAnzyySdq1KhRru0rV65Uu3btdPjwYbePyZcCALjR8KUAAG40/5gvBTh9+rRKlSqVZ3toaCjLAAAAAAo522O1Tp06euWVV3T27FnXtjNnziguLk516tSxcTIAAADYzfa7AYwdO1b33nuvbrrpJtWsWVOStGXLFjmdTn355Zc2TwcAAAA72b5mVbqwFGDmzJnasWOHJCkmJkYdO3aUj4/PFR2PNasAbjSsWQVwo8nvmlXbr6zGx8erVKlS6tatW67tU6ZM0eHDhzVgwACbJgMAAIDdbF+z+t5776ly5cp5tletWlUTJ060YSIAAACYwvZYTU1NVVhYWJ7tJUuWVEpKig0TAQAAwBS2x2rZsmW1Zs2aPNvXrFmj8PBwGyYCAACAKWxfs9qtWzf997//VVZWlu666y5J0vLly9W/f3++wQoAAKCQsz1W+/Xrp7S0ND3zzDM6d+6cJMnb21sDBgzQwIEDbZ4OAAAAdjLi1lWSlJGRocTERPn4+KhSpUpyOp1XfCxuXQXgRsOtqwDcaP4xt666yM/PT3fccYfdYwAAAMAgtn/ACgAAALgcYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCyHZVmW3UMA/0SZmZmKj4/XwIED5XQ67R4HAK4af6/BRMQqcIVOnDihwMBAHT9+XAEBAXaPAwBXjb/XYCKWAQAAAMBYxCoAAACMRawCAADAWMQqcIWcTqdeeeUVPoQA4IbB32swER+wAgAAgLG4sgoAAABjEasAAAAwFrEKAAAAYxGrAADgmkpISFBQUJDdY+AfilgFJDkcjr/8M2TIEO3du/eyj69fv17Shb+QL27z9PRUcHCwateuraFDh+r48eM2v0oAyL/IyEiNHTvW7jEAFbF7AMAEKSkprn+eM2eOBg8erKSkJNc2Pz8/HTlyRJL01VdfqWrVqrmeHxIS4vrngIAAJSUlybIspaena+3atYqPj9fUqVO1Zs0ahYeHX+NXAwDXR3Z2thwOhzw8uPaFa4d/uwBJpUuXdv0JDAyUw+HItc3Pz8+1b0hISK7HSpcuraJFi7oev/jcsLAwxcTEqEuXLlq7dq0yMjLUv39/O14egBtQTk6ORowYoaioKDmdTpUrV06vvfaaJGnr1q2666675OPjo5CQED311FPKyMhwPbdTp05q3bq1Ro4cqbCwMIWEhKhHjx7KysqSJDVq1Ej79u3T888/73q3SPq/t/MXLlyoKlWqyOl0av/+/Tp27JieeOIJBQcHq1ixYmrevLl27tx5/X8puCERq8B1EBoaqo4dO2rhwoXKzs62exwAN4CBAwdq+PDhGjRokH7++WfNmjVLpUqV0qlTp9SsWTMFBwfr+++/18cff6yvvvpKzz77bK7nr1y5Urt27dLKlSs1bdo0JSQkKCEhQZI0d+5c3XTTTRo6dKhSUlJyvft0+vRpvfHGG5o0aZK2b9+u0NBQderUST/88IMWLlyodevWybIs3Xfffa74Ba4GywAAN9WtWzfPW15/vGJxOZUrV9bJkyeVlpam0NDQazUegELg5MmTGjdunCZMmKDY2FhJUsWKFXXnnXfqgw8+0NmzZzV9+nT5+vpKkiZMmKAHHnhAb7zxhkqVKiVJCg4O1oQJE+Tp6anKlSvr/vvv1/Lly9WtWzcVL15cnp6e8vf3V+nSpXOdOysrS++8845q1qwpSdq5c6cWLlyoNWvWqG7dupKkmTNnqmzZspo/f74efvjh6/VrwQ2KWAXcNGfOHMXExLj9vItfFnfx7TQAuFKJiYnKzMzU3XfffcnHatas6QpVSapXr55ycnKUlJTkitWqVavK09PTtU9YWJi2bt36t+f28vJSjRo1cp2vSJEiql27tmtbSEiIoqOjlZiYeEWvD/gjYhVwU9myZRUVFeX28xITExUQEJDrw1gAcCV8fHyu+hh/XGsvXfgP6ZycnHydm//oxvXEmlXgOjh06JBmzZql1q1b86lZAFetUqVK8vHx0fLly/M8FhMToy1btujUqVOubWvWrJGHh4eio6PzfQ4vL698rbGPiYnR+fPntWHDBte2tLQ0JSUlqUqVKvk+H3A5/L8m4Ka0tDSlpqbm+nP27FnX45ZlKTU1VSkpKUpMTNSUKVNUt25dBQYGavjw4TZODuBG4e3trQEDBqh///6aPn26du3apfXr12vy5Mnq2LGjvL29FRsbq23btmnlypV67rnn9Pjjj7uWAORHZGSkVq1apYMHD7pu3XcplSpVUqtWrdStWzetXr1aW7Zs0WOPPaYyZcqoVatWBfFyUcixDABwU5MmTfJs+/DDD9W+fXtJ0okTJxQWFiaHw6GAgABFR0crNjZWvXr1UkBAwPUeF8ANatCgQSpSpIgGDx6s3377TWFhYerevbuKFSumpUuXqlevXrrjjjtUrFgxPfjggxo9erRbxx86dKiefvppVaxYUZmZma5195cydepU9erVSy1atNC5c+fUoEEDLVmyJM9SA+BKOKy/+rcPAAAAsBHLAAAAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYB4Ap16tRJrVu3dv3cqFEj/fe//73uc3z99ddyOBxKT0+/7D4Oh0Pz58/P9zGHDBmiWrVqXdVce/fulcPh0ObNm6/qOAAKN2IVwA2lU6dOcjgccjgc8vLyUlRUlIYOHarz589f83PPnTtXw4YNy9e++QlMAIBUxO4BAKCg3XvvvZo6daoyMzO1ZMkS9ejRQ0WLFtXAgQPz7Hvu3Dl5eXkVyHmLFy9eIMcBAPwfrqwCuOE4nU6VLl1aERER+s9//qMmTZpo4cKFkv7vrfvXXntN4eHhio6OliQdOHBA7dq1U1BQkIoXL65WrVpp7969rmNmZ2erd+/eCgoKUkhIiPr37y/LsnKd98/LADIzMzVgwACVLVtWTqdTUVFRmjx5svbu3avGjRtLkoKDg+VwONSpUydJUk5OjuLj41W+fHn5+PioZs2a+uSTT3KdZ8mSJbr55pvl4+Ojxo0b55ozvwYMGKCbb75ZxYoVU4UKFTRo0CBlZWXl2e+9995T2bJlVaxYMbVr107Hjx/P9fikSZMUExMjb29vVa5cWe+8885lz3ns2DF17NhRJUuWlI+PjypVqqSpU6e6PTuAwoUrqwBueD4+PkpLS3P9vHz5cgUEBGjZsmWSpKysLDVr1kx16tTRt99+qyJFiujVV1/Vvffeq59++kleXl4aNWqUEhISNGXKFMXExGjUqFGaN2+e7rrrrsue94knntC6des0fvx41axZU3v27NGRI0dUtmxZffrpp3rwwQeVlJSkgIAA+fj4SJLi4+P1v//9TxMnTlSlSpW0atUqPfbYYypZsqQaNmyoAwcOqG3bturRo4eeeuop/fDDD+rTp4/bvxN/f38lJCQoPDxcW7duVbdu3eTv76/+/fu79klOTtZHH32kRYsW6cSJE+rSpYueeeYZzZw5U5I0c+ZMDR48WBMmTNAtt9yiTZs2qVu3bvL19VVsbGyecw4aNEg///yzPv/8c5UoUULJyck6c+aM27MDKGQsALiBxMbGWq1atbIsy7JycnKsZcuWWU6n0+rbt6/r8VKlSlmZmZmu58yYMcOKjo62cnJyXNsyMzMtHx8fa+nSpZZlWVZYWJg1YsQI1+NZWVnWTTfd5DqXZVlWw4YNrV69elmWZVlJSUmWJGvZsmWXnHPlypWWJOvYsWOubWfPnrWKFStmrV27Nte+Xbp0sR599FHLsixr4MCBVpUqVXI9PmDAgDzH+jNJ1rx58y77+Jtvvmnddtttrp9feeUVy9PT0/r1119d2z7//HPLw8PDSklJsSzLsipWrGjNmjUr13GGDRtm1alTx7Isy9qzZ48lydq0aZNlWZb1wAMPWJ07d77sDABwKVxZBXDD+eyzz+Tn56esrCzl5OSoQ4cOGjJkiOvx6tWr51qnumXLFiUnJ8vf3z/Xcc6ePatdu3bp+PHjSklJUe3atV2PFSlSRLfffnuepQAXbd68WZ6enmrYsGG+505OTtbp06d1zz335Np+7tw53XLLLZKkxMTEXHNIUp06dfJ9jovmzJmj8ePHa9euXcrIyND58+cVEBCQa59y5cqpTJkyuc6Tk5OjpKQk+fv7a9euXerSpYu6devm2uf8+fMKDAy85Dn/85//6MEHH9TGjRvVtGlTtW7dWnXr1nV7dgCFC7EK4IbTuHFjvfvuu/Ly8lJ4eLiKFMn9V52vr2+unzMyMnTbbbe53t7+o5IlS17RDBff1ndHRkaGJGnx4sW5IlG6sA63oKxbt04dO3ZUXFycmjVrpsDAQM2ePVujRo1ye9YPPvggTzx7enpe8jnNmzfXvn37tGTJEi1btkx33323evTooZEjR175iwFwwyNWAdxwfH19FRUVle/9b731Vs2ZM0ehoaF5ri5eFBYWpg0bNqhBgwaSLlxB/PHHH3Xrrbdecv/q1asrJydH33zzjZo0aZLn8YtXdrOzs13bqlSpIqfTqf3791/2imxMTIzrw2IXrV+//u9f5B+sXbtWEREReumll1zb9u3bl2e//fv367ffflN4eLjrPB4eHoqOjlapUqUUHh6u3bt3q2PHjvk+d8mSJRUbG6vY2FjVr19f/fr1I1YB/CXuBgCg0OvYsaNKlCihVq1a6dtvv9WePXv09ddfq2fPnvr1118lSb169dLw4cM1f/587dixQ88888xf3iM1MjJSsbGxevLJJzV//nzXMT/66CNJUkREhBwOhz777DMdPnxYGRkZ8vf3V9++ffX8889r2rRp2rVrlzZu3Ki33npL06ZNkyR1795dO3fuVL9+/ZSUlKRZs2YpISHBrddbqVIl7d+/X7Nnz9auXbs0fvx4zZs3L89+3t7eio2N1ZYtW/Ttt9+qZ8+eateunUqXLi1JiouLU3x8vMaPH69ffvlFW7du1dSpUzV69OhLnnfw4MFasGCBkpOTtX37dn322WeKiYlxa3YAhQ+xCqDQK1asmFatWqVy5cqpbdu2iomJUZcuXXT27FnXldY+ffro8ccfV2xsrOrUqSN/f3+1adPmL4/77rvv6qGHHtIzzzyjypUrq1u3bjp16pQkqUyZMoqLi9MLL7ygUqVK6dlnn5UkDRs2TIMGDVJ8fLxiYmJ07733avHixSpfvrykC+tIP/30U82fP181a9bUxIkT9frrr7v1elu2bKnnn39ezz77rGrVqqW1a9dq0KBBefaLiopS27Ztdd9996lp06aqUaNGrltTde3aVZMmTdLUqVNVvXp1NWzYUAkJCa5Z/8zLy0sDBw5UjRo11KBBA3l6emr27NluzQ6g8HFYl/t0AAAAAGAzrqwCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBY/w9mJDi7BUxOLQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export MobileNetV3 model to CoreML**"
      ],
      "metadata": {
        "id": "2HaYNrrUvioo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Output as CoreML\n",
        "###########################\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "!pip install --quiet coremltools\n",
        "import coremltools as ct\n",
        "\n",
        "# Load a pre-trained version of MobileNetV3\n",
        "class TorchClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TorchClassificationModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            model_ft,\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "torch_model = TorchClassificationModel().eval()\n",
        "torch_model = torch_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "# Trace with random data\n",
        "example_input = torch.rand(1, 3, 224, 224) # after test, will get 'size mismatch' error message with size 256x256\n",
        "traced_model = torch.jit.trace(torch_model, example_input)\n",
        "\n",
        "\n",
        "# Download class labels (from a separate file)\n",
        "#import urllib\n",
        "#label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'\n",
        "#class_labels = urllib.request.urlopen(label_url).read().decode(\"utf-8\").splitlines()\n",
        "class_labels = [\"cont\", \"grav\"]\n",
        "\n",
        "\n",
        "\n",
        "# Convert to Core ML using the Unified Conversion API\n",
        "# mlmodel = ct.convert(\n",
        "#     traced_model,\n",
        "#     inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape)], #name \"input_1\" is used in 'quickstart'\n",
        "#     classifier_config = ct.ClassifierConfig(class_labels) # provide only if step 2 was performed\n",
        "# )\n",
        "\n",
        "\n",
        "#Set the image scale and bias for input image preprocessing.\n",
        "scale = 1.0 / (255.0 * 0.226)\n",
        "red_bias = -0.485 / 0.226\n",
        "green_bias = -0.456 / 0.226\n",
        "blue_bias = -0.406 / 0.226\n",
        "\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(name=\"input_1\", shape=example_input.shape, scale=scale, bias=[red_bias, green_bias, blue_bias])],\n",
        "    classifier_config=ct.ClassifierConfig(class_labels)\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model_parent_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/250px_for_MobileNetV3_training\"\n",
        "mlmodel.save(f\"{model_parent_path}/MobileNetV3_extended.mlmodel\")\n"
      ],
      "metadata": {
        "id": "Hp2FOqU89Qgh",
        "outputId": "852987b9-62c5-4da4-de93-4fa915bad38e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 467/468 [00:00<00:00, 2854.09 ops/s]\n",
            "Running MIL Common passes: 100%|██████████| 40/40 [00:00<00:00, 76.30 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 11/11 [00:00<00:00, 127.54 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 665/665 [00:00<00:00, 1664.59 ops/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6SpdWnl0EhsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WcHCKuiscqHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPUmFvYREXMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCSPS2omEXKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JcDbxC9OEXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lIgEoaLwEWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Make folders for YOLO5 training**"
      ],
      "metadata": {
        "id": "Wmgd-xTbxFMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5トレーニング用\n",
        "#もしdst_folderがあれば削除して新しく作り直す\n",
        "dst_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\"\n",
        "\n",
        "if os.path.exists(dst_folder):\n",
        "    shutil.rmtree(dst_folder)\n",
        "for i in [\"train\", \"valid\"]:\n",
        "    for j in [\"images\", \"labels\"]:\n",
        "        os.makedirs(f\"{dst_folder}/{i}/{j}\")\n",
        "        #os.makedirs(f\"{dst_folder}/labels\")\n",
        "\n",
        "for file in img_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/images/{os.path.basename(file)}\")\n",
        "for file in img_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/images/{os.path.basename(file)}\")\n",
        "for file in label_train[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/train/labels/{os.path.basename(file)}\")\n",
        "for file in label_val[0]:\n",
        "    shutil.copy(file, f\"{dst_folder}/valid/labels/{os.path.basename(file)}\")\n"
      ],
      "metadata": {
        "id": "lKe9k8SirUGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $dst_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAUdjy9A0YZw",
        "outputId": "5d8631eb-b096-471d-96c5-0f3913a7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.yaml\n",
        "# path\n",
        "train: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\n",
        "val: /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\n",
        "\n",
        "# num of classes\n",
        "nc: 2\n",
        "\n",
        "#class names\n",
        "names: ['cont', 'grav'] # class名を定義"
      ],
      "metadata": {
        "id": "giDFflceMi9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e0bdb6-f54e-47f9-e9f4-413245975212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setup YOLOv5**"
      ],
      "metadata": {
        "id": "cdEoEk_996YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjV_xXLpd5__",
        "outputId": "476d809b-269b-4cd6-a0fe-82f4da46dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 31.0/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train YOLOv5**"
      ],
      "metadata": {
        "id": "shiv0uvTdH7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "!python train.py --img 640 --batch 16 --epochs 100 --data /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/dataset.yaml --weights yolov5n.pt\n"
      ],
      "metadata": {
        "id": "spn1bRX60hYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best.pyをrenameしてgdriveに移動しておく\n",
        "orig_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolov5/runs/train/exp/weights/best.pt\"\n",
        "dst_pt = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "shutil.copy(orig_pt, dst_pt)"
      ],
      "metadata": {
        "id": "2_mRrhFn-ONj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951a6753-9a5b-4e71-e026-7416d32bcc91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv5 Intereference**"
      ],
      "metadata": {
        "id": "kX9AdOK31h1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference (folder内全部)\n",
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images"
      ],
      "metadata": {
        "id": "Du5NiwCDdTcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images\")\n",
        "train = os.listdir(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\")\n",
        "\n",
        "print(len(train), len(valid))"
      ],
      "metadata": {
        "id": "oA6h6A4u_K7Z",
        "outputId": "430d02b5-7a51-42f0-f012-cd33f4d3178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2649 664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interference (per image)\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[100]"
      ],
      "metadata": {
        "id": "jmg05lZkDKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt --img 640 --conf 0.25 --source $img"
      ],
      "metadata": {
        "id": "mQxqh5QMDrYR",
        "outputId": "66105c9d-2766-4fff-ef56-107bb88cda87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt'], source=/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/6540.JPG: 448x640 1 grav, 18.4ms\n",
            "Speed: 0.7ms pre-process, 18.4ms inference, 38.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    print(img_tensor.shape)\n",
        "\n",
        "    print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLCs5mn32MvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/gravcont_yolo5n.pt\"\n",
        "image_path = glob.glob(\"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid/images/*\")\n",
        "img = image_path[2]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "cv2_imshow(img_cv2)\n"
      ],
      "metadata": {
        "id": "54vbyhSR-EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference Olympia dataset**"
      ],
      "metadata": {
        "id": "uzZr3wMsrxJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "cpKHAnmE_wFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f69277f-119c-41dc-d348-1bccda7d470d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "\n",
        "# 横幅を640pxにリサイズしたデータセット\n",
        "dataset_grav = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/treated_640px\"\n",
        "dataset_cont = \"/content/drive/MyDrive/Deep_learning/Olympia_dataset/untreated_640px\""
      ],
      "metadata": {
        "id": "CrVmlh1Csdxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    device = 'cpu'\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "VPTVErBetQT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "pred = interference(img, weight)\n",
        "\n",
        "# output result\n",
        "x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "# probability\n",
        "prob = pred[0][0][4].item()\n",
        "\n",
        "# class\n",
        "class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "img_cv2 = cv2.imread(img)\n",
        "\n",
        "# calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "img_height, img_width, _ = img_cv2.shape[:3]\n",
        "print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "padding_x = (img_height - min(img_width, img_height))/2\n",
        "padding_y = (img_width - min(img_width, img_height))/2\n",
        "x1 = x1 - padding_x\n",
        "y1 = y1 - padding_y\n",
        "x2 = x2 - padding_x\n",
        "y2 = y2 - padding_y\n",
        "print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "\n",
        "# draw bounding box\n",
        "cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "# show image\n",
        "cv2_imshow(img_cv2)"
      ],
      "metadata": {
        "id": "_NeSLz6rtalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Export coreML including non-max supression**"
      ],
      "metadata": {
        "id": "g7JhasvF89PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Yolov5 repo\n",
        "import os\n",
        "%cd /content\n",
        "!git clone https://github.com/hietalajulius/yolov5.git\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt -r requirements-export.txt"
      ],
      "metadata": {
        "id": "E7CfdEw-ylvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_path = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\""
      ],
      "metadata": {
        "id": "9uFOe6N9Aiwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python export-nms.py --include coreml --weights $weight_path\n"
      ],
      "metadata": {
        "id": "tBf-4p1BArEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference and crop Extended dataset**"
      ],
      "metadata": {
        "id": "mMbAS9qBSXsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLOv5\n",
        "%cd /content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()"
      ],
      "metadata": {
        "id": "argQTM34hQEI",
        "outputId": "0f8f99ad-cf66-430e-f771-1741a546d52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-72-g064365d Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (NVIDIA A100-SXM4-40GB, 40536MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (12 CPUs, 83.5 GB RAM, 24.9/166.8 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# パスを指定する\n",
        "weight = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/yolo5n_100epoch.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train/images\"\n",
        "output_folder = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train\""
      ],
      "metadata": {
        "id": "SK0LQ6a7hpmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.common import DetectMultiBackend\n",
        "#from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, strip_optimizer, xyxy2xywh)\n",
        "#from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox #padding\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def interference(img, weight):\n",
        "    #stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
        "    #imgsz = check_img_size([640], s=stride)  # check image size\n",
        "\n",
        "    #class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "    # transform = transforms.Compose([\n",
        "    #             transforms.Resize(size=(480,640)),\n",
        "    #             transforms.ToTensor(),\n",
        "    #             # transforms.Normalize(\n",
        "    #             #     mean=[0.5, 0.5, 0.5],\n",
        "    #             #     std=[0.5, 0.5, 0.5]\n",
        "    #             #    )\n",
        "    #             ])\n",
        "\n",
        "    img_cv2 = cv2.imread(img) #CV2で開く\n",
        "    img_cv2 = letterbox(img_cv2, (640,640), stride=32, auto=False)[0] #resize, 上下padding (color 114)\n",
        "\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    img_cv2 = img_cv2.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    img_cv2 = np.ascontiguousarray(img_cv2)\n",
        "    img_tensor = torch.from_numpy(img_cv2).float()\n",
        "\n",
        "    #img_tensor = transform(img_np)\n",
        "    img_tensor /= 255\n",
        "    #print(img_tensor.shape)\n",
        "\n",
        "    #print(img_tensor)\n",
        "    img_tensor = torch.unsqueeze(img_tensor, 0)  # バッチ対応\n",
        "\n",
        "\n",
        "    pred = model(img_tensor, visualize=False, augment=False)\n",
        "\n",
        "    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45, classes=None,  max_det=1000)\n",
        "\n",
        "    print(f\"pred: {pred}\")\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "6qZSIfF5hjGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# image_path = glob.glob(f\"{dataset_grav}/*\")\n",
        "# img = image_path[100]\n",
        "\n",
        "class_names = {0:\"cont\", 1:\"grav\"}\n",
        "\n",
        "device = 'cpu' # device = None or 'cpu' or 0 or '0' or '0,1,2,3'\n",
        "device = select_device(device)\n",
        "model = DetectMultiBackend(weight, device=device, dnn=False)\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for img in tqdm(glob.glob(f\"{input_folder}/*\")):\n",
        "\n",
        "    pred = interference(img, weight)\n",
        "\n",
        "    # output result\n",
        "    x1, y1, x2, y2, prob, class_num = torch.round(pred[0][0])\n",
        "\n",
        "    # probability\n",
        "    prob = pred[0][0][4].item()\n",
        "\n",
        "    # class\n",
        "    class_name = class_names[pred[0][0][5].item()]\n",
        "\n",
        "    print(\"診断は %s、確率は%.1f％です。\" %(class_name, prob*100))\n",
        "\n",
        "    img_cv2 = cv2.imread(img)\n",
        "\n",
        "    # calculate coordinates of the bounding box (640*640にpaddingされている分の座標を足す)\n",
        "    img_height, img_width, _ = img_cv2.shape[:3]\n",
        "    print(f\"img_height: {img_height}, img_width: {img_width}\")\n",
        "    padding_x = (img_height - min(img_width, img_height))/2\n",
        "    padding_y = (img_width - min(img_width, img_height))/2\n",
        "    x1 = x1 - padding_x\n",
        "    y1 = y1 - padding_y\n",
        "    x2 = x2 - padding_x\n",
        "    y2 = y2 - padding_y\n",
        "    print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "    # draw bounding box\n",
        "    #cv2.rectangle(img_cv2, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "\n",
        "    # show image\n",
        "    #cv2_imshow(img_cv2)\n",
        "\n",
        "    # バウンディングボックスで画像を切り抜く」\n",
        "\n",
        "    if x1 < 0: #負の場合のエラー回避\n",
        "        x1 = 0\n",
        "\n",
        "    cropped_image = img_cv2[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "    # 切り抜いた画像を保存する\n",
        "    save_path = f\"{output_folder}/{os.path.basename(img)}\"\n",
        "    print(save_path)\n",
        "    #cv2_imshow(cropped_image)\n",
        "    cv2.imwrite(save_path, cropped_image)"
      ],
      "metadata": {
        "id": "iJqs6HmydRUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewrite csv file (bootcamp用csvのimage_pathを改変)\n",
        "import pandas as pd\n",
        "\n",
        "csv_1_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/train_list.csv\"\n",
        "csv_2_orig = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_for_YOLO_training/valid_list.csv\"\n",
        "csv_1 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/train_list.csv\"\n",
        "csv_2 = \"/content/drive/MyDrive/Deep_learning/GO_extended_dataset/periocular_cropped_using_YOLO/valid_list.csv\"\n",
        "\n",
        "def rewrite_csv(df):\n",
        "    path_list = []\n",
        "    for path in df[\"image_path\"]:\n",
        "        path = path.replace(\"periocular_for_YOLO_training\", \"periocular_cropped_using_YOLO\")\n",
        "        path = path.replace(\"images/\", \"\")\n",
        "        path_list.append(path)\n",
        "    df[\"image_path\"] = path_list\n",
        "    return(df)\n",
        "\n",
        "df = pd.read_csv(csv_1_orig)\n",
        "df = rewrite_csv(df)\n",
        "print(df)\n",
        "df.to_csv(csv_1, index=False)\n",
        "\n",
        "df = pd.read_csv(csv_2_orig)\n",
        "df = rewrite_csv(df)\n",
        "df.to_csv(csv_2,  index=False)"
      ],
      "metadata": {
        "id": "z9kG4PiPlCyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js-kBmr0vhqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln9uTV9Nvhrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwPGcLe_vhu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}